# æ–‡ä»¶å: data_forensics.py
# ä½œç”¨ï¼šä¸€ä¸ªé«˜æ•ˆçš„æ•°æ®æ³•è¯å·¥å…·ï¼Œç”¨äºè¯Šæ–­å®½è¡¨ä¸­å­—æ®µçš„NaNå€¼ï¼Œ
#      å¹¶åŒºåˆ†å…¶åŸå› æ˜¯"åˆç†ç¼ºå¤±"ï¼ˆä¸Šå¸‚å‰/é€€å¸‚å/åœç‰Œï¼‰è¿˜æ˜¯"å¯ç–‘ç¼ºå¤±"ã€‚
#      ä½¿ç”¨å‘é‡åŒ–æ“ä½œæé«˜æ•ˆç‡ï¼Œæ”¯æŒåˆ†åŒºæ•°æ®ç»“æ„ã€‚

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import warnings

from quant_lib.config.constant_config import LOCAL_PARQUET_DATA_DIR

warnings.filterwarnings('ignore')

# æ£€æŸ¥æ˜¯å¦æœ‰pyarrowæ”¯æŒ
try:
    import pyarrow
    HAS_PYARROW = True
except ImportError:
    HAS_PYARROW = False
    print("âš ï¸ è­¦å‘Š: æœªå®‰è£…pyarrowï¼Œå°†å°è¯•ä½¿ç”¨fastparquetæˆ–å…¶ä»–å¼•æ“")


class DataForensics:
    """æ•°æ®æ³•è¯è¯Šæ–­å™¨ - é«˜æ•ˆå‘é‡åŒ–ç‰ˆæœ¬"""

    def __init__(self, data_path: Path = None):
        """
        åˆå§‹åŒ–è¯Šæ–­å™¨ï¼Œé¢„åŠ è½½åŸºç¡€å‚ç…§æ•°æ®ã€‚
        """
        self.data_path = data_path or LOCAL_PARQUET_DATA_DIR
        print(f"æ•°æ®æ³•è¯è¯Šæ–­å™¨åˆå§‹åŒ–...")
        print(f"ä½¿ç”¨æ•°æ®è·¯å¾„: {self.data_path}")

        # 1. é¢„åŠ è½½è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼Œä½œä¸ºæˆ‘ä»¬çš„"æˆ·ç±ç³»ç»Ÿ"
        try:
            self.stock_basic = self._safe_read_parquet(self.data_path / 'stock_basic.parquet')
            self.stock_basic['list_date'] = pd.to_datetime(self.stock_basic['list_date'])
            self.stock_basic['delist_date'] = pd.to_datetime(self.stock_basic['delist_date'])

            # åˆ›å»ºä¾¿äºæŸ¥è¯¢çš„Series
            self.list_dates = self.stock_basic.set_index('ts_code')['list_date']
            self.delist_dates = self.stock_basic.set_index('ts_code')['delist_date']
            print(f"âœ“ è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯åŠ è½½æˆåŠŸï¼Œå…± {len(self.stock_basic)} åªè‚¡ç¥¨ã€‚")
        except Exception as e:
            raise FileNotFoundError(f"æ— æ³•åŠ è½½ stock_basic.parquetï¼Œè¯Šæ–­æ— æ³•è¿›è¡Œ: {e}")

        # 2. é¢„åŠ è½½äº¤æ˜“æ—¥å†
        try:
            self.trade_cal = self._safe_read_parquet(self.data_path / 'trade_cal.parquet')
            self.trade_cal['cal_date'] = pd.to_datetime(self.trade_cal['cal_date'])
            self.trading_dates = self.trade_cal[self.trade_cal['is_open'] == 1]['cal_date'].sort_values()
            print(f"âœ“ äº¤æ˜“æ—¥å†åŠ è½½æˆåŠŸï¼Œå…± {len(self.trading_dates)} ä¸ªäº¤æ˜“æ—¥ã€‚")
        except Exception as e:
            print(f"âš ï¸ è­¦å‘Š: æ— æ³•åŠ è½½äº¤æ˜“æ—¥å†: {e}")
            self.trading_dates = None

    def _safe_read_parquet(self, file_path: Path) -> pd.DataFrame:
        """
        å®‰å…¨è¯»å–parquetæ–‡ä»¶ï¼Œå°è¯•ä¸åŒçš„å¼•æ“
        """
        engines = ['pyarrow', 'fastparquet'] if HAS_PYARROW else ['fastparquet']

        for engine in engines:
            try:
                return pd.read_parquet(file_path, engine=engine)
            except ImportError:
                continue
            except Exception as e:
                if engine == engines[-1]:  # æœ€åä¸€ä¸ªå¼•æ“ä¹Ÿå¤±è´¥äº†
                    raise e
                continue

        # å¦‚æœæ‰€æœ‰å¼•æ“éƒ½å¤±è´¥ï¼Œå°è¯•ä¸æŒ‡å®šå¼•æ“
        try:
            return pd.read_parquet(file_path)
        except Exception as e:
            raise ImportError(f"æ— æ³•è¯»å–parquetæ–‡ä»¶ {file_path}ã€‚è¯·å®‰è£… pyarrow æˆ– fastparquet: pip install pyarrow") from e

    def _load_dataset(self, dataset_name: str) -> pd.DataFrame:
        """
        æ™ºèƒ½åŠ è½½æ•°æ®é›†ï¼Œæ”¯æŒåˆ†åŒºå’Œå•æ–‡ä»¶ä¸¤ç§æ ¼å¼

        Args:
            dataset_name: æ•°æ®é›†åç§°ï¼Œå¦‚ 'daily_hfq' æˆ– 'stock_basic.parquet'

        Returns:
            åŠ è½½çš„DataFrame
        """
        dataset_path = self.data_path / dataset_name

        if dataset_path.is_dir():
            # åˆ†åŒºæ•°æ®ï¼Œç›´æ¥è¯»å–æ•´ä¸ªç›®å½•
            print(f"  -> æ£€æµ‹åˆ°åˆ†åŒºæ•°æ®ï¼ŒåŠ è½½æ•´ä¸ªç›®å½•: {dataset_name}")
            df = self._safe_read_parquet(dataset_path)
        elif dataset_path.with_suffix('.parquet').exists():
            # å•æ–‡ä»¶æ•°æ®
            print(f"  -> æ£€æµ‹åˆ°å•æ–‡ä»¶æ•°æ®: {dataset_name}")
            df = self._safe_read_parquet(dataset_path.with_suffix('.parquet'))
        else:
            raise FileNotFoundError(f"æ•°æ®é›†ä¸å­˜åœ¨: {dataset_name}")

        return df

    def diagnose_field_nan(self, field_name: str, dataset_name: str, 
                          sample_stocks: int = 10, detailed_analysis: bool = True):
        """
        å¯¹æŒ‡å®šå­—æ®µçš„NaNå€¼è¿›è¡Œé«˜æ•ˆè¯Šæ–­ã€‚

        Args:
            field_name (str): è¦è¯Šæ–­çš„å­—æ®µå (å¦‚ 'close')
            dataset_name (str): è¯¥å­—æ®µæ‰€åœ¨çš„æ•°æ®é›†å (å¦‚ 'daily_hfq')
            sample_stocks (int): éšæœºæŠ½å–å¤šå°‘åªè‚¡ç¥¨è¿›è¡Œè¯¦ç»†è¯Šæ–­
            detailed_analysis (bool): æ˜¯å¦è¿›è¡Œè¯¦ç»†çš„ä¸ªè‚¡åˆ†æ
        """
        print("\n" + "="*70)
        print(f"ğŸ” å¼€å§‹å¯¹æ•°æ®é›† <{dataset_name}> ä¸­çš„å­—æ®µ <{field_name}> è¿›è¡ŒNaNè¯Šæ–­")
        print("="*70)

        # 1. åŠ è½½ç›®æ ‡æ•°æ®
        try:
            df = self._load_dataset(dataset_name)
            
            # æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨
            if field_name not in df.columns:
                print(f"ğŸš¨ é”™è¯¯: å­—æ®µ '{field_name}' ä¸å­˜åœ¨äºæ•°æ®é›† '{dataset_name}' ä¸­")
                print(f"å¯ç”¨å­—æ®µ: {list(df.columns)}")
                return
                
            # å°†é•¿è¡¨è½¬æ¢ä¸ºå®½è¡¨
            print(f"  -> æ­£åœ¨è½¬æ¢ä¸ºå®½è¡¨æ ¼å¼...")
            wide_df = df.pivot_table(index='trade_date', columns='ts_code', values=field_name)
            wide_df.index = pd.to_datetime(wide_df.index)
            wide_df = wide_df.sort_index()
            
            print(f"âœ“ æˆåŠŸåŠ è½½å¹¶è½¬æ¢å®½è¡¨ï¼Œå½¢çŠ¶: {wide_df.shape}")
            print(f"  -> æ—¶é—´èŒƒå›´: {wide_df.index.min().date()} è‡³ {wide_df.index.max().date()}")
            print(f"  -> è‚¡ç¥¨æ•°é‡: {wide_df.shape[1]}")
            
        except Exception as e:
            print(f"ğŸš¨ é”™è¯¯: åŠ è½½æˆ–è½¬æ¢ {dataset_name} å¤±è´¥: {e}")
            return

        # 2. å…¨å±€NaNç»Ÿè®¡
        nan_mask = wide_df.isna()
        total_nans = nan_mask.sum().sum()
        total_cells = wide_df.size
        nan_ratio = total_nans / total_cells
        
        print(f"\nğŸ“Š å…¨å±€NaNç»Ÿè®¡:")
        print(f"  -> æ€»NaNæ•°é‡: {total_nans:,}")
        print(f"  -> æ€»å•å…ƒæ ¼æ•°: {total_cells:,}")
        print(f"  -> NaNæ¯”ä¾‹: {nan_ratio:.2%}")

        if total_nans == 0:
            print("âœ… [ä¼˜ç§€] è¯¥å­—æ®µæ²¡æœ‰ä»»ä½•NaNå€¼ï¼")
            return

        # 3. å‘é‡åŒ–å½’å› åˆ†æ
        print(f"\nğŸ•µï¸ å¼€å§‹å‘é‡åŒ–å½’å› åˆ†æ...")
        attribution_results = self._vectorized_attribution_analysis(wide_df, nan_mask)
        
        # 4. è¾“å‡ºå½’å› ç»Ÿè®¡
        self._print_attribution_summary(attribution_results, total_nans)
        
        # 5. è¯¦ç»†ä¸ªè‚¡åˆ†æï¼ˆå¯é€‰ï¼‰
        if detailed_analysis and sample_stocks > 0:
            self._detailed_stock_analysis(wide_df, nan_mask, sample_stocks)

    def _vectorized_attribution_analysis(self, wide_df: pd.DataFrame, 
                                       nan_mask: pd.DataFrame) -> Dict[str, int]:
        """
        ä½¿ç”¨å‘é‡åŒ–æ“ä½œè¿›è¡Œé«˜æ•ˆçš„NaNå½’å› åˆ†æ
        
        Returns:
            å½’å› ç»“æœå­—å…¸
        """
        print("  -> æ‰§è¡Œå‘é‡åŒ–å½’å› è®¡ç®—...")
        
        # è·å–æ‰€æœ‰æœ‰NaNçš„è‚¡ç¥¨
        stocks_with_nan = nan_mask.columns[nan_mask.any()]
        
        # åˆå§‹åŒ–è®¡æ•°å™¨
        attribution = {
            'before_listing': 0,
            'after_delisting': 0, 
            'during_trading': 0,
            'unknown_stock': 0
        }
        
        # æ‰¹é‡å¤„ç†è‚¡ç¥¨
        for stock in stocks_with_nan:
            stock_nan_dates = wide_df.index[nan_mask[stock]]
            
            # è·å–è¯¥è‚¡ç¥¨çš„ä¸Šå¸‚å’Œé€€å¸‚æ—¥æœŸ
            list_date = self.list_dates.get(stock)
            delist_date = self.delist_dates.get(stock)
            
            if pd.isna(list_date):
                attribution['unknown_stock'] += len(stock_nan_dates)
                continue
                
            # å‘é‡åŒ–æ¯”è¾ƒ
            before_listing_mask = stock_nan_dates < list_date
            attribution['before_listing'] += before_listing_mask.sum()
            
            if pd.notna(delist_date):
                after_delisting_mask = stock_nan_dates > delist_date
                attribution['after_delisting'] += after_delisting_mask.sum()
                
                # å‰©ä½™çš„å°±æ˜¯äº¤æ˜“æœŸé—´çš„NaN
                during_trading_count = len(stock_nan_dates) - before_listing_mask.sum() - after_delisting_mask.sum()
            else:
                during_trading_count = len(stock_nan_dates) - before_listing_mask.sum()
                
            attribution['during_trading'] += during_trading_count
            
        return attribution

    def _print_attribution_summary(self, attribution: Dict[str, int], total_nans: int):
        """æ‰“å°å½’å› åˆ†ææ‘˜è¦"""
        print(f"\nğŸ“‹ NaNå½’å› åˆ†æç»“æœ:")
        print(f"  âœ… ä¸Šå¸‚å‰ç¼ºå¤±: {attribution['before_listing']:,} ({attribution['before_listing']/total_nans:.1%}) - åˆç†")
        print(f"  âœ… é€€å¸‚åç¼ºå¤±: {attribution['after_delisting']:,} ({attribution['after_delisting']/total_nans:.1%}) - åˆç†") 
        print(f"  â„¹ï¸  äº¤æ˜“æœŸé—´ç¼ºå¤±: {attribution['during_trading']:,} ({attribution['during_trading']/total_nans:.1%}) - å¤§æ¦‚ç‡åœç‰Œ")
        print(f"  â“ æœªçŸ¥è‚¡ç¥¨ç¼ºå¤±: {attribution['unknown_stock']:,} ({attribution['unknown_stock']/total_nans:.1%}) - éœ€è¦æ£€æŸ¥")
        
        # è®¡ç®—å¯ç–‘ç¨‹åº¦
        suspicious_ratio = attribution['unknown_stock'] / total_nans
        if suspicious_ratio > 0.05:  # è¶…è¿‡5%è®¤ä¸ºå¯ç–‘
            print(f"\nâš ï¸  è­¦å‘Š: æœªçŸ¥è‚¡ç¥¨ç¼ºå¤±æ¯”ä¾‹è¾ƒé«˜ ({suspicious_ratio:.1%})ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ï¼")
        else:
            print(f"\nâœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¤§éƒ¨åˆ†NaNéƒ½æœ‰åˆç†è§£é‡Šã€‚")

    def _detailed_stock_analysis(self, wide_df: pd.DataFrame, nan_mask: pd.DataFrame,
                                sample_stocks: int):
        """è¯¦ç»†çš„ä¸ªè‚¡åˆ†æ"""
        print(f"\nğŸ”¬ è¯¦ç»†ä¸ªè‚¡åˆ†æ (æŠ½æ · {sample_stocks} åªè‚¡ç¥¨):")
        print("-" * 50)

        # æ‰¾åˆ°NaNæœ€å¤šçš„è‚¡ç¥¨è¿›è¡ŒæŠ½æ ·åˆ†æ
        nan_counts_per_stock = nan_mask.sum().sort_values(ascending=False)
        stocks_to_check = nan_counts_per_stock.head(sample_stocks).index

        for i, stock in enumerate(stocks_to_check, 1):
            print(f"\n[{i}] è‚¡ç¥¨: {stock} (NaNæ•°é‡: {nan_counts_per_stock[stock]})")

            stock_series = wide_df[stock]
            nan_dates = stock_series[stock_series.isna()].index

            list_date = self.list_dates.get(stock)
            delist_date = self.delist_dates.get(stock)

            if pd.isna(list_date):
                print("    â“ è­¦å‘Š: åœ¨stock_basicä¸­æœªæ‰¾åˆ°è¯¥è‚¡ç¥¨ä¿¡æ¯")
                continue

            # å½’å› åˆ†æ
            before_listing = nan_dates[nan_dates < list_date]
            after_delisting = nan_dates[nan_dates > delist_date] if pd.notna(delist_date) else pd.DatetimeIndex([])
            during_trading = nan_dates.drop(before_listing).drop(after_delisting, errors='ignore')

            print(f"    ğŸ“… ä¸Šå¸‚: {list_date.date()}, é€€å¸‚: {delist_date.date() if pd.notna(delist_date) else 'æœªé€€å¸‚'}")

            if not before_listing.empty:
                print(f"    âœ… ä¸Šå¸‚å‰NaN: {len(before_listing)}ä¸ª")
            if not after_delisting.empty:
                print(f"    âœ… é€€å¸‚åNaN: {len(after_delisting)}ä¸ª")
            if not during_trading.empty:
                print(f"    â„¹ï¸  äº¤æ˜“æœŸé—´NaN: {len(during_trading)}ä¸ª")

                # åˆ†æè¿ç»­æ€§
                if len(during_trading) > 1:
                    gaps = (during_trading.to_series().diff() > pd.Timedelta('1 day')).sum()
                    print(f"       -> å½¢æˆ {gaps + 1} ä¸ªè¿ç»­ç¼ºå¤±åŒºé—´")

                    # æ˜¾ç¤ºæœ€é•¿çš„ç¼ºå¤±åŒºé—´
                    if len(during_trading) > 5:
                        print(f"       -> æœ€è¿‘ç¼ºå¤±æ—¥æœŸ: {during_trading[-3:].strftime('%Y-%m-%d').tolist()}")

    def batch_diagnose(self, field_dataset_pairs: List[Tuple[str, str]],
                      sample_stocks: int = 5, detailed_analysis: bool = False):
        """
        æ‰¹é‡è¯Šæ–­å¤šä¸ªå­—æ®µ

        Args:
            field_dataset_pairs: [(field_name, dataset_name), ...] çš„åˆ—è¡¨
            sample_stocks: æ¯ä¸ªå­—æ®µæŠ½æ ·åˆ†æçš„è‚¡ç¥¨æ•°é‡
            detailed_analysis: æ˜¯å¦è¿›è¡Œè¯¦ç»†åˆ†æ
        """
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡è¯Šæ–­ {len(field_dataset_pairs)} ä¸ªå­—æ®µ...")

        results_summary = []

        for i, (field_name, dataset_name) in enumerate(field_dataset_pairs, 1):
            print(f"\n{'='*80}")
            print(f"ğŸ“Š æ‰¹é‡è¯Šæ–­è¿›åº¦: {i}/{len(field_dataset_pairs)}")
            print(f"{'='*80}")

            try:
                # æ‰§è¡Œå•ä¸ªå­—æ®µè¯Šæ–­
                self.diagnose_field_nan(
                    field_name=field_name,
                    dataset_name=dataset_name,
                    sample_stocks=sample_stocks,
                    detailed_analysis=detailed_analysis
                )
                results_summary.append((field_name, dataset_name, "âœ… æˆåŠŸ"))

            except Exception as e:
                print(f"âŒ è¯Šæ–­å¤±è´¥: {e}")
                results_summary.append((field_name, dataset_name, f"âŒ å¤±è´¥: {str(e)[:50]}"))

        # è¾“å‡ºæ‰¹é‡è¯Šæ–­æ‘˜è¦
        print(f"\n{'='*80}")
        print("ğŸ“‹ æ‰¹é‡è¯Šæ–­æ‘˜è¦:")
        print(f"{'='*80}")
        for field_name, dataset_name, status in results_summary:
            print(f"  {status} | {field_name} @ {dataset_name}")

    def generate_data_quality_report(self, output_path: Optional[str] = None) -> Dict:
        """
        ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š

        Args:
            output_path: æŠ¥å‘Šä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™åªè¿”å›ç»“æœä¸ä¿å­˜

        Returns:
            æ•°æ®è´¨é‡æŠ¥å‘Šå­—å…¸
        """
        print(f"\nğŸ“Š ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š...")

        # å®šä¹‰è¦æ£€æŸ¥çš„æ ¸å¿ƒå­—æ®µ
        core_fields = [
            ('close', 'daily_hfq'),
            ('vol', 'daily_hfq'),
            ('pe_ttm', 'daily_basic'),
            ('pb', 'daily_basic'),
            ('turnover_rate', 'daily_basic')
        ]

        report = {
            'generated_at': datetime.now().isoformat(),
            'data_path': str(self.data_path),
            'fields_analyzed': [],
            'overall_quality_score': 0.0
        }

        quality_scores = []

        for field_name, dataset_name in core_fields:
            print(f"\n  -> åˆ†æ {field_name} @ {dataset_name}")

            try:
                # åŠ è½½æ•°æ®å¹¶è®¡ç®—NaNç»Ÿè®¡
                df = self._load_dataset(dataset_name)
                if field_name not in df.columns:
                    continue

                wide_df = df.pivot_table(index='trade_date', columns='ts_code', values=field_name)
                nan_mask = wide_df.isna()

                total_nans = nan_mask.sum().sum()
                total_cells = wide_df.size
                nan_ratio = total_nans / total_cells if total_cells > 0 else 0

                # è®¡ç®—è´¨é‡åˆ†æ•° (1 - nan_ratio)
                quality_score = max(0, 1 - nan_ratio)
                quality_scores.append(quality_score)

                # å‘é‡åŒ–å½’å› åˆ†æ
                attribution = self._vectorized_attribution_analysis(wide_df, nan_mask)

                field_report = {
                    'field_name': field_name,
                    'dataset_name': dataset_name,
                    'total_nans': total_nans,
                    'total_cells': total_cells,
                    'nan_ratio': nan_ratio,
                    'quality_score': quality_score,
                    'attribution': attribution
                }

                report['fields_analyzed'].append(field_report)

            except Exception as e:
                print(f"    âŒ åˆ†æå¤±è´¥: {e}")
                continue

        # è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
        if quality_scores:
            report['overall_quality_score'] = sum(quality_scores) / len(quality_scores)

        # ä¿å­˜æŠ¥å‘Š
        if output_path:
            import json
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"âœ… æ•°æ®è´¨é‡æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_path}")

        return report


# --- ä½¿ç”¨ç¤ºä¾‹ ---
if __name__ == '__main__':
    # 1. å®ä¾‹åŒ–è¯Šæ–­å™¨
    forensics = DataForensics()

    # 2. å•ä¸ªå­—æ®µè¯Šæ–­
    # forensics.diagnose_field_nan(
    #     field_name='close',
    #     dataset_name='daily_hfq',
    #     sample_stocks=8,
    #     detailed_analysis=True
    # )
    batch_fields = [
        # ('industry','stock_basic.parquet'),
        ('pe_ttm', 'daily_basic'),

        ('close', 'daily_hfq'),
        ('turnover_rate', 'daily_basic'),

        ('pb','daily_basic'),
        ('circ_mv','daily_basic'),
        # ('list_date','stock_basic.parquet'),
        ('total_mv','daily_basic')
    ]

    # 3. æ‰¹é‡è¯Šæ–­ç¤ºä¾‹
    forensics.batch_diagnose(batch_fields, sample_stocks=3)

    # 4. ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
    report = forensics.generate_data_quality_report('data_quality_report.json')

    print("\n" + "="*70)
    print("ğŸ¯ è¯Šæ–­å®Œæˆï¼ä½ å¯ä»¥ç»§ç»­è¯Šæ–­å…¶ä»–å­—æ®µ:")

