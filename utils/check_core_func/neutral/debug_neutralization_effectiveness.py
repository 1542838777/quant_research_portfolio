#!/usr/bin/env python3
"""
ä¸­æ€§åŒ–æ•ˆæœéªŒè¯è„šæœ¬
éªŒè¯å› å­ä¸­æ€§åŒ–æ˜¯å¦ç”Ÿæ•ˆçš„å¤šç»´åº¦æ£€æŸ¥
"""

import pandas as pd
import numpy as np
from scipy.stats import spearmanr
import warnings
warnings.filterwarnings('ignore')

def verify_neutralization_effectiveness(original_factor_df, neutralized_factor_df, neutral_dfs, 
                                      test_dates=None, verbose=True):
    """
    å…¨é¢éªŒè¯ä¸­æ€§åŒ–æ•ˆæœ
    
    Args:
        original_factor_df: åŸå§‹å› å­æ•°æ® (index=date, columns=stocks)
        neutralized_factor_df: ä¸­æ€§åŒ–åå› å­æ•°æ®
        neutral_dfs: ä¸­æ€§åŒ–ç”¨çš„é£æ ¼å› å­å­—å…¸
        test_dates: æµ‹è¯•æ—¥æœŸåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæµ‹è¯•æ‰€æœ‰æ—¥æœŸ
    """
    if test_dates is None:
        test_dates = original_factor_df.index[:10]  # æµ‹è¯•å‰10å¤©
    
    results = {}
    
    print("=== ğŸ”¬ ä¸­æ€§åŒ–æ•ˆæœéªŒè¯æŠ¥å‘Š ===\n")
    
    # 1. åŸºç¡€ç»Ÿè®¡æ£€æŸ¥
    print("1ï¸âƒ£ åŸºç¡€ç»Ÿè®¡æ£€æŸ¥")
    orig_stats = get_factor_stats(original_factor_df)
    neut_stats = get_factor_stats(neutralized_factor_df)
    
    print(f"   åŸå§‹å› å­: å‡å€¼={orig_stats['mean']:.6f}, æ ‡å‡†å·®={orig_stats['std']:.6f}")
    print(f"   ä¸­æ€§åŒ–å: å‡å€¼={neut_stats['mean']:.6f}, æ ‡å‡†å·®={neut_stats['std']:.6f}")
    print(f"   æ•°æ®è¦†ç›–: åŸå§‹{orig_stats['coverage']:.2%} â†’ ä¸­æ€§åŒ–å{neut_stats['coverage']:.2%}")
    
    # 2. ä¸é£æ ¼å› å­ç›¸å…³æ€§æ£€æŸ¥
    print("\n2ï¸âƒ£ ä¸é£æ ¼å› å­ç›¸å…³æ€§æ£€æŸ¥")
    for style_name, style_df in neutral_dfs.items():
        if style_name.startswith('industry_'):
            continue  # è·³è¿‡è¡Œä¸šå“‘å˜é‡
            
        orig_corr = calculate_average_correlation(original_factor_df, style_df, test_dates)
        neut_corr = calculate_average_correlation(neutralized_factor_df, style_df, test_dates)
        
        print(f"   vs {style_name:12s}: {orig_corr:7.4f} â†’ {neut_corr:7.4f} "
              f"(é™ä½äº† {abs(orig_corr - neut_corr):.4f})")
        
        results[f'corr_reduction_{style_name}'] = abs(orig_corr - neut_corr)
    
    # 3. è¡Œä¸šä¸­æ€§åŒ–æ£€æŸ¥
    print("\n3ï¸âƒ£ è¡Œä¸šä¸­æ€§åŒ–æ£€æŸ¥")
    industry_results = check_industry_neutralization(
        original_factor_df, neutralized_factor_df, neutral_dfs, test_dates
    )
    
    for industry, reduction in industry_results.items():
        print(f"   {industry}: è¡Œä¸šæ•ˆåº”é™ä½ {reduction:.4f}")
    
    # 4. æˆªé¢ç›¸å…³æ€§ä¿æŒæ£€æŸ¥
    print("\n4ï¸âƒ£ æˆªé¢ç›¸å…³æ€§ä¿æŒæ£€æŸ¥")
    cross_corr = check_cross_sectional_correlation(original_factor_df, neutralized_factor_df, test_dates)
    print(f"   æˆªé¢ç›¸å…³æ€§ä¿æŒåº¦: {cross_corr:.4f} (>0.5ä¸ºè‰¯å¥½)")
    
    # 5. å…·ä½“æ—¥æœŸéªŒè¯
    if verbose:
        print("\n5ï¸âƒ£ å…·ä½“æ—¥æœŸéªŒè¯")
        for date in test_dates[:3]:  # å±•ç¤ºå‰3å¤©çš„è¯¦ç»†æƒ…å†µ
            daily_check(original_factor_df, neutralized_factor_df, neutral_dfs, date)
    
    return results

def get_factor_stats(factor_df):
    """è·å–å› å­åŸºç¡€ç»Ÿè®¡"""
    values = factor_df.stack().dropna()
    total_possible = factor_df.shape[0] * factor_df.shape[1]
    coverage = len(values) / total_possible
    
    return {
        'mean': values.mean(),
        'std': values.std(),
        'coverage': coverage,
        'count': len(values)
    }

def calculate_average_correlation(factor_df, style_df, test_dates):
    """è®¡ç®—å› å­ä¸é£æ ¼å› å­çš„å¹³å‡ç›¸å…³æ€§"""
    correlations = []
    
    for date in test_dates:
        if date not in factor_df.index or date not in style_df.index:
            continue
            
        factor_values = factor_df.loc[date].dropna()
        style_values = style_df.loc[date].dropna()
        
        # æ‰¾åˆ°å…±åŒè‚¡ç¥¨
        common_stocks = factor_values.index.intersection(style_values.index)
        if len(common_stocks) < 10:  # è‡³å°‘10åªè‚¡ç¥¨æ‰è®¡ç®—ç›¸å…³æ€§
            continue
            
        factor_common = factor_values.loc[common_stocks]
        style_common = style_values.loc[common_stocks]
        
        # è®¡ç®—Spearmanç›¸å…³æ€§ï¼ˆæ›´ç¨³å¥ï¼‰
        corr, pval = spearmanr(factor_common, style_common)
        if not np.isnan(corr):
            correlations.append(corr)
    
    return np.mean(correlations) if correlations else 0.0

def check_industry_neutralization(original_factor_df, neutralized_factor_df, neutral_dfs, test_dates):
    """æ£€æŸ¥è¡Œä¸šä¸­æ€§åŒ–æ•ˆæœ"""
    # æå–è¡Œä¸šå“‘å˜é‡
    industry_dummies = {k: v for k, v in neutral_dfs.items() if k.startswith('industry_')}
    
    if not industry_dummies:
        return {'no_industry_data': 0.0}
    
    results = {}
    
    # é‡æ„è¡Œä¸šå½’å±çŸ©é˜µ
    for date in test_dates[:3]:  # åªæ£€æŸ¥å‰3å¤©ï¼Œé¿å…è¿‡å¤šè¾“å‡º
        if date not in original_factor_df.index:
            continue
            
        # ä»å“‘å˜é‡é‡æ„æ¯åªè‚¡ç¥¨çš„è¡Œä¸š
        stock_industries = {}
        for stock in original_factor_df.columns:
            for ind_name, ind_df in industry_dummies.items():
                if date in ind_df.index and stock in ind_df.columns:
                    if ind_df.loc[date, stock] == 1:
                        industry_code = ind_name.replace('industry_', '')
                        stock_industries[stock] = industry_code
                        break
        
        if len(stock_industries) < 5:  # è¡Œä¸šä¿¡æ¯å¤ªå°‘
            continue
            
        # è®¡ç®—è¡Œä¸šå†…å› å­å‡å€¼çš„ç¦»æ•£ç¨‹åº¦
        orig_industry_effect = calculate_industry_effect(
            original_factor_df.loc[date], stock_industries
        )
        neut_industry_effect = calculate_industry_effect(
            neutralized_factor_df.loc[date], stock_industries
        )
        
        reduction = orig_industry_effect - neut_industry_effect
        results[f'{date.date()}'] = reduction
    
    return results

def calculate_industry_effect(factor_series, stock_industries):
    """è®¡ç®—è¡Œä¸šæ•ˆåº”å¼ºåº¦"""
    industry_means = {}
    
    for stock, industry in stock_industries.items():
        if stock in factor_series.index and not pd.isna(factor_series[stock]):
            if industry not in industry_means:
                industry_means[industry] = []
            industry_means[industry].append(factor_series[stock])
    
    # è®¡ç®—æ¯ä¸ªè¡Œä¸šçš„å‡å€¼
    industry_avg = {ind: np.mean(values) for ind, values in industry_means.items() 
                    if len(values) >= 2}
    
    if len(industry_avg) < 2:
        return 0.0
        
    # è¡Œä¸šå‡å€¼çš„æ ‡å‡†å·®ä»£è¡¨è¡Œä¸šæ•ˆåº”å¼ºåº¦
    return np.std(list(industry_avg.values()))

def check_cross_sectional_correlation(original_factor_df, neutralized_factor_df, test_dates):
    """æ£€æŸ¥æˆªé¢ç›¸å…³æ€§æ˜¯å¦ä¿æŒ"""
    correlations = []
    
    for date in test_dates:
        if date not in original_factor_df.index or date not in neutralized_factor_df.index:
            continue
            
        orig_values = original_factor_df.loc[date].dropna()
        neut_values = neutralized_factor_df.loc[date].dropna()
        
        # æ‰¾åˆ°å…±åŒè‚¡ç¥¨
        common_stocks = orig_values.index.intersection(neut_values.index)
        if len(common_stocks) < 10:
            continue
            
        orig_common = orig_values.loc[common_stocks]
        neut_common = neut_values.loc[common_stocks]
        
        corr, pval = spearmanr(orig_common, neut_common)
        if not np.isnan(corr):
            correlations.append(corr)
    
    return np.mean(correlations) if correlations else 0.0

def daily_check(original_factor_df, neutralized_factor_df, neutral_dfs, date):
    """å•æ—¥è¯¦ç»†æ£€æŸ¥"""
    print(f"\n   ğŸ“… {date.date()} è¯¦ç»†æ£€æŸ¥:")
    
    orig_values = original_factor_df.loc[date].dropna()
    neut_values = neutralized_factor_df.loc[date].dropna()
    
    print(f"      æœ‰æ•ˆè‚¡ç¥¨æ•°: {len(orig_values)} â†’ {len(neut_values)}")
    print(f"      å› å­å‡å€¼: {orig_values.mean():.6f} â†’ {neut_values.mean():.6f}")
    print(f"      å› å­æ ‡å‡†å·®: {orig_values.std():.6f} â†’ {neut_values.std():.6f}")
    
    # æ£€æŸ¥æç«¯å€¼å˜åŒ–
    orig_extreme = (orig_values.abs() > orig_values.abs().quantile(0.95)).sum()
    neut_extreme = (neut_values.abs() > neut_values.abs().quantile(0.95)).sum()
    print(f"      æç«¯å€¼æ•°é‡: {orig_extreme} â†’ {neut_extreme}")

if __name__ == "__main__":
    print("ä¸­æ€§åŒ–æ•ˆæœéªŒè¯è„šæœ¬")
    print("è¯·åœ¨ä¸»ç¨‹åºä¸­è°ƒç”¨ verify_neutralization_effectiveness() å‡½æ•°")
    
    # ç¤ºä¾‹ç”¨æ³•:
    # results = verify_neutralization_effectiveness(
    #     original_factor_df=your_original_factor,
    #     neutralized_factor_df=your_neutralized_factor, 
    #     neutral_dfs=your_neutral_dfs,
    #     test_dates=your_test_dates
    # )