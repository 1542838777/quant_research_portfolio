#!/usr/bin/env python3
"""
è¯Šæ–­çš®å°”é€Šå•è°ƒç³»æ•°å¼‚å¸¸é«˜çš„é—®é¢˜

"""
import statistics

import pandas as pd
import numpy as np
from scipy.stats import spearmanr
import warnings
warnings.filterwarnings('ignore')

#åŸºäºå•ä¸ªæ—¥æœŸ æ¥çœ‹
def everyday_mono(sample_dates,factor_df,returns_df,common_stocks ,n_quantiles  ):
    MIN_SAMPLES = max(50, n_quantiles * 10)

    daily_sample_counts = []
    daily_monotonicity = []
    for date in sample_dates:
        factor_cross = factor_df.loc[date, common_stocks].dropna()
        return_cross = returns_df.loc[date, common_stocks].dropna()
        valid_stocks = factor_cross.index.intersection(return_cross.index)

        daily_sample_counts.append(len(valid_stocks))

        if len(valid_stocks) >= MIN_SAMPLES:
            factor_values = factor_cross[valid_stocks]
            return_values = return_cross[valid_stocks]

            factor_ranks = factor_values.rank(method='first')
            quantiles = pd.qcut(factor_ranks, n_quantiles, labels=False) + 1

            group_returns = []
            for q in range(1, n_quantiles + 1):
                mask = quantiles == q
                if mask.sum() > 0:
                    group_returns.append(return_values[mask].mean())
                else:
                    group_returns.append(np.nan)

            if not any(np.isnan(ret) for ret in group_returns):
                mono, _ = spearmanr(range(1, n_quantiles + 1), group_returns)
                daily_monotonicity.append(mono)
            else:
                daily_monotonicity.append(np.nan)
        else:
            daily_monotonicity.append(1.0)  # æ¨¡æ‹Ÿfillna(0)é€ æˆçš„å®Œç¾å•è°ƒæ€§

    print(f"   å„æ—¥å•è°ƒæ€§: {[f'{m:.3f}' if not pd.isna(m) else 'NaN' for m in daily_monotonicity]}")
    print(f"   æ¯æ—¥æ ·æœ¬æ•°é‡åŠ èµ·æ¥å‡å€¼: { statistics.mean(daily_sample_counts)}")
    print(f"   æ¯æ—¥å•è°ƒæ€§åŠ èµ·æ¥å‡å€¼: { statistics.mean(daily_monotonicity)}")

    insufficient_samples = sum(1 for count in daily_sample_counts if count < MIN_SAMPLES)
    print(f"   æ ·æœ¬ä¸è¶³çš„æ—¥æœŸæ•°: {insufficient_samples} / {len(sample_dates)}")

    if insufficient_samples > 0:
        print(f"   âš ï¸  {insufficient_samples}ä¸ªæ—¥æœŸçš„æ ·æœ¬æ•°ä¸è¶³ï¼Œä¼šå¯¼è‡´äººå·¥å•è°ƒæ€§!")


def mono_by_q(factor_df, forward_returns, common_stocks, param):
    n_quantiles=5
    #å¯¹factor_dfåˆ†ç»„
    # 1. è®¡ç®—æœªæ¥æ”¶ç›Šç‡

    # 2. æ•°æ®è½¬æ¢ä¸å¯¹é½ï¼šä»â€œå®½è¡¨â€åˆ°â€œé•¿è¡¨â€
    # æœ‰æ•ˆåŸŸæ©ç ï¼šæ˜¾å¼å®šä¹‰åˆ†ææ ·æœ¬
    # å•ä¸€äº‹å®æ¥æº - æ˜ç¡®å®šä¹‰æ‰€æœ‰æœ‰æ•ˆçš„(date, stock)åæ ‡ç‚¹
    valid_mask = factor_df.notna() & forward_returns.notna()

    # åº”ç”¨æ©ç ï¼Œç¡®ä¿å› å­å’Œæ”¶ç›Šå…·æœ‰å®Œå…¨ç›¸åŒçš„NaNåˆ†å¸ƒ
    final_factor = factor_df.where(valid_mask)
    final_returns = forward_returns.where(valid_mask)

    # æ•°æ®è½¬æ¢ï¼šä»"å®½è¡¨"åˆ°"é•¿è¡¨"ï¼ˆç°åœ¨æ˜¯å®‰å…¨çš„ï¼‰
    factor_long = final_factor.stack().rename('factor')
    returns_long = final_returns.stack().rename('return')

    # åˆå¹¶æ•°æ®ï¼ˆä¸å†éœ€è¦dropnaï¼Œå› ä¸ºå·²ç»å®Œå…¨å¯¹é½ï¼‰
    merged_df = pd.concat([factor_long, returns_long], axis=1)


    # 4. ç¨³å¥çš„åˆ†ç»„ï¼šä½¿ç”¨rank()è¿›è¡Œç­‰æ•°é‡åˆ†ç»„ (æˆ‘ä»¬åšæŒçš„ç¨³å¥æ–¹æ³•)
    # æŒ‰æ—¥æœŸ(level=0)åˆ†(å› ä¸ºæ˜¯å¤šé‡ç´¢å¼•ï¼Œè¿™é‡Œå–ç¬¬ä¸€ä¸ªç´¢å¼•ï¼šæ—¶é—´)ç»„ï¼Œå¯¹æ¯ä¸ªæˆªé¢å†…çš„å› å­å€¼è¿›è¡Œæ’å
    merged_df['rank'] = merged_df.groupby(level=0)['factor'].rank(method='first')

    # å› ä¸ºrankåˆ—æ˜¯å”¯ä¸€çš„ï¼Œæ‰€ä»¥ä¸éœ€è¦æ‹…å¿ƒduplicatesé—®é¢˜ã€‚
    # ã€æ”¹è¿›ã€‘æ›´ä¸¥æ ¼çš„åˆ†ç»„æ ·æœ¬è¦æ±‚ï¼Œç¡®ä¿ç»Ÿè®¡ç¨³å®šæ€§
    MIN_SAMPLES_FOR_GROUPING = max(50, n_quantiles * 10)  # æ€»æ ·æœ¬è‡³å°‘50ä¸ªï¼Œæˆ–æ¯ç»„è‡³å°‘10ä¸ª
    merged_df['quantile'] = merged_df.groupby(level=0)['rank'].transform(
        lambda x: pd.qcut(x, n_quantiles, labels=False, duplicates='drop') + 1
        if len(x) >= MIN_SAMPLES_FOR_GROUPING else np.nan
    )
    # 5. è®¡ç®—å„åˆ†ä½æ•°çš„å¹³å‡æ”¶ç›Š ï¼ˆæ—¶é—´+ç»„åˆ« ä¸ºä¸€ä¸ªgroupã€‚è¿›è¡Œæ±‚æ”¶ç›Šç‡å¹³å‡ï¼‰ ä»Šå¤©q1ç»„æ”¶ç›Šå¹³å‡ç»“æœ
    daily_quantile_returns = merged_df.groupby([merged_df.index.get_level_values(0), 'quantile'])['return'].mean()

    # 6. æ•°æ®è½¬æ¢ï¼šä»â€œé•¿è¡¨â€æ¢å¤åˆ°â€œå®½è¡¨â€
    quantile_returns_wide = daily_quantile_returns.unstack()
    mean_returns = quantile_returns_wide.mean()
    quantile_means = mean_returns.tolist()

    monotonicity_spearman, p_value = spearmanr(np.arange(1, n_quantiles + 1), quantile_means)

    print(f"å¤šæ—¥æœŸç»Ÿä¸€ ç›´æ¥æŒ‰ç»„åˆ«åˆ†ç±» çš®å°”æ–¯æ›¼å•è°ƒç³»æ•°:{monotonicity_spearman}")

def analyze_debug_data():
    """åˆ†æè°ƒè¯•æ•°æ®ï¼Œå®šä½å•è°ƒç³»æ•°å¼‚å¸¸çš„åŸå› """
    
    print("=" * 80)
    print("çš®å°”é€Šå•è°ƒç³»æ•°å¼‚å¸¸è¯Šæ–­æŠ¥å‘Š")
    print("=" * 80)
    
    # è¯»å–æµ‹è¯•æ•°æ®
    try:
        factor_df = pd.read_csv('/tests/workspace/mem_momentum_12_1.csv',
                                index_col=0, parse_dates=True)
        returns_df = pd.read_csv('/tests/workspace/mem_forward_return_o2c.csv',
                                 index_col=0, parse_dates=True)
        
        print(f"æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"   å› å­æ•°æ®å½¢çŠ¶: {factor_df.shape}")
        print(f"   æ”¶ç›Šæ•°æ®å½¢çŠ¶: {returns_df.shape}")
        print(f"   å› å­æ•°æ®æ—¥æœŸèŒƒå›´: {factor_df.index.min()} ~ {factor_df.index.max()}")
        print(f"   æ”¶ç›Šæ•°æ®æ—¥æœŸèŒƒå›´: {returns_df.index.min()} ~ {returns_df.index.max()}")
        
    except Exception as e:
        print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # 1. æ£€æŸ¥æ•°æ®å¯¹é½é—®é¢˜
    print("\n1. æ•°æ®å¯¹é½æ£€æŸ¥")
    print("-" * 50)
    
    common_dates = factor_df.index.intersection(returns_df.index)
    common_stocks = factor_df.columns.intersection(returns_df.columns)
    
    print(f"   å…±åŒæ—¥æœŸæ•°é‡: {len(common_dates)} / {len(factor_df.index)} (å› å­) vs {len(returns_df.index)} (æ”¶ç›Š)")
    print(f"   å…±åŒè‚¡ç¥¨æ•°é‡: {len(common_stocks)} / {len(factor_df.columns)} (å› å­) vs {len(returns_df.columns)} (æ”¶ç›Š)")
    
    if len(common_dates) < 10:
        print("   âš ï¸  è­¦å‘Š: å…±åŒæ—¥æœŸæ•°é‡è¿‡å°‘ï¼Œå¯èƒ½å¯¼è‡´ç»Ÿè®¡å¼‚å¸¸!")


    # 4. éå†æ¯å¤©çš„å•è°ƒç³»æ•° æœ€åå–å‡å€¼ åˆ†æ
    print(f"\nğŸ“… 4. å¤šæ—¥æœŸåˆ†æ (æ ·æœ¬: å‰10ä¸ªå…±åŒæ—¥æœŸ)")

    everyday_mono(common_dates,factor_df,returns_df,common_stocks,5)
    #å¤šæ—¥æœŸç»Ÿä¸€ ç›´æ¥æŒ‰ç»„åˆ«åˆ†ç±»
    mono_by_q(factor_df,returns_df,common_stocks,5)

    # 5. æ€»ç»“å’Œå»ºè®®
    print(f"\nğŸ’¡ 5. é—®é¢˜è¯Šæ–­æ€»ç»“")
    print("-" * 50)
    print("æ ¹æ®åˆ†æï¼Œçš®å°”é€Šå•è°ƒç³»æ•°å¼‚å¸¸é«˜çš„å¯èƒ½åŸå› :")
    print("1. âœ… æ ·æœ¬æ•°é‡ä¸è¶³: å½“æ—¥æœ‰æ•ˆè‚¡ç¥¨æ•° < 50æ—¶ï¼Œæ‰€æœ‰åˆ†ç»„è¢«è®¾ä¸ºNaN")
    print("2. âœ… NaNå¡«å……ç­–ç•¥: fillna(0)å°†NaNç»„æ”¶ç›Šè®¾ä¸º0ï¼Œåˆ›é€ äººå·¥å•è°ƒæ€§")
    print("3. âœ… æ•°æ®ç¨€ç–æ€§: å› å­æˆ–æ”¶ç›Šæ•°æ®å­˜åœ¨å¤§é‡ç¼ºå¤±å€¼")
    
    print(f"\nğŸ”§ å»ºè®®çš„ä¿®å¤æ–¹æ¡ˆ:")
    print("1. é™ä½MIN_SAMPLES_FOR_GROUPINGé˜ˆå€¼ (ä»50é™è‡³20-30)")
    print("2. æ”¹è¿›NaNå¤„ç†ç­–ç•¥ï¼Œé¿å…ç›´æ¥å¡«å……0")
    print("3. å¢åŠ æ•°æ®è´¨é‡æ£€æŸ¥ï¼Œè¿‡æ»¤å¼‚å¸¸æ—¥æœŸ")
    print("4. å¯¹å•è°ƒæ€§è®¡ç®—æ·»åŠ æ ·æœ¬æ•°é‡æ£€æŸ¥")
    
    print("=" * 80)

if __name__ == "__main__":
    analyze_debug_data()