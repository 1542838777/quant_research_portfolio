"""
æ•°æ®åŠ è½½æ¨¡å—

è¯¥æ¨¡å—æä¾›äº†æ•°æ®åŠ è½½ã€å¤„ç†å’Œå¯¹é½çš„åŠŸèƒ½ã€‚
æ”¯æŒä»æœ¬åœ°æ–‡ä»¶ã€æ•°æ®åº“å’ŒAPIåŠ è½½æ•°æ®ã€‚
"""

import os
import pandas as pd
import numpy as np
import pyarrow.parquet as pq
from pathlib import Path
from typing import Dict, List, Optional, Union, Tuple
from collections import defaultdict

from odbc import noError

from quant_lib import setup_logger
from quant_lib.config.constant_config import LOCAL_PARQUET_DATA_DIR
from quant_lib.config.logger_config import log_warning

# è·å–æ¨¡å—çº§åˆ«çš„logger
logger = setup_logger(__name__)


class DataLoader:
    """
    æ•°æ®åŠ è½½å™¨ç±»
    
    è´Ÿè´£ä»å„ç§æ•°æ®æºåŠ è½½æ•°æ®ï¼Œå¹¶è¿›è¡Œé¢„å¤„ç†ã€å¯¹é½ç­‰æ“ä½œã€‚
    æ”¯æŒæœ¬åœ°Parquetæ–‡ä»¶ã€æ•°æ®åº“å’ŒAPIæ•°æ®æºã€‚
    
    Attributes:
        data_path (Path): æ•°æ®å­˜å‚¨è·¯å¾„
        cache (Dict): æ•°æ®ç¼“å­˜
        field_map (Dict): å­—æ®µåˆ°æ•°æ®æºçš„æ˜ å°„
    """

    # ok
    def __init__(self, data_path: Optional[Path] = None, use_cache: bool = True):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            data_path: æ•°æ®å­˜å‚¨è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
            use_cache: æ˜¯å¦ä½¿ç”¨å†…å­˜ç¼“å­˜
        """
        self.data_path = data_path or LOCAL_PARQUET_DATA_DIR

        if not self.data_path.exists():
            os.makedirs(self.data_path, exist_ok=True)
            logger.info(f"æ•°æ®è·¯å¾„ä¸å­˜åœ¨,ç°å·²åˆ›å»ºæ•°æ®è·¯å¾„: {self.data_path}")

        self.field_map = self._build_field_map_to_file_name()
        logger.info(f"å­—æ®µ->æ‰€åœ¨æ–‡ä»¶Name--æ˜ å°„æ„å»ºå®Œæ¯•ï¼Œå…±å‘ç° {len(self.field_map)} ä¸ªå­—æ®µ")
        # åœ¨åˆå§‹åŒ–æ—¶å°±åŠ è½½äº¤æ˜“æ—¥å†ï¼Œå› ä¸ºå®ƒæ˜¯åç»­æ“ä½œçš„åŸºç¡€(æ­¤å¤„è¿˜æ²¡åŒºåˆ†æ˜¯å¦openï¼Œæ˜¯å…¨éƒ¨
        self.trade_cal = self._load_trade_cal()

    def check_local_date_period_completeness(self, file_to_fields, start_date, end_date):
        for logical_name, columns_to_need_load in file_to_fields.items():
            logger.info(f"å¼€å§‹æ£€æŸ¥{logical_name} æ—¶é—´æ®µå®Œæ•´")
            file_path = self.data_path / logical_name

            df = pd.read_parquet(file_path)
            if logical_name in ['index_daily.parquet', 'daily_basic', 'daily_basic', 'index_weights',
                                'daily', 'stk_limit', 'margin_detail']:
                self.check_local_date_period_completeness_for_trade(logical_name, df, start_date, end_date)
            if 'trade_cal.parquet' == logical_name:
                self.check_local_date_period_completeness_col(logical_name, df, 'cal_date', start_date, end_date)
            if 'namechange.parquet' == logical_name:
                self.check_local_date_period_completeness_col(logical_name, df, 'ann_date', start_date, end_date)
            if 'stock_basic.parquet' == logical_name:
                self.check_local_date_period_completeness_col(logical_name, df, 'list_date', start_date, end_date)
            if 'fina_indicator.parquet' == logical_name:
                self.check_local_date_period_completeness_col(logical_name, df, 'ann_date', start_date, end_date)

    def _load_trade_cal(self) -> pd.DataFrame:
        """åŠ è½½äº¤æ˜“æ—¥å†"""
        try:
            trade_cal_df = pd.read_parquet(self.data_path / 'trade_cal.parquet')
            trade_cal_df['cal_date'] = pd.to_datetime(trade_cal_df['cal_date'])
            trade_cal_df.sort_values('cal_date', inplace=True)
            return trade_cal_df
        except Exception as e:
            logger.error(f"åŠ è½½äº¤æ˜“æ—¥å†å¤±è´¥: {e}")
            raise

    def get_trading_dates(self, start_date: str, end_date: str) -> pd.DatetimeIndex:
        """æ ¹æ®èµ·æ­¢æ—¥æœŸï¼Œä»äº¤æ˜“æ—¥å†ä¸­è·å–äº¤æ˜“æ—¥åºåˆ—ã€‚"""
        mask = (self.trade_cal['cal_date'] >= start_date) & \
               (self.trade_cal['cal_date'] <= end_date) & \
               (self.trade_cal['is_open'] == 1)
        dates = pd.to_datetime(self.trade_cal.loc[mask, 'cal_date'].unique())
        return pd.DatetimeIndex(sorted(dates))  # æ˜¾å¼æ’åºï¼Œç¡®ä¿æœ‰åº

    def _build_field_map_to_file_name(self) -> Dict[str, str]:
        """
        æ„å»ºå­—æ®µåˆ°æ•°æ®æºçš„æ˜ å°„
        
        Returns:
            å­—æ®µåˆ°æ•°æ®æºçš„æ˜ å°„å­—å…¸
        """
        field_to_files_map = {}

        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰parquetæ–‡ä»¶
        for file_path in self.data_path.rglob('*.parquet'):
            try:
                # åªè¯»å–schemaä»¥è·å–åˆ—å
                columns = pq.read_schema(file_path).names

                # data.xxx å°±æ˜¯é€»è¾‘æ•°æ®é›†åç§°ï¼ˆå³ï¼šæŒ‰å¹´ä»½åˆ†åŒºçš„æ•°æ®
                if file_path.stem == 'data':
                    # åˆ†åŒºæ•°æ®
                    logical_name = file_path.parent.parent.name
                else:
                    # å•æ–‡ä»¶
                    logical_name = file_path.stem + '.parquet'

                # æ„å»ºå­—æ®µæ˜ å°„
                for col in columns:
                    if (col in ['total_mv', 'circ_mv', 'turnover_rate']) & (
                            logical_name != 'daily_basic'):
                        continue
                    if (col in ['list_date', 'delist_date']) & (
                            logical_name != 'stock_basic.parquet'):
                        continue
                    if (col in ['close', 'open', 'high', 'low']) & (  # å®æµ‹ amount å’Œvol åœ¨dailyå’Œ åœ¨daily_hfqæ•°å€¼ä¸€æ¨¡ä¸€æ ·ï¼
                            logical_name == 'daily_hfq'):  # ï¼Œæˆ‘ä»¬éœ€è¦daily_hfq(åå¤æƒçš„æ•°æ®)è¡¨é‡Œé¢çš„æ•°æ® #æœ€æ–°ä¿®æ”¹ æ‰‹åŠ¨è®¡ç®—ï¼Œä¸ä¾èµ–ä¸çº¯æ´çš„hfq
                        field_to_files_map[col + '_hfq'] = logical_name
                        continue
                    if (col in ['close', 'vol']) & (
                            logical_name == 'daily'):
                        field_to_files_map[col + '_raw'] = logical_name
                        continue
                    if (col in ['amount']) & (
                            logical_name == 'daily'):
                        field_to_files_map[col] = logical_name
                        continue
                        # 'turnover_rate', 'circ_mv', 'total_mv'  è¿™äº›æ˜¯â€œçº¯å‡€åŸææ–™â€ï¼Œå®ƒä»¬æ˜¯æ¯æ—¥æ›´æ–°çš„ã€ä¸ä¾èµ–äºè´¢æŠ¥å‘å¸ƒæ—¶é—´çš„éšæ—¶ç‚¹ï¼ˆPoint-in-Timeï¼‰æ•°æ®
                    not_allow_load_fieds_for_not_fq = ['adj_factor', 'pe_ttm', 'pb', 'ps_ttm']
                    if (col in not_allow_load_fieds_for_not_fq):  #
                        continue  # (f'ä¸¥è°¨åŠ è½½ä¾èµ–æŠ¥å‘Šæ—¥å‘å¸ƒçš„æ•°æ®{col} édailyæ•°æ® ,è¯·å°†config ç”¨adj_factorçš„ from_dailyé…ç½® ç½®ä¸ºfalseï¼Œè¿™æ ·å°±ä¸ä¼šæ­¤é˜¶æ®µåŠ è½½äº†')
                    if col not in field_to_files_map:
                        field_to_files_map[col] = logical_name
            except Exception as e:
                logger.error(f"è¯»å–æ–‡ä»¶ {file_path} çš„å…ƒæ•°æ®å¤±è´¥: {e}")

        return field_to_files_map

    # ok
    def get_raw_dfs_by_require_fields(self,
                                      fields: List[str],
                                      buffer_start_date: str,
                                      end_date: str,
                                      ts_codes: Optional[List[str]] = None) -> Dict[str, pd.DataFrame]:
        """
        åŠ è½½æ•°æ®
        
        Args:
            fields: éœ€è¦åŠ è½½çš„å­—æ®µåˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            ts_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™åŠ è½½æ‰€æœ‰è‚¡ç¥¨
            
        Returns:
            å­—æ®µåˆ°DataFrameçš„æ˜ å°„å­—å…¸
        """
        logger.info(f"å¼€å§‹åŠ è½½æ•°æ®: å­—æ®µ={fields}, æ—¶é—´èŒƒå›´={buffer_start_date}è‡³{end_date}")

        # ç¡®å®šéœ€è¦åŠ è½½çš„æ•°æ®é›†å’Œå­—æ®µ
        file_to_fields = defaultdict(list)
        base_fields = ['ts_code', 'trade_date']

        for field in list(set(fields + base_fields)):
            logical_name = self.field_map.get(field)
            if logical_name is None:
                raise ValueError(f"æœªæ‰¾åˆ°å­—æ®µ {field} çš„æ•°æ®æº")

            file_to_fields[logical_name].append(field)
        # self.check_local_date_period_completeness(file_to_fields, start_date, end_date) todo åé¢å®ç›˜å¼€å¯
        # åŠ è½½å’Œå¤„ç†æ•°æ®
        raw_wide_dfs = {}  # è£… å®½åŒ–çš„df
        raw_long_dfs = {}  # åŸç”Ÿçš„ ä»æœ¬åœ°æ‹¿åˆ°çš„ key :æ–‡ä»¶ï¼Œvalueï¼šdfï¼ˆæ‰€æœ‰åˆ—ï¼ï¼‰
        for logical_name, columns_to_need_load in file_to_fields.items():
            try:
                file_path = self.data_path / logical_name

                # æ£€æŸ¥æ–‡ä»¶ä¸­å®é™…å­˜åœ¨çš„å­—æ®µ
                columns_to_need_load =  self.fix_names_for_origin(columns_to_need_load,logical_name)
                available_columns = pd.read_parquet(file_path).columns
                columns_can_read = list(set(columns_to_need_load + base_fields) & set(available_columns))

                if not columns_can_read:
                    log_warning(f"æ–‡ä»¶ {logical_name} ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•éœ€è¦çš„å­—æ®µ")
                    continue

                # åŠ è½½æ•°æ®
                long_df = pd.read_parquet(
                    file_path,
                    columns=list(set(columns_can_read))
                )

                # æ—¶é—´ç­›é€‰
                long_df = self.extract_during_period(long_df, logical_name, buffer_start_date, end_date)

                # è‚¡ç¥¨æ± ç­›é€‰
                if ts_codes is not None and 'ts_code' in long_df.columns:
                    long_df = long_df[long_df['ts_code'].isin(ts_codes)]

                raw_long_dfs[logical_name] = long_df

            except Exception as e:
                logger.error(f"å¤„ç†æ•°æ®é›† {logical_name} å¤±è´¥: {e}")
                raise ValueError(f"å¤„ç†æ•°æ®é›† {logical_name} å¤±è´¥: {e}")

        # --- 3. å°†æ‰€æœ‰æ•°æ®å¤„ç†æˆç»Ÿä¸€çš„é¢æ¿å®½è¡¨æ ¼å¼ ---
        trading_dates = self.get_trading_dates(buffer_start_date, end_date)
        for field in sorted(fields):
            logical_name = self.field_map.get(field)
            if not logical_name or logical_name not in raw_long_dfs:
                raise ValueError(f"æœªæ‰¾åˆ°æˆ–åŠ è½½å¤±è´¥: å­—æ®µ '{field}' çš„æ•°æ®æº '{logical_name}'")
            source_df = raw_long_dfs[logical_name]
            if 'trade_date' in source_df.columns:
                # a) å¯¹äºæœ¬èº«å°±æ˜¯æ¯æ—¥æ›´æ–°çš„é¢æ¿æ•°æ®
                df = source_df.copy()
                df['trade_date'] = pd.to_datetime(df['trade_date'])
                df = df[df['trade_date'].isin(trading_dates)]

                # æ˜ç¡®åœ°å®šä¹‰é‡å¤çš„é”®
                duplicate_keys = ['trade_date', 'ts_code']

                # åœ¨è½¬æ¢å‰ï¼Œå…ˆä½¿ç”¨ drop_duplicates è¿›è¡Œæ¸…æ´—
                # keep='last' æ˜¯ä¸€ä¸ªé‡è¦çš„é€‰æ‹©ï¼šæˆ‘ä»¬å‡å®šæ–‡ä»¶æœ«å°¾çš„è®°å½•æ˜¯æœ€æ–°çš„ã€æœ€å‡†ç¡®çš„
                unique_long_df = df.drop_duplicates(subset=duplicate_keys, keep='last')

                # ç¡®è®¤æ²¡æœ‰é‡å¤é¡¹åï¼Œå¯ä»¥å®‰å…¨åœ°è¿›è¡Œè½¬æ¢
                #  æ­¤æ—¶å¯ä»¥ç›´æ¥ä½¿ç”¨ pivot()ï¼Œå®ƒæ¯” pivot_table() ç•¥å¿«ï¼Œä¸”èƒ½å†æ¬¡éªŒè¯å”¯ä¸€æ€§
                field_for_origin =   self.fix_name_for_origin(field, logical_name)
                wide_df = unique_long_df.pivot(index='trade_date', columns='ts_code', values=field_for_origin)
            else:
                # b) å¯¹äºéœ€è¦â€œå¹¿æ’­â€åˆ°æ¯æ—¥çš„é™æ€å±æ€§æ•°æ® (å¦‚name, industry)
                logger.info(f"  æ­£åœ¨å°†é™æ€å­—æ®µ '{field}' å¹¿æ’­åˆ°æ¯æ—¥é¢æ¿...")
                static_series = source_df.drop_duplicates(subset=['ts_code']).set_index('ts_code')[field]

                # #  æ–¹å¼ï¼ˆ1ï¼‰ï¼šç›´æ¥å¹¿æ’­
                # for ts_code in wide_df.columns:
                #     # æ„é€ ç©º DataFrameï¼Œè¡Œæ˜¯æ—¥æœŸï¼Œåˆ—æ˜¯è‚¡ç¥¨ä»£ç 
                #     wide_df = pd.DataFrame(index=pd.DatetimeIndex(trading_dates), columns=static_series.index)
                #     wide_df[ts_code] = static_series[ts_code]
                # æ–¹å¼2 æ›´é«˜æ•ˆ ç±»ä¼¼é“ºç –
                ##
                # np.tile(A, (M, 1)) = æŠŠä¸€è¡Œæ•°ç»„ Aï¼Œé‡å¤ M è¡Œï¼Œä¸é‡å¤åˆ—ã€
                #
                # ä¹Ÿå°±æ˜¯è¯´ï¼š
                #
                # M æ§åˆ¶çš„æ˜¯â€œä½ æœ‰å¤šå°‘è¡Œâ€ï¼ˆè¡Œæ–¹å‘â€œé“ºç –â€ï¼‰
                #
                # 1 è¡¨ç¤ºâ€œåˆ—ä¸è¦æ‰©å±•â€ï¼ˆåªä¿ç•™åŸæ¥çš„è‚¡ç¥¨ç»´åº¦ï¼‰#
                wide_df = pd.DataFrame(
                    data=np.tile(static_series.values, (len(trading_dates), 1)),  # ä½¿ç”¨numpy.tileé«˜æ•ˆå¤åˆ¶æ•°æ®
                    index=trading_dates,
                    columns=static_series.index
                )
            raw_wide_dfs[field] = wide_df

        # å¯¹é½æ•°æ®
        aligned_data = self._align_dataframes(raw_wide_dfs)

        # aligned_data = self.rename_for_safe(aligned_data)
        return aligned_data #close â€”â€”raw å·²ç» hfq é€šè¿‡èšå®½ æ¯”å¯¹ æ•°æ®å®Œå…¨å¯¹ä¸Š

    def _align_dataframes(self, dfs: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:  # ok
        """
        ã€ä¿®å¤ç‰ˆã€‘å¯¹é½å¤šä¸ªDataFrame - ä»¥ä¸»è¦æ•°æ®è¡¨ä¸ºåŸºå‡†ï¼Œé¿å…è¿‡åº¦æ•°æ®ä¸¢å¤±

        Args:
            dfs: å­—æ®µåˆ°DataFrameçš„æ˜ å°„å­—å…¸

        Returns:
            å¯¹é½åçš„DataFrameå­—å…¸
        """
        if not dfs:
            raise ValueError("å±…ç„¶æ‰€ä¼ éœ€å¯¹é½æ•°æ®æ˜¯ç©ºçš„")

        # ã€ä¿®å¤ã€‘é€‰æ‹©åŸºå‡†è¡¨ - ä¼˜å…ˆé€‰æ‹©ä»·æ ¼æ•°æ®ï¼Œå…¶æ¬¡é€‰æ‹©è¦†ç›–åº¦æœ€é«˜çš„è¡¨
        primary_candidates = ['close_hfq', 'open_hfq',
                              'low_hfq']  # primary_candidates = ['close_raw', 'close', 'open_raw', 'open', 'high_raw', 'low_raw']
        base_key = None
        base_df = None

        # é¦–å…ˆå°è¯•æ‰¾åˆ°ä»·æ ¼æ•°æ®ä½œä¸ºåŸºå‡†
        for candidate in primary_candidates:
            if candidate in dfs:
                base_key = candidate
                base_df = dfs[candidate]
                break

        # å¦‚æœæ²¡æœ‰ä»·æ ¼æ•°æ®ï¼Œé€‰æ‹©è¦†ç›–åº¦æœ€é«˜çš„è¡¨
        if base_df is None:
            max_coverage = 0
            for name, df in dfs.items():
                coverage = df.notna().sum().sum()
                if coverage > max_coverage:
                    max_coverage = coverage
                    base_key = name
                    base_df = df

        logger.info(f"ğŸ“Š æ•°æ®å¯¹é½: ä½¿ç”¨ '{base_key}' ä½œä¸ºåŸºå‡†è¡¨ {base_df.shape}")

        target_dates = base_df.index
        target_stocks = base_df.columns

        # ã€ä¿®å¤ã€‘ä»¥åŸºå‡†è¡¨ä¸ºå‡†å¯¹é½æ‰€æœ‰æ•°æ®ï¼Œè€Œä¸æ˜¯å–äº¤é›†
        aligned_data = {}
        for name, df in dfs.items():
            aligned_df = df.reindex(index=target_dates, columns=target_stocks)
            aligned_df = aligned_df.sort_index()

            # ç»Ÿè®¡å¯¹é½åçš„è¦†ç›–åº¦
            total_cells = aligned_df.size
            valid_cells = aligned_df.notna().sum().sum()
            coverage = valid_cells / total_cells if total_cells > 0 else 0
            logger.info(f"  {name}: å¯¹é½åå½¢çŠ¶ {aligned_df.shape}, è¦†ç›–åº¦ {coverage:.1%}")

            # ä¸è¿›è¡Œå¡«å……ï¼Œä¿æŒåŸå§‹ç¼ºå¤±å€¼ï¼Œä¸Šå±‚DataManageré…åˆuniverseå†³å®šå¡«å……ç­–ç•¥
            aligned_data[name] = aligned_df

        logger.info(f"æ•°æ®å¯¹é½å®Œæˆ: {len(target_dates)}ä¸ªäº¤æ˜“æ—¥, {len(target_stocks)}åªè‚¡ç¥¨")
        return aligned_data

    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        self.cache = {}
        logger.info("æ•°æ®ç¼“å­˜å·²æ¸…é™¤")

    def extract_during_period(self, long_df, logical_name, start_date, end_date):
        """
        æ ¹æ®æ—¶é—´èŒƒå›´ç­›é€‰æ•°æ®

        Args:
            long_df: è¾“å…¥çš„DataFrame
            logical_name: æ•°æ®æ–‡ä»¶çš„é€»è¾‘åç§°
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            ç­›é€‰åçš„DataFrame
        """
        if 'trade_date' in long_df.columns:
            long_df['trade_date'] = pd.to_datetime(long_df['trade_date'])
            long_df = long_df[
                (long_df['trade_date'] >= pd.Timestamp(start_date)) &
                (long_df['trade_date'] <= pd.Timestamp(end_date))
                ]
            return long_df
        # elif logical_name == 'stock_basic.parquet':
        #     # å¯¹äºè‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼Œç­›é€‰ä¸Šå¸‚æ—¥æœŸæ—©äºå¼€å§‹æ—¥æœŸçš„è‚¡ç¥¨
        #     long_df['list_date'] = pd.to_datetime(long_df['list_date'])
        #     long_df = long_df[long_df['list_date'] < pd.Timestamp(start_date)]
        #
        #     # æ·»åŠ äº¤æ˜“æ—¥æœŸåˆ—ï¼Œä¾¿äºæ•°æ®ç»Ÿä¸€å¤„ç†
        #     trading_dates = get_trading_dates(start_date, end_date)#  å¾…ç¡®è®¤åˆ°åº•æ˜¯ éœ€è¦start_date end_dateæœŸé—´çš„äº¤æ˜“æ—¥ ï¼Œè¿˜æ˜¯è¿ç»­çš„æ¯æ—¥ ç¡®å®éœ€è¦è¿™æ ·ï¼
        #     # ä¸ºæ¯ä¸ªè‚¡ç¥¨åˆ›å»ºæ‰€æœ‰äº¤æ˜“æ—¥çš„è®°å½•
        #     stocks = long_df['ts_code'].unique()
        #     dates_df = pd.DataFrame(
        #         [(date, code) for date in trading_dates for code in stocks],
        #         columns=['trade_date', 'ts_code']
        #     )
        #     dates_df['trade_date'] = pd.to_datetime(dates_df['trade_date'])
        #
        #     # åˆå¹¶åŸºæœ¬ä¿¡æ¯åˆ°æ‰€æœ‰äº¤æ˜“æ—¥
        #     result_df = pd.merge(dates_df, long_df, on='ts_code', how='left')
        #     return result_df

        return long_df  # å¦‚æœæ²¡æœ‰æ—¥æœŸåˆ—ï¼Œè¿”å›åŸå§‹æ•°æ® åæ­£åé¢æœ‰ å¯¹é½ï¼

    def check_local_date_period_completeness_col(self, logical_name, df, col, start_date, end_date):
        df[col] = pd.to_datetime(df[col])
        min_date = df[col].min()
        max_date = df[col].max()
        start_date = pd.to_datetime(start_date)
        end_date = pd.to_datetime(end_date)
        if min_date > start_date:
            raise ValueError(
                f"[{logical_name}] æœ€æ—© trade_date = {min_date.date()} æ™šäº start_date = {start_date.date()} âŒ")
        if max_date < end_date:
            raise ValueError(
                f"[{logical_name}] æœ€æ™š trade_date = {max_date.date()} æ—©äº end_date = {end_date.date()} âŒ")
        print(f"[{logical_name}] æ—¥æœŸè¦†ç›–å®Œæ•´ âœ…")
        pass

    def check_local_date_period_completeness_for_namechange(self, logical_name, df, start_date, end_date):

        pass

    #
    # def rename_for_safe(self, aligned_data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
    #     """
    #     å¯¹åŠ è½½çš„æ•°æ®å­—å…¸è¿›è¡Œå®‰å…¨çš„é‡å‘½åï¼Œå°†é€šç”¨ä»·æ ¼å­—æ®µç»Ÿä¸€åŠ ä¸Š _raw åç¼€ã€‚
    #     ç¡®ä¿ä¸‹æ¸¸æ¨¡å—æ¥æ”¶åˆ°çš„æ˜¯å«ä¹‰æ˜ç¡®çš„æ•°æ®ã€‚
    #     """
    #     # åˆ›å»ºä¸€ä¸ªæ–°çš„å­—å…¸æ¥å­˜å‚¨ç»“æœï¼Œ é¿å…ä¿®æ”¹åŸå§‹ä¼ å…¥çš„å¯¹è±¡
    #     renamed_data = aligned_data.copy()
    #
    #     # å®šä¹‰éœ€è¦è¢«é‡å‘½åçš„ç›®æ ‡åˆ—
    #     cols_to_rename = ['close', 'open', 'high', 'low']
    #
    #     for old_name in cols_to_rename:
    #         # æ£€æŸ¥æ—§çš„åç§°æ˜¯å¦å­˜åœ¨äºå­—å…¸ä¸­
    #         if old_name in renamed_data:
    #             new_name = f"{old_name}_hfq"
    #             # ä½¿ç”¨ .pop() æ–¹æ³•ï¼Œå°†æ—§é”®çš„å€¼èµ‹ç»™æ–°é”®ï¼Œå¹¶ä»å­—å…¸ä¸­ç§»é™¤æ—§é”®
    #             renamed_data[new_name] = renamed_data.pop(old_name)
    #
    #
    #     ##
    #     # ä¸ºä»€ä¹ˆ amount (æˆäº¤é¢) è¦ç”¨ raw çš„ï¼Ÿ
    #     # ä¸€å¥è¯æ¦‚æ‹¬ï¼šå› ä¸ºamountï¼ˆæˆäº¤é¢ï¼‰æ˜¯ä¸€ä¸ªåä¹‰ä»·å€¼ï¼ˆNominal Valueï¼‰æŒ‡æ ‡ï¼Œå®ƒè¡¡é‡çš„æ˜¯â€œä»Šå¤©æœ‰å¤šå°‘é’±åœ¨äº¤æ˜“â€ï¼Œè€Œè¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆä¸å†å²ä¸Šçš„åˆ†çº¢é€è‚¡æ— å…³ã€‚#
    #     if 'amount' in renamed_data:
    #         renamed_data['amount'] = renamed_data.pop('amount')
    #
    #     return renamed_data
    def fix_name_for_origin(self, field, logical_name):
        if field.endswith('_hfq') & logical_name.endswith('_hfq'):
            return field.replace('_hfq', '')
        if field.endswith('_raw') & (logical_name == 'daily'):
            return field.replace('_raw', '')
        return field

    def fix_names_for_origin(self, columns_to_need_load, logical_name):
       return  [self.fix_name_for_origin(column,logical_name) for column in columns_to_need_load]

