# æ–‡ä»¶å: data_validator.py
# ä½œç”¨ï¼šä¸€ä¸ªç”Ÿäº§çº§çš„ã€å¥å£®çš„æ•°æ®å®Œæ•´æ€§æ£€éªŒå·¥å…·ã€‚
#      ç”¨äºå®¡è®¡ downloader.py ä¸‹è½½çš„æ•°æ®æ˜¯å¦å®Œæ•´ã€å‡†ç¡®ã€è‡ªæ´½ã€‚
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Optional, Tuple
import warnings
import json
from functools import reduce

# å‡è®¾æ‚¨çš„é¡¹ç›®ç»“æ„ï¼Œè¿™æ˜¯æ‚¨å¸¸é‡é…ç½®æ–‡ä»¶æ‰€åœ¨çš„ä½ç½®
# è¯·æ ¹æ®æ‚¨çš„å®é™…æƒ…å†µä¿®æ”¹
from quant_lib.config.constant_config import LOCAL_PARQUET_DATA_DIR, parquet_file_names, every_day_parquet_file_names
from quant_lib.utils import is_trading_day

warnings.filterwarnings('ignore')


class DataIntegrityChecker:
    """æ•°æ®å®Œæ•´æ€§æ£€éªŒå™¨ - ä¸“ä¸šå®šç‰ˆ"""

    def __init__(self, data_path: Optional[Path] = None, start_year: int = 2018):
        """
        åˆå§‹åŒ–æ£€éªŒå™¨
        Args:
            data_path: æ•°æ®å­˜å‚¨è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
            start_year: æ£€æŸ¥çš„èµ·å§‹å¹´ä»½
        """
        self.data_path = data_path or LOCAL_PARQUET_DATA_DIR

        self.start_day = pd.to_datetime('20180101')
        self.end_day = pd.Timestamp.today().normalize()
        self.end_day = pd.to_datetime('20250712')
        self._cache = {}  # ä¼˜åŒ–å»ºè®®ï¼šå¢åŠ ç±»å†…ç¼“å­˜ï¼Œé¿å…é‡å¤è¯»å–æ–‡ä»¶

        if not self.data_path.exists():
            raise FileNotFoundError(f"æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {self.data_path}")

        print(f"æ•°æ®å®Œæ•´æ€§æ£€éªŒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"æ•°æ®è·¯å¾„: {self.data_path}")
        print(f"æ£€éªŒæ—¶é—´èŒƒå›´: {self.start_day} - {self.end_day}")

    def _load_data(self, file_name: str, partitioned: bool = False) -> Optional[pd.DataFrame]:
        """
        ã€ä¼˜åŒ–ã€‘ä¸€ä¸ªå¸¦ç¼“å­˜çš„ã€ç»Ÿä¸€çš„æ•°æ®åŠ è½½æ–¹æ³•
        Args:
            file_name: ç›¸å¯¹äºæ•°æ®æ ¹ç›®å½•çš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹å
            partitioned: æ˜¯å¦æ˜¯æŒ‰å¹´ä»½åˆ†åŒºçš„ç›®å½•
        """
        if file_name in self._cache:
            return self._cache[file_name]

        file_path = self.data_path / file_name
        if file_path.exists():
            try:
                df = pd.read_parquet(file_path)
                self._cache[file_name] = df
                return df.copy()  # è¿”å›å‰¯æœ¬ä»¥é˜²æ„å¤–ä¿®æ”¹
            except Exception as e:
                print(f"  - è¯»å–å¤±è´¥: {file_name}, {e}")
                return None
        print(f"  - æ–‡ä»¶æˆ–ç›®å½•ä¸å­˜åœ¨: {file_name}")
        return None

    def check_basic_files(self) -> None:
        """æ£€æŸ¥åŸºç¡€æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        print("\n" + "=" * 50)
        print("1. åŸºç¡€æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥")
        print("=" * 50)

        basic_files = {
            'trade_cal.parquet': 'äº¤æ˜“æ—¥å†',
            'stock_basic.parquet': 'è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯',
            'namechange.parquet': 'è‚¡ç¥¨åç§°å˜æ›´å†å²'
        }

        for file_name, description in basic_files.items():
            df = self._load_data(file_name)
            exists = df is not None
            status = "âœ“ å­˜åœ¨" if exists else "âœ— ç¼ºå¤±"
            print(f"{description:15} {status}")

            if exists:
                print(f"  - è®°å½•æ•°: {len(df):,}")
                if file_name == 'trade_cal.parquet':
                    df['cal_date'] = pd.to_datetime(df['cal_date'])

                    mask = (df['cal_date'] >= self.start_day) & (df['cal_date'] <= self.end_day) & (df['is_open'] == 1)
                    df = df[mask]

                    print(f"  - äº¤æ˜“æ—¥æ•°é‡: {df.shape[0]:,}")
                    print(f"  - æ—¶é—´èŒƒå›´: {df['cal_date'].min()} ~ {df['cal_date'].max()}")
                elif file_name == 'stock_basic.parquet':
                    print(f"  - è‚¡ç¥¨æ•°é‡: {df['ts_code'].nunique():,}")

    def check_trading_days_completeness(self) -> None:
        """æ£€æŸ¥äº¤æ˜“æ—¥å®Œæ•´æ€§"""
        print("\n" + "=" * 50)
        print("2. äº¤æ˜“æ—¥å®Œæ•´æ€§æ£€æŸ¥")
        print("=" * 50)

        trade_cal = self._load_data('trade_cal.parquet')
        if trade_cal is None:
            raise ValueError("âœ— æ— æ³•åŠ è½½äº¤æ˜“æ—¥å†ï¼Œè·³è¿‡æ­¤é¡¹æ£€æŸ¥ã€‚")

        trade_cal['cal_date'] = pd.to_datetime(trade_cal['cal_date'])

        mask = (trade_cal['is_open'] == 1) & (trade_cal['cal_date'] >= self.start_day) & (
                trade_cal['cal_date'] <= self.end_day)
        expected_dates = set(trade_cal[mask]['cal_date'])

        for dataset in every_day_parquet_file_names:
            print(f"\næ£€æŸ¥ {dataset} çš„äº¤æ˜“æ—¥å®Œæ•´æ€§:")
            df = self._load_data(dataset)
            if df is None:
                continue

            actual_dates = set(pd.to_datetime(df['trade_date'].unique()))
            missing_days = expected_dates - actual_dates

            if not missing_days:
                print(f"  âœ“ [é€šè¿‡] æ‰€æœ‰ {len(expected_dates)} ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®å‡å­˜åœ¨ã€‚")
            else:
                print(f"  ğŸš¨ [å¤±è´¥] ç¼ºå¤±äº† {len(missing_days)} ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®ï¼")
                print(f"     ä¾‹å¦‚: {sorted([d.strftime('%Y-%m-%d') for d in list(missing_days)[:5]])}"
                      f"{'...' if len(missing_days) > 5 else ''}")

    def check_stock_coverage_robust(self, sample_size: int = 30) -> None:
        """ã€å…³é”®ä¿®æ­£ & é‡æ„ç‰ˆã€‘ä½¿ç”¨æ¯æ—¥æŠ½æ ·æ£€æŸ¥è‚¡ç¥¨è¦†ç›–åº¦çš„å®Œæ•´æ€§"""
        print("\n" + "=" * 50)
        print("3. è‚¡ç¥¨è¦†ç›–åº¦æ£€æŸ¥ (æ¯æ—¥éšæœºæŠ½æ ·)")
        print("=" * 50)

        # --- 1. ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰éœ€è¦çš„æ•°æ® ---
        stock_basic = self._load_data('stock_basic.parquet')
        price_df = self._load_data('daily_hfq')
        trade_cal = self._load_data('trade_cal.parquet')

        if stock_basic is None or price_df is None or trade_cal is None:
            print("âœ— ç¼ºå°‘stock_basic, daily_hfqæˆ–trade_calæ•°æ®ï¼Œè·³è¿‡æ­¤é¡¹æ£€æŸ¥ã€‚")
            return

        # --- 2. ä¸€æ¬¡æ€§è¿›è¡Œæ•°æ®ç±»å‹è½¬æ¢å’Œå‡†å¤‡ ---
        stock_basic['list_date'] = pd.to_datetime(stock_basic['list_date'])
        stock_basic['delist_date'] = pd.to_datetime(stock_basic['delist_date'])

        # ã€Bugä¿®å¤ã€‘å¼ºåˆ¶å°†æ—¥æœŸåˆ—è½¬ä¸ºå­—ç¬¦ä¸²å†è½¬ä¸ºdatetimeï¼Œç¡®ä¿èƒ½æ­£ç¡®è§£ææ•´æ•°æ—¥æœŸ
        price_df['trade_date'] = pd.to_datetime(price_df['trade_date'].astype(str))
        trade_cal['cal_date'] = pd.to_datetime(trade_cal['cal_date'].astype(str))

        # --- 3. å‡†å¤‡äº¤æ˜“æ—¥å†å’ŒæŠ½æ · ---
        trading_dates_series = trade_cal[trade_cal['is_open'] == 1]['cal_date'].sort_values()

        # åˆ›å»ºä¸€ä¸ª æ—¥æœŸ -> ä¸Šä¸€äº¤æ˜“æ—¥ çš„æ˜ å°„ï¼Œé¿å…åœ¨å¾ªç¯ä¸­é‡å¤æŸ¥æ‰¾
        prev_trade_date_map = trading_dates_series.shift(1).set_axis(trading_dates_series)

        all_trading_dates = price_df['trade_date'].unique()
        sample_dates = np.random.choice(all_trading_dates, min(sample_size, len(all_trading_dates)), replace=False)

        print(f"  å°†åœ¨ {len(sample_dates)} ä¸ªéšæœºäº¤æ˜“æ—¥ä¸Šè¿›è¡ŒæŠ½æ ·æ£€æŸ¥...")

        all_coverage = []
        # --- 4. æ¸…çˆ½ã€é«˜æ•ˆçš„å¾ªç¯ä½“ ---
        missing_stocks_ret = []
        for date in sorted(sample_dates):
            # è·å–ä¸Šä¸€äº¤æ˜“æ—¥
            prev_date = prev_trade_date_map.get(date)
            if pd.isna(prev_date):
                continue
            # è®¡ç®—å½“æ—¥çš„æœŸæœ›è‚¡ç¥¨æ± 
            expected_mask_today = (stock_basic['list_date'] <= date) & \
                                  ((stock_basic['delist_date'].isna()) | (stock_basic['delist_date'] > date)) \
                                  & (~stock_basic['ts_code'].str.endswith('.BJ'))
            expected_stocks_today = set(stock_basic[expected_mask_today]['ts_code'])

            # è®¡ç®—å½“æ—¥çš„å®é™…è‚¡ç¥¨æ± 
            actual_stocks_today = set(price_df[price_df['trade_date'] == date]['ts_code'])

            # è®¡ç®—è¦†ç›–ç‡
            coverage = len(actual_stocks_today) / len(expected_stocks_today) if expected_stocks_today else 1.0
            all_coverage.append(coverage)

            # æ‰“å°è¯Šæ–­ä¿¡æ¯
            if coverage < 1:
                print(
                    f"  {date}: âš  è¦†ç›–ç‡è¾ƒä½: {coverage:.2%}ï¼Œç¼ºå¤± {len(expected_stocks_today) - len(actual_stocks_today)} åªè‚¡ç¥¨")

                # --- æ›´æ¸…æ™°çš„è¯Šæ–­é€»è¾‘ ---
                missing_stocks = expected_stocks_today - actual_stocks_today
                missing_stocks_ret.append(missing_stocks)
                if missing_stocks:
                    print(f"    -> ç¼ºå¤±è‚¡ç¥¨ç¤ºä¾‹: {list(missing_stocks)[:3]}")

                    # æ£€æŸ¥è¿™äº›ç¼ºå¤±çš„è‚¡ç¥¨ï¼Œæ˜¨å¤©æ˜¯å¦å­˜åœ¨
                    prev_day_actual_stocks = set(price_df[price_df['trade_date'] == prev_date]['ts_code'])
                    existed_yesterday = set(missing_stocks).intersection(prev_day_actual_stocks)
                    print(f'ä»Šå¤©ç¼ºå¤±æ•°é‡{len(missing_stocks)},æ˜¨å¤©å­˜åœ¨çš„æ•°é‡{existed_yesterday}')
                    if len(existed_yesterday) == len(missing_stocks):
                        print('ä»Šå¤©æ²¡æœ‰çš„ï¼Œæ˜¨å¤©å…¨æœ‰')

        coommen_miss_ts_codes = reduce(lambda a, b: set(a) & set(b), missing_stocks_ret)
        print("æŠ½æ ·10å¤©éƒ½ç¼ºå¤±çš„è‚¡ç¥¨:", coommen_miss_ts_codes)

        # --- 5. æœ€ç»ˆæŠ¥å‘Š ---
        avg_coverage = np.mean(all_coverage) if all_coverage else 0
        if avg_coverage < 0.98:
            print(f"\n  ğŸš¨ [è­¦å‘Š] éšæœºæŠ½æ ·æ—¥çš„å¹³å‡è¦†ç›–ç‡ä¸º {avg_coverage:.2%}, å¯èƒ½å­˜åœ¨è‚¡ç¥¨ç¼ºå¤±é—®é¢˜(ä¸»è¦ä¸ºåœç‰Œ)ã€‚")
        else:
            print(f"\n  âœ“ [é€šè¿‡] éšæœºæŠ½æ ·æ—¥çš„å¹³å‡è¦†ç›–ç‡ä¸º {avg_coverage:.2%}, è¦†ç›–åº¦è¾ƒé«˜ã€‚")

    def check_metric_consistency(self) -> None:
        """ã€æ–°å¢ã€‘æ£€éªŒå…³é”®æŒ‡æ ‡çš„äº¤å‰ä¸€è‡´æ€§"""
        print("\n" + "=" * 50)
        print("4. å…³é”®æŒ‡æ ‡äº¤å‰éªŒè¯ (å¸‚å€¼)")
        print("=" * 50)

        daily_basic = self._load_data('daily_basic')
        if daily_basic is None:
            print("âœ— ç¼ºå°‘daily_basicæ•°æ®ï¼Œè·³è¿‡æ­¤é¡¹æ£€æŸ¥ã€‚")
            return

        df = daily_basic.copy()
        df = df[df['total_mv'] > 0]

        # æ³¨æ„å•ä½: total_share(ä¸‡è‚¡), total_mv(ä¸‡å…ƒ), close(å…ƒ)
        df['calculated_mv'] = df['close'] * df['total_share']
        df['mv_diff_ratio'] = (df['total_mv'] - df['calculated_mv']).abs() / df['total_mv']

        high_diff_records_ratio = (df['mv_diff_ratio'] > 0.01).mean()

        print(f"  â€œæ€»å¸‚å€¼â€ä¸â€œæ”¶ç›˜ä»·Ã—æ€»è‚¡æœ¬â€å·®å¼‚å¤§äº1%çš„è®°å½•å æ¯”: {high_diff_records_ratio:.2%}")

        if high_diff_records_ratio > 0.01:
            print(f"  ğŸš¨ [è­¦å‘Š] å¸‚å€¼äº¤å‰éªŒè¯ï¼štotal_mvä¸'close*total_share'å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼")
            print(f"     è¿™é€šå¸¸æ˜¯ç”±äºæœªå¯¹`close`ä»·æ ¼è¿›è¡Œå¤æƒå¤„ç†å¯¼è‡´çš„ã€‚")
        else:
            print(f"  âœ“ [é€šè¿‡] å¸‚å€¼äº¤å‰éªŒè¯é€šè¿‡ï¼Œæ•°æ®å†…éƒ¨ä¸€è‡´æ€§è¾ƒé«˜ã€‚")

    def run_all_checks(self, save_path: Optional[str] = 'data_integrity_report.txt') -> None:
        """æ‰§è¡Œæ‰€æœ‰æ£€éªŒå¹¶ç”ŸæˆæŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print(f"æ•°æ®å®Œæ•´æ€§æ£€æŸ¥æŠ¥å‘Š ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})")
        print("=" * 60)

        self.check_basic_files()
        self.check_trading_days_completeness()
        self.check_stock_coverage_robust()
        self.check_metric_consistency()

        print("\n" + "=" * 60)
        print("æ‰€æœ‰æ£€æŸ¥å®Œæˆï¼è¯·æŸ¥çœ‹ä¸Šè¿°è¾“å‡ºäº†è§£æ•°æ®å®Œæ•´æ€§æƒ…å†µã€‚")
        print("=" * 60)


if __name__ == '__main__':
    # åˆ›å»ºæ£€éªŒå™¨å®ä¾‹
    checker = DataIntegrityChecker()

    # æ‰§è¡Œæ‰€æœ‰æ£€æŸ¥
    checker.run_all_checks()
