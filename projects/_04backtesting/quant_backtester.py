"""
é‡åŒ–å›æµ‹å™¨ - ä¸“ä¸šçº§å› å­ç­–ç•¥å›æµ‹å·¥å…·

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. å¤šå› å­ç­–ç•¥å›æµ‹å¯¹æ¯”
2. çœŸå®äº¤æ˜“æˆæœ¬å»ºæ¨¡
3. å®Œæ•´çš„é£é™©è°ƒæ•´æ”¶ç›Šåˆ†æ
4. å¯è§†åŒ–å’ŒæŠ¥å‘Šç”Ÿæˆ
5. å®ç›˜çº§æ•°æ®å¯¹é½å’ŒéªŒè¯

è®¾è®¡ç†å¿µï¼š
- é¢å‘å¯¹è±¡ï¼Œå¯æ‰©å±•
- æ•°æ®å®‰å…¨ï¼Œä¸¥æ ¼å¯¹é½éªŒè¯
- äº¤æ˜“æˆæœ¬çœŸå®å»ºæ¨¡
- ç»“æœå¯å¤ç°ï¼Œå¯è§£é‡Š
"""

import pandas as pd
import numpy as np
import vectorbt as vbt
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass
from pathlib import Path
import warnings
from datetime import datetime

from vectorbt.portfolio import CallSeqType

from quant_lib.config.logger_config import setup_logger
from quant_lib.rebalance_utils import generate_rebalance_dates
from utils.math.math_utils import convert_to_sequential_percents

logger = setup_logger(__name__)
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False


@dataclass
class BacktestConfig:
    """å›æµ‹é…ç½®å‚æ•°"""
    # ç­–ç•¥å‚æ•°
    top_quantile: float = 0.2  # åšå¤šåˆ†ä½æ•°é˜ˆå€¼ï¼ˆå‰20%ï¼‰
    rebalancing_freq: str = 'W'  # è°ƒä»“é¢‘ç‡ ('M'=æœˆæœ«, 'W'=å‘¨æœ«, 'Q'=å­£æœ«)

    # äº¤æ˜“æˆæœ¬å‚æ•°
    commission_rate: float = 0.0003  # ä½£é‡‘è´¹ç‡ï¼ˆä¸‡3ï¼‰
    slippage_rate: float = 0.0010  # æ»‘ç‚¹ç‡ï¼ˆåƒ1ï¼‰
    stamp_duty: float = 0.0010  # å°èŠ±ç¨ï¼ˆå•è¾¹ï¼Œå–å‡ºæ”¶å–ï¼‰
    min_commission: float = 5.0  # æœ€å°ä½£é‡‘ï¼ˆå…ƒï¼‰

    # å›æµ‹å‚æ•°
    initial_cash: float = 1000000.0  # åˆå§‹èµ„é‡‘ï¼ˆ100ä¸‡ï¼‰
    max_positions: int = 10  # æœ€å¤§æŒä»“æ•°é‡

    # é£æ§å‚æ•°
    max_weight_per_stock: float = 0.10  # å•è‚¡æœ€å¤§æƒé‡ï¼ˆ10%ï¼‰
    min_weight_threshold: float = 0.01  # æœ€å°æƒé‡é˜ˆå€¼ï¼ˆ1%ï¼‰

    # æ•°æ®éªŒè¯å‚æ•°
    min_data_coverage: float = 0.8  # æœ€å°æ•°æ®è¦†ç›–ç‡
    max_missing_consecutive_days: int = 5  # æœ€å¤§è¿ç»­ç¼ºå¤±å¤©æ•°


class DataValidator:
    """æ•°æ®éªŒè¯å™¨"""

    @staticmethod
    def validate_dataframes(price_df: pd.DataFrame, *factor_dfs: pd.DataFrame) -> Dict[str, any]:
        """
        éªŒè¯ä»·æ ¼æ•°æ®å’Œå› å­æ•°æ®çš„ä¸€è‡´æ€§
        
        Args:
            price_df: ä»·æ ¼æ•°æ®
            factor_dfs: å› å­æ•°æ®åˆ—è¡¨
            
        Returns:
            Dict: éªŒè¯ç»“æœç»Ÿè®¡
        """
        validation_results = {
            'is_valid': True,
            'errors': [],
            'warnings': [],
            'stats': {}
        }

        try:
            # æ£€æŸ¥ç´¢å¼•ç±»å‹
            if not isinstance(price_df.index, pd.DatetimeIndex):
                validation_results['errors'].append("ä»·æ ¼æ•°æ®ç´¢å¼•å¿…é¡»æ˜¯DatetimeIndex")
                validation_results['is_valid'] = False

            # æ£€æŸ¥æ•°æ®å¯¹é½
            reference_index = price_df.index
            reference_columns = price_df.columns

            for i, factor_df in enumerate(factor_dfs):
                factor_name = f"å› å­{i + 1}"

                # ç´¢å¼•å¯¹é½æ£€æŸ¥
                if not factor_df.index.equals(reference_index):
                    missing_dates = reference_index.difference(factor_df.index)
                    if len(missing_dates) > 0:
                        validation_results['warnings'].append(
                            f"{factor_name}ç¼ºå¤±{len(missing_dates)}ä¸ªäº¤æ˜“æ—¥"
                        )

                # åˆ—å¯¹é½æ£€æŸ¥
                if not factor_df.columns.equals(reference_columns):
                    missing_stocks = reference_columns.difference(factor_df.columns)
                    if len(missing_stocks) > 0:
                        validation_results['warnings'].append(
                            f"{factor_name}ç¼ºå¤±{len(missing_stocks)}åªè‚¡ç¥¨"
                        )

            # ç»Ÿè®¡ä¿¡æ¯
            validation_results['stats'] = {
                'date_range': (price_df.index.min(), price_df.index.max()),
                'trading_days': len(price_df.index),
                'stock_count': len(price_df.columns),
                'data_coverage': (1 - price_df.isnull().sum().sum() / price_df.size) * 100
            }

            logger.info(f"æ•°æ®éªŒè¯å®Œæˆ: {validation_results['stats']}")

        except Exception as e:
            validation_results['errors'].append(f"éªŒè¯è¿‡ç¨‹å¼‚å¸¸: {str(e)}")
            validation_results['is_valid'] = False

        return validation_results

    @staticmethod
    def align_dataframes(price_df: pd.DataFrame, *factor_dfs: pd.DataFrame) -> Tuple[pd.DataFrame, ...]:
        """
        å¯¹é½ä»·æ ¼æ•°æ®å’Œå› å­æ•°æ®
        
        Args:
            price_df: ä»·æ ¼æ•°æ®
            factor_dfs: å› å­æ•°æ®åˆ—è¡¨
            
        Returns:
            Tuple: å¯¹é½åçš„æ•°æ®æ¡†åˆ—è¡¨
        """
        # ä½¿ç”¨vectorbtçš„å¯¹é½åŠŸèƒ½
        aligned_data = vbt.base.reshape_fns.broadcast(price_df, *factor_dfs,
                                                      keep_pd=True,
                                                      align_index=True,
                                                      align_columns=True)

        logger.info(f"æ•°æ®å¯¹é½å®Œæˆï¼Œæœ€ç»ˆç»´åº¦: {aligned_data[0].shape}")
        return aligned_data


class StrategySignalGenerator:
    """ç­–ç•¥ä¿¡å·ç”Ÿæˆå™¨"""

    @staticmethod
    def generate_long_signals(
            factor_df: pd.DataFrame,
            config: BacktestConfig
    ) -> pd.DataFrame:
        """
        ç”Ÿæˆåšå¤šä¿¡å·

        Args:
            factor_df: å› å­æ•°æ®
            config: å›æµ‹é…ç½®

        Returns:
            pd.DataFrame: æŒä»“ä¿¡å·ï¼ˆTrue=æŒæœ‰ï¼ŒFalse=ä¸æŒæœ‰ï¼‰
        """
        # 1. è®¡ç®—æ¯æ—¥æ’åç™¾åˆ†ä½ï¼ˆè¶Šå¤§æ’åè¶Šé å‰ï¼‰
        ranks = factor_df.rank(axis=1, pct=True, method='average', na_option='keep')

        # 2. ç¡®å®šåšå¤šä¿¡å·ï¼ˆæ’ååœ¨å‰top_quantileï¼‰
        long_signals_raw = ranks >= (1 - config.top_quantile)

        # 3. æŒ‰è°ƒä»“é¢‘ç‡è¿›è¡Œé‡é‡‡æ ·
        # .resample().last() è·å–æ¯ä¸ªå‘¨æœŸæœ€åä¸€å¤©çš„ä¿¡å·
        # .reindex().ffill() å°†ä¿¡å·å‰å¡«å……åˆ°æ¯ä¸ªäº¤æ˜“æ—¥
        rebalance_signals = long_signals_raw.resample(config.rebalancing_freq).last()
        final_signals = rebalance_signals.reindex(factor_df.index, method='ffill')

        # 4. æ§åˆ¶æœ€å¤§æŒä»“æ•°é‡
        if config.max_positions > 0:
            final_signals = StrategySignalGenerator._limit_positions(
                final_signals, ranks, config.max_positions
            )

        logger.info(f"ä¿¡å·ç”Ÿæˆå®Œæˆï¼Œå¹³å‡æŒä»“æ•°: {final_signals.sum(axis=1).mean():.1f}")
        return final_signals.fillna(False)

    @staticmethod
    def _limit_positions(signals: pd.DataFrame, ranks: pd.DataFrame, max_positions: int) -> pd.DataFrame:
        """
        é™åˆ¶æœ€å¤§æŒä»“æ•°é‡ï¼Œä¼˜å…ˆé€‰æ‹©æ’åæœ€é«˜çš„è‚¡ç¥¨
        
        Args:
            signals: åŸå§‹ä¿¡å·
            ranks: æ’åæ•°æ®
            max_positions: æœ€å¤§æŒä»“æ•°
            
        Returns:
            pd.DataFrame: é™åˆ¶åçš„ä¿¡å·
        """
        limited_signals = pd.DataFrame(False, index=signals.index, columns=signals.columns)

        for date in signals.index:
            date_signals = signals.loc[date]
            date_ranks = ranks.loc[date]

            if date_signals.sum() > max_positions:
                # é€‰æ‹©æ’åæœ€é«˜çš„max_positionsåªè‚¡ç¥¨
                valid_stocks = date_signals[date_signals].index
                top_stocks = date_ranks[valid_stocks].nlargest(max_positions).index
                limited_signals.loc[date, top_stocks] = True
            else:
                limited_signals.loc[date] = date_signals

        return limited_signals



    @staticmethod
    def generate_long_holding_signals(factor_df: pd.DataFrame, price_df, config: BacktestConfig) -> pd.DataFrame:
        """
        ç”Ÿæˆæ¯æ—¥ç›®æ ‡"æŒä»“"å¸ƒå°”çŸ©é˜µï¼Œç¡®ä¿æ»¡ä»“è¿ä½œ
        """
        # è®¡ç®—æ¯æ—¥æ’å
        ranks = factor_df.rank(axis=1, pct=True, method='average', na_option='keep')

        # ç”Ÿæˆæ¯æ—¥æŒä»“ä¿¡å·ï¼Œè€Œä¸æ˜¯åªåœ¨è°ƒä»“æ—¥
        daily_holding_signals = pd.DataFrame(False, index=factor_df.index, columns=factor_df.columns)
        # è·å–è°ƒä»“æ—¥æœŸ
        rebalance_dates = ranks.copy().reindex(generate_rebalance_dates(ranks.index,config.rebalancing_freq)).dropna(how='all').index

        # å½“å‰æŒä»“ç»„åˆï¼ˆåœ¨è°ƒä»“é—´éš”æœŸé—´ä¿æŒä¸å˜ï¼‰
        current_positions = None

        for date in factor_df.index:
            # æ£€æŸ¥æ˜¯å¦ä¸ºè°ƒä»“æ—¥
            is_rebalance_day = date in rebalance_dates

            if is_rebalance_day:
                # è°ƒä»“æ—¥ï¼šé‡æ–°é€‰æ‹©è‚¡ç¥¨
                daily_valid_ranks = ranks.loc[date].dropna()

                if len(daily_valid_ranks) > 0:
                    # è®¡ç®—ç›®æ ‡æŒä»“æ•°
                    num_to_select = int(np.ceil(len(daily_valid_ranks) * config.top_quantile))
                    if config.max_positions:
                        num_to_select = min(num_to_select, config.max_positions)

                    # é€‰æ‹©æ’åæœ€é«˜çš„è‚¡ç¥¨
                    chosen_stocks = daily_valid_ranks.nlargest(num_to_select).index
                    current_positions = chosen_stocks
                    # logger.info(f"è°ƒä»“æ—¥{date.strftime('%Y-%m-%d')}: é€‰æ‹©{len(chosen_stocks)}åªè‚¡ç¥¨")

            if current_positions is not None: #å…¶å®å°±æ˜¯å˜ç›¸çš„ffill ï¼Œä¿æŒè¿™æ¬¡è°ƒä»“åŠåé¢nå¤©åŒçŠ¶æ€ ï¼Œç›´åˆ°ä¸‹ä¸€æ¬¡è°ƒä»“ï¼
                #æœ€æ–°æ³¨é‡Šï¼Œäº¤ç»™ä¸‹æ¸¸ å»åˆ¤æ–­
                # # æ£€æŸ¥è‚¡ç¥¨ æ˜¯å¦å¯äº¤æ˜“==>ï¼ˆæœ‰ä»·æ ¼æ•°æ®ï¼‰
                # current_with_price_positions = price_df.loc[date, current_positions].notna()
                # tradable_positions = current_positions[current_with_price_positions]
                daily_holding_signals.loc[date, current_positions] = True

        # éªŒè¯æŒä»“ä¿¡å·è´¨é‡
        daily_positions = daily_holding_signals.sum(axis=1)
        avg_positions = daily_positions.mean()
        zero_position_days = (daily_positions == 0).sum()

        logger.info(f"  å¹³å‡æ¯æ—¥æŒä»“æ•°: {avg_positions:.1f}")
        logger.info(f"  é›¶æŒä»“å¤©æ•°: {zero_position_days}/{len(daily_positions)}")
        logger.info(f"  æŒä»“è¦†ç›–ç‡: {(1 - zero_position_days / len(daily_positions)):.1%}")

        if zero_position_days > len(daily_positions) * 0.1:  # è¶…è¿‡10%çš„æ—¥å­æ²¡æœ‰æŒä»“
            logger.warning(f"âš ï¸ æŒä»“ä¿¡å·è´¨é‡å·®ï¼š{zero_position_days}å¤©æ— æŒä»“")
        else:
            logger.info(f"âœ… æŒä»“ä¿¡å·è´¨é‡è‰¯å¥½")

        return daily_holding_signals

    @staticmethod
    def generate_rebalancing_signals(holding_signals: pd.DataFrame, force_exit_limit: int = None) -> Tuple[
        pd.DataFrame, pd.DataFrame]:
        """
        å°†"æŒä»“"çŸ©é˜µè½¬æ¢ä¸ºç²¾ç¡®çš„"ä¹°å…¥"å’Œ"å–å‡º"ä¿¡å·çŸ©é˜µã€‚
        - å¢åŠ äº†å¼ºåˆ¶ç±»å‹è½¬æ¢ï¼Œå½»åº•è§£å†³ 'invert' ufunc TypeError é—®é¢˜ã€‚
        - æ–°å¢60å¤©å¼ºåˆ¶å–å‡ºé€»è¾‘ æ…ç”¨ï¼ ä¿æŒå‡½æ•°å•ä¸€
        """

        # ã€æ ¸å¿ƒä¿®æ­£ã€‘åœ¨ä½¿ç”¨ ~ æ“ä½œç¬¦ä¹‹å‰ï¼Œè¿›è¡Œä¸¥æ ¼çš„ç±»å‹å’Œç©ºå€¼å¤„ç†

        # 1. ç¡®ä¿å½“å‰æŒä»“ä¿¡å·æ˜¯å¸ƒå°”å‹
        current_holdings = holding_signals.astype(bool)

        # 2. å¯¹å‰ä¸€å¤©çš„æŒä»“ä¿¡å·ï¼Œå…ˆå¡«å……ç§»ä½äº§ç”Ÿçš„NaNï¼Œå†å¼ºåˆ¶è½¬ä¸ºå¸ƒå°”å‹
        prev_holdings = holding_signals.vbt.fshift(1).fillna(False).astype(bool)

        # 3. åŸºç¡€ä¹°å–ä¿¡å·
        entries = current_holdings & ~prev_holdings
        exits = ~current_holdings & prev_holdings

        # å¼ºåˆ¶å–å‡ºé€»è¾‘ - ç”¨äºè°ƒè¯•äº¤æ˜“æ‰§è¡Œé—®é¢˜
        forced_exits=None
        # logger.info("  -> æ­£åœ¨æ·»åŠ 60å¤©å¼ºåˆ¶å–å‡ºé€»è¾‘...")
        if force_exit_limit:
            # åˆ›å»ºæŒä»“å¤©æ•°è®¡æ•°å™¨
            holding_days = pd.DataFrame(0, index=holding_signals.index, columns=holding_signals.columns)
            forced_exits = pd.DataFrame(False, index=holding_signals.index, columns=holding_signals.columns)
            # forå¾ªç¯ å¡«å……å¤©æ•°ï¼ ä»¥åŠåˆ¤æ–­å¤©æ•°è¶…è¿‡limit ç»™å‡ºå¼ºåˆ¶å–å‡ºä¿¡å·
            for i in range(1, len(holding_signals)):
                # å¯¹äºæŒç»­æŒæœ‰çš„è‚¡ç¥¨ï¼Œå¤©æ•°+1
                continuing_holds = current_holdings.iloc[i] & prev_holdings.iloc[i]
                holding_days.iloc[i] = np.where(continuing_holds,
                                                holding_days.iloc[i - 1] + 1,
                                                0)

                # å¯¹äºæ–°ä¹°å…¥çš„è‚¡ç¥¨ï¼Œå¤©æ•°é‡ç½®ä¸º1
                new_entries = entries.iloc[i]  # è¿™ä¸ªä¸ºtrueï¼Œé‚£ä¹ˆæ˜¨å¤©ä¸€å®šæ˜¯falseï¼Œæ²¡æ¯›ç—…
                holding_days.iloc[i] = np.where(new_entries, 1, holding_days.iloc[i])

                # å¼ºåˆ¶å–å‡ºæŒæœ‰è¶…è¿‡180å¤©çš„è‚¡ç¥¨ (180å¤©)
                force_exit_mask = holding_days.iloc[i] >= force_exit_limit
                forced_exits.iloc[i] = force_exit_mask & current_holdings.iloc[i]
                logger.info(f"  -> å¼ºåˆ¶è¶…è¿‡{force_exit_limit}å¤©å–å‡ºè§¦å‘æ¬¡æ•°: {forced_exits.sum().sum()}")

        final_exits = exits
        # 5. åˆå¹¶åŸæœ‰å–å‡ºä¿¡å·å’Œå¼ºåˆ¶å–å‡ºä¿¡å·
        if force_exit_limit:
            final_exits = exits | forced_exits
        return entries, final_exits


class TradingCostCalculator:
    """äº¤æ˜“æˆæœ¬è®¡ç®—å™¨ - é‡æ„ç‰ˆæœ¬"""

    @staticmethod
    def get_single_side_costs(config: BacktestConfig) -> Dict[str, float]:
        """
        è·å–å•è¾¹äº¤æ˜“æˆæœ¬
        
        Args:
            config: å›æµ‹é…ç½®
            
        Returns:
            Dict: å•è¾¹æˆæœ¬å­—å…¸
        """
        # åŸºç¡€å•è¾¹æˆæœ¬ (ä½£é‡‘)
        base_commission = config.commission_rate

        # å•è¾¹æ»‘ç‚¹
        single_slippage = config.slippage_rate

        # å°èŠ±ç¨åªåœ¨å–å‡ºæ—¶æ”¶å–ï¼Œæˆ‘ä»¬å°†å…¶åˆ†æ‘Šåˆ°ä¹°å–ä¸¤è¾¹
        # æˆ–è€…åŒ…å«åœ¨ä¸€ä¸ªç¨é«˜çš„ç»¼åˆè´¹ç‡ä¸­
        adjusted_commission = base_commission + (config.stamp_duty / 2)  # åˆ†æ‘Šå°èŠ±ç¨

        costs = {
            'commission': adjusted_commission,
            'slippage': single_slippage,
            'combined_fee': adjusted_commission  # vectorbtçš„feeså‚æ•°
        }

        logger.info(f"å•è¾¹äº¤æ˜“æˆæœ¬: ä½£é‡‘{adjusted_commission:.4f}, æ»‘ç‚¹{single_slippage:.4f}")
        return costs


class QuantBacktester:
    """é‡åŒ–å›æµ‹å™¨ä¸»ç±»"""

    def __init__(self, config: Optional[BacktestConfig] = None):
        """
        åˆå§‹åŒ–å›æµ‹å™¨
        
        Args:
            config: å›æµ‹é…ç½®ï¼ŒNoneæ—¶ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or BacktestConfig()
        self.validator = DataValidator()
        self.signal_generator = StrategySignalGenerator()
        self.cost_calculator = TradingCostCalculator()

        # å­˜å‚¨å›æµ‹ç»“æœ
        self.portfolios: Dict[str, any] = {}
        self.validation_results: Dict = {}

        logger.info("QuantBacktesteråˆå§‹åŒ–å®Œæˆ")
        logger.info(f"é…ç½®å‚æ•°: {self.config}")

    def prepare_data(
            self,
            price_df: pd.DataFrame,
            factor_dict: Dict[str, pd.DataFrame]
    ) -> Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]:
        """
        å‡†å¤‡å’ŒéªŒè¯å›æµ‹æ•°æ®
        
        Args:
            price_df: ä»·æ ¼æ•°æ®
            factor_dict: å› å­æ•°æ®å­—å…¸ {"å› å­å": DataFrame}
            
        Returns:
            Tuple: (å¯¹é½åçš„ä»·æ ¼æ•°æ®, å¯¹é½åçš„å› å­æ•°æ®å­—å…¸)
        """
        logger.info(f"å¼€å§‹å‡†å¤‡æ•°æ®ï¼Œä»·æ ¼æ•°æ®ç»´åº¦: {price_df.shape}")
        logger.info(f"å› å­æ•°é‡: {len(factor_dict)}")

        # éªŒè¯æ•°æ®
        factor_dfs = list(factor_dict.values())
        self.validation_results = self.validator.validate_dataframes(price_df, *factor_dfs)

        if not self.validation_results['is_valid']:
            raise ValueError(f"æ•°æ®éªŒè¯å¤±è´¥: {self.validation_results['errors']}")

        if self.validation_results['warnings']:
            for warning in self.validation_results['warnings']:
                logger.warning(warning)

        # å¯¹é½æ•°æ®
        aligned_data = self.validator.align_dataframes(price_df, *factor_dfs)
        aligned_price = aligned_data[0]
        aligned_factors = {
            name: aligned_data[i + 1]
            for i, name in enumerate(factor_dict.keys())
        }

        logger.info(f"æ•°æ®å‡†å¤‡å®Œæˆï¼Œæœ€ç»ˆç»´åº¦: {aligned_price.shape}")
        return aligned_price, aligned_factors

    def _generate_improved_signals(self, holding_signals, price_df, max_holding_days=None):
        """
        ç”Ÿæˆæ”¹è¿›çš„ä¹°å–ä¿¡å·ï¼Œç¡®ä¿äº¤æ˜“èƒ½æ­£å¸¸å…³é—­
        Args:
            holding_signals: æŒä»“ä¿¡å·çŸ©é˜µ
            price_df: ä»·æ ¼æ•°æ®
            max_holding_days: æœ€å¤§æŒä»“å¤©æ•°
        Returns:
            Tuple: (ä¹°å…¥ä¿¡å·, å–å‡ºä¿¡å·)
        """
        logger.info(f"æ”¹è¿›å–å‡ºä¿¡å· - æ»¡æœ€å¤§æŒä»“å¤©æ•°å¼ºåˆ¶å–: {max_holding_days}")
        entries = pd.DataFrame(False, index=holding_signals.index, columns=holding_signals.columns)
        exits = pd.DataFrame(False, index=holding_signals.index, columns=holding_signals.columns)

        # æŒä»“å¤©æ•°è®¡æ•°å™¨
        holding_days = pd.DataFrame(0, index=holding_signals.index, columns=holding_signals.columns)
        not_finishied_exit = None
        for i in range(len(holding_signals)):
            if i == 0:
                # ç¬¬ä¸€å¤©: ç›´æ¥ä¹°å…¥ç›®æ ‡è‚¡ç¥¨
                entries.iloc[i] = holding_signals.iloc[i]
                holding_days.iloc[i] = np.where(entries.iloc[i], 1, 0)
            else:
                prev_holdings = holding_signals.iloc[i - 1]
                curr_holdings = holding_signals.iloc[i]

                new_entries = curr_holdings & ~prev_holdings
                entries.iloc[i] = new_entries

                # æ­£å¸¸å–å‡ºä¿¡å·
                today_need_exit = self.today_need_exit(prev_holdings, curr_holdings, not_finishied_exit)
                today_can_exit = today_need_exit &  (price_df.iloc[i].notna())#æœ‰ä»·æ ¼æ‰èƒ½å–
                #check çœ‹çœ‹ä»Šå¤©ä»·æ ¼åœ¨ä¸åœ¨ï¼Œä»·æ ¼ä¸åœ¨ å–ä¸å‡ºå»ï¼
                not_finishied_exit = today_need_exit & (price_df.iloc[i].isna()) #ä»Šå¤©éœ€è¦å–çš„ï¼Œå–ä¸èµ°çš„è¯ï¼Œæ˜å¤©å–ï¼
                exits.iloc[i] = today_can_exit
                if max_holding_days is None:
                    continue
                # éœ€è¦åˆ¤æ–­æŒä»“å¤©æ•°
                continuing_holds = curr_holdings & prev_holdings #æ˜¨å¤©åœ¨åœºï¼Œä»Šå¤©ä¹Ÿåœ¨
                holding_days.iloc[i] = np.where(continuing_holds,
                                                holding_days.iloc[i - 1] + 1,
                                                0)
                holding_days.iloc[i] = np.where(new_entries, 1, holding_days.iloc[i]) #å¾ˆå¯¹ é€šè¿‡æµ‹è¯•

                # å¼ºåˆ¶é€€å‡º - æŒæœ‰è¶…è¿‡æœ€å¤§å¤©æ•°
                today_need_force_exit_mask = (holding_days.iloc[i] >= max_holding_days) & curr_holdings#ç®—ä¸Šä»Šå¤©æŒä»“ï¼Œå½“å¥½æ˜¯45å¤©ï¼Œä»Šå¤©è¯¥å–äº†ï¼
                today_can_force_exit_mask = today_need_force_exit_mask &  (price_df.iloc[i].notna())#æœ‰ä»·æ ¼æ‰èƒ½å–

                # check çœ‹çœ‹ä»Šå¤©ä»·æ ¼åœ¨ä¸åœ¨ï¼Œä»·æ ¼ä¸åœ¨ å–ä¸å‡ºå»ï¼
                not_finishied_exit = (today_need_force_exit_mask & (price_df.iloc[i].isna())) | not_finishied_exit  # ä»Šå¤©éœ€è¦å–çš„ï¼Œå–ä¸èµ°çš„è¯ï¼Œæ˜å¤©å–ï¼
                # åˆå¹¶é€€å‡ºä¿¡å·
                exits.iloc[i] = today_can_exit | today_can_force_exit_mask

        # åœ¨æœ€åä¸€ä¸ªäº¤æ˜“æ—¥å¼ºåˆ¶æ¸…ä»“æ‰€æœ‰æŒä»“
        last_day_holdings = holding_signals.iloc[-1]
        exits.iloc[-1] = exits.iloc[-1] | last_day_holdings

        logger.info(f"æ”¹è¿›ä¿¡å·ç”Ÿæˆå®Œæˆ:ä¹°å…¥ä¿¡å·: {entries.sum().sum()} æ€»å–å‡ºä¿¡å·: {exits.sum().sum()} --  è¾¾åˆ°æœ€é•¿æŒæœ‰å¼ºåˆ¶é€€å‡ºæ¬¡æ•°: ({((holding_days >= max_holding_days) & holding_signals).sum().sum()}) --æœ€åä¸€æ—¥æ¸…ä»“ï¼š({last_day_holdings.sum()}) ")
        return entries, exits

    def run_backtest(
            self,
            price_df: pd.DataFrame,
            factor_dict: Dict[str, pd.DataFrame]
    ) -> Dict[str, any]:
        # 1. æ•°æ®å‡†å¤‡
        all_dfs = [price_df] + list(factor_dict.values())
        aligned_dfs = vbt.base.reshape_fns.broadcast(*all_dfs, keep_pd=True, align_index=True, align_columns=True)

        aligned_price = aligned_dfs[0]
        aligned_factors = {name: df for name, df in zip(factor_dict.keys(), aligned_dfs[1:])}
        logger.info(f"æ•°æ®å¯¹é½å®Œæˆï¼Œæœ€ç»ˆç»´åº¦: {aligned_price.shape}")
        # 2. é€ä¸ªå› å­å›æµ‹
        for factor_name, factor_data in aligned_factors.items():
            logger.info(f"ğŸš€ å¼€å§‹å›æµ‹å› å­: {factor_name}")
            # 3. ä¿¡å·ç”Ÿæˆæµæ°´çº¿
            # é¦–å…ˆï¼Œç”Ÿæˆæ¯æ—¥çš„ç›®æ ‡æŒä»“çŠ¶æ€ å…¨æ˜¯true false è¡¨ç¤ºå½“æ—¥rankæƒ…å†µçš„true flase
            holding_signals = self.signal_generator.generate_long_holding_signals(factor_data, aligned_price,
                                                                                  self.config)

            origin_weights_df = self.get_position_weights_by_per_weight(holding_signals)
            self.myself_debug_data(origin_weights_df)
            #ç…§é¡¾vector ä¸“é—¨ä¸ºä»–ç®—æœ¯ï¼
            weights_df = convert_to_sequential_percents(origin_weights_df)
            # è®¡ç®—åˆç†çš„ç»¼åˆäº¤æ˜“è´¹ç”¨
            # ä¹°å…¥æˆæœ¬: ä½£é‡‘(ä¸‡3) + æ»‘ç‚¹(åƒ1) = 0.0003 + 0.001 = 0.0013
            # å–å‡ºæˆæœ¬: ä½£é‡‘(ä¸‡3) + å°èŠ±ç¨(åƒ1) + æ»‘ç‚¹(åƒ1) = 0.0003 + 0.001 + 0.001 = 0.0023
            # å¹³å‡åŒè¾¹æˆæœ¬: (0.0013 + 0.0023) / 2 = 0.0018
            comprehensive_fee_rate = (
                    self.config.commission_rate +  # ä½£é‡‘ 0.0003
                    self.config.slippage_rate +  # æ»‘ç‚¹ 0.001
                    self.config.stamp_duty / 2  # å°èŠ±ç¨åˆ†æ‘Š 0.0005
            )
            # æ”¹è¿›é€€å‡ºä¿¡å·ç”Ÿæˆ - ç¡®ä¿åœ¨æ—¶é—´çª—å£ç»“æŸæ—¶å¼ºåˆ¶é€€å‡º (è¿™æ ·åšï¼Œåªæ˜¯ä¸ºäº†ç®€å•ç›´è§‚çœ‹å‡ºæˆ‘çš„ç­–ç•¥æ•ˆæœï¼
            improved_entries, improved_exits = self._generate_improved_signals(
                holding_signals, aligned_price, max_holding_days=30
            )
            # ã€æ–°å¢è°ƒè¯•ã€‘æ£€æŸ¥ä¿¡å·çš„è¯¦ç»†æƒ…å†µ
            self.debug_signal_generation(holding_signals, self.config, improved_entries, improved_exits, origin_weights_df,0,len(holding_signals)-1)

            # 1. æ£€æŸ¥å®é™…çš„äº¤æ˜“è®°å½•
            portfolio = vbt.Portfolio.from_signals(
                call_seq='auto',  # first sell then buy å®æµ‹! å¿…é¡»é…ç½®ï¼
                group_by=True,  # å¿…é¡»é…ç½®
                cash_sharing=True,  # å¿…é¡»é…ç½®

                size_type="percent",  # å®æµ‹ï¼ æŒä»“é‡‘é¢ä¸ºç™¾åˆ†æ¯”
                size=weights_df,

                #è‡ªå®šä¹‰df ä»·æ ¼æ•°æ®
                close=aligned_price,
                #ä¿¡å·
                entries=improved_entries,
                exits=improved_exits,

                #äº¤æ˜“è¿‡ç¨‹ä¸­æŒ‡æ ‡
                init_cash=self.config.initial_cash,
                fees=comprehensive_fee_rate,
                freq='D' #å½¢å®¹ä»·æ ¼çš„
            )
            # 3. æ£€æŸ¥æŒä»“è®°å½•
            trades = portfolio.positions.records_readable
            expected_trades = improved_entries.sum().sum()
            logger.info(f"  æœŸæœ›äº¤æ˜“æ•°: {expected_trades}")
            logger.info(f"  å®é™…äº¤æ˜“æ•°: {len(trades)}")
            print(portfolio.stats())

            self.plot_cumulative_returns_curve(portfolio)
            self.portfolios[factor_name] = portfolio

        logger.info(f"ğŸ‰ {factor_dict.keys()}å› å­å›æµ‹å®Œæˆ")

        return self.portfolios

    def _recalculate_trade_metric(self, corrected_stats, trades, metric):
        """é‡æ–°è®¡ç®—ç‰¹å®šçš„äº¤æ˜“æŒ‡æ ‡"""
        # ã€ä¿®å¤ã€‘æ­£ç¡®è¿‡æ»¤å·²å…³é—­äº¤æ˜“ - Statuså¯èƒ½æ˜¯å­—ç¬¦ä¸²'Closed'æˆ–æ•´æ•°1
        if 'Status' in trades.columns:
            status_values = trades['Status'].unique()
            if 'Closed' in status_values:
                closed_trades = trades[trades['Status'] == 'Closed']
            elif 1 in status_values:
                closed_trades = trades[trades['Status'] == 1]
            else:
                # å¦‚æœStatuså€¼æœªçŸ¥ï¼Œå‡è®¾æ‰€æœ‰äº¤æ˜“éƒ½å·²å…³é—­
                closed_trades = trades
        else:
            closed_trades = trades

        if len(closed_trades) > 0:
            winning_trades = closed_trades[closed_trades['PnL'] > 0]
            losing_trades = closed_trades[closed_trades['PnL'] < 0]

            if metric == 'Win Rate [%]':
                win_rate = len(winning_trades) / len(closed_trades) * 100
                corrected_stats[metric] = win_rate
            elif metric == 'Best Trade [%]':
                corrected_stats[metric] = closed_trades['Return'].max() * 100
            elif metric == 'Worst Trade [%]':
                corrected_stats[metric] = closed_trades['Return'].min() * 100
            elif metric == 'Avg Winning Trade [%]':
                if len(winning_trades) > 0:
                    corrected_stats[metric] = winning_trades['Return'].mean() * 100
                else:
                    corrected_stats[metric] = 0.0
            elif metric == 'Avg Losing Trade [%]':
                if len(losing_trades) > 0:
                    corrected_stats[metric] = losing_trades['Return'].mean() * 100
                else:
                    corrected_stats[metric] = 0.0
            elif metric == 'Expectancy':
                corrected_stats[metric] = closed_trades['PnL'].mean()
        else:
            corrected_stats[metric] = 0.0


    def get_comparison_table(self, metrics: Optional[List[str]] = None) -> pd.DataFrame:
        """
        ç”Ÿæˆå› å­å¯¹æ¯”è¡¨
        
        Args:
            metrics: è¦å¯¹æ¯”çš„æŒ‡æ ‡åˆ—è¡¨
            
        Returns:
            pd.DataFrame: å¯¹æ¯”ç»“æœè¡¨
        """
        if not self.portfolios:
            raise ValueError("è¯·å…ˆè¿è¡Œå›æµ‹")

        if metrics is None:
            metrics = [
                'Total Return [%]',
                'Sharpe Ratio',
                'Calmar Ratio',
                'Max Drawdown [%]',
                'Win Rate [%]',
                'Profit Factor'
            ]

        comparison_data = {}
        for factor_name, portfolio in self.portfolios.items():
            stats = portfolio.stats()
            comparison_data[factor_name] = stats[metrics]

        comparison_df = pd.DataFrame(comparison_data).T
        logger.info("å› å­å¯¹æ¯”è¡¨ç”Ÿæˆå®Œæˆ")
        return comparison_df

    def plot_cumulative_returns(self,
                                figsize: Tuple[int, int] = (15, 8),
                                save_path: Optional[str] = None) -> None:
        """
        ç»˜åˆ¶ç´¯ç§¯æ”¶ç›Šç‡æ›²çº¿
        
        Args:
            figsize: å›¾ç‰‡å¤§å°
            save_path: ä¿å­˜è·¯å¾„
        """
        if not self.portfolios:
            raise ValueError("è¯·å…ˆè¿è¡Œå›æµ‹")

        plt.figure(figsize=figsize)

        for factor_name, portfolio in self.portfolios.items():
            returns = portfolio.returns()
            cumulative_returns = (1 + returns).cumprod()
            plt.plot(cumulative_returns.index, cumulative_returns.values,
                     label=factor_name, linewidth=2)

        plt.title('å› å­ç­–ç•¥ç´¯ç§¯æ”¶ç›Šç‡å¯¹æ¯”', fontsize=16, fontweight='bold')
        plt.xlabel('æ—¥æœŸ', fontsize=12)
        plt.ylabel('ç´¯ç§¯æ”¶ç›Šç‡', fontsize=12)
        plt.legend(fontsize=11)
        plt.grid(True, alpha=0.3)
        plt.xticks(rotation=45)

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"å›¾è¡¨å·²ä¿å­˜: {save_path}")

        plt.show()

    def plot_drawdown_analysis(self,
                               figsize: Tuple[int, int] = (15, 10),
                               save_path: Optional[str] = None) -> None:
        """
        ç»˜åˆ¶å›æ’¤åˆ†æå›¾
        
        Args:
            figsize: å›¾ç‰‡å¤§å°
            save_path: ä¿å­˜è·¯å¾„
        """
        if not self.portfolios:
            raise ValueError("è¯·å…ˆè¿è¡Œå›æµ‹")

        n_factors = len(self.portfolios)
        fig, axes = plt.subplots(n_factors, 1, figsize=figsize, sharex=True)
        if n_factors == 1:
            axes = [axes]

        for i, (factor_name, portfolio) in enumerate(self.portfolios.items()):
            drawdown = portfolio.drawdown()
            axes[i].fill_between(drawdown.index, drawdown.values, 0,
                                 color='red', alpha=0.3)
            axes[i].set_title(f'{factor_name} - å›æ’¤åˆ†æ', fontsize=14)
            axes[i].set_ylabel('å›æ’¤ (%)', fontsize=12)
            axes[i].grid(True, alpha=0.3)

        plt.xlabel('æ—¥æœŸ', fontsize=12)
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"å›æ’¤å›¾å·²ä¿å­˜: {save_path}")

        plt.show()

    def generate_full_report(self,
                             report_dir: str = "backtest_reports") -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„å›æµ‹æŠ¥å‘Š
        
        Args:
            report_dir: æŠ¥å‘Šä¿å­˜ç›®å½•
            
        Returns:
            str: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        if not self.portfolios:
            raise ValueError("è¯·å…ˆè¿è¡Œå›æµ‹")

        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        report_path = Path(report_dir)
        report_path.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ç”Ÿæˆå¯¹æ¯”è¡¨
        comparison_df = self.get_comparison_table()
        comparison_file = report_path / f"factor_comparison_{timestamp}.csv"
        comparison_df.to_csv(comparison_file, encoding='utf-8-sig')

        # ç”Ÿæˆå›¾è¡¨
        returns_chart = report_path / f"cumulative_returns_{timestamp}.png"
        self.plot_cumulative_returns(save_path=str(returns_chart))

        drawdown_chart = report_path / f"drawdown_analysis_{timestamp}.png"
        self.plot_drawdown_analysis(save_path=str(drawdown_chart))

        # ç”Ÿæˆè¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š
        report_file = report_path / f"detailed_report_{timestamp}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("é‡åŒ–ç­–ç•¥å›æµ‹è¯¦ç»†æŠ¥å‘Š\n")
            f.write("=" * 80 + "\n\n")

            f.write("å›æµ‹é…ç½®:\n")
            f.write(f"  è°ƒä»“é¢‘ç‡: {self.config.rebalancing_freq}\n")
            f.write(f"  åšå¤šåˆ†ä½: {self.config.top_quantile:.1%}\n")
            f.write(f"  åˆå§‹èµ„é‡‘: {self.config.initial_cash:,.0f}\n")
            f.write(f"  äº¤æ˜“è´¹ç‡: {TradingCostCalculator.calculate_total_fees(self.config):.4f}\n\n")

            f.write("æ•°æ®éªŒè¯ç»“æœ:\n")
            for key, value in self.validation_results['stats'].items():
                f.write(f"  {key}: {value}\n")
            f.write("\n")

            f.write("å› å­å¯¹æ¯”ç»“æœ:\n")
            f.write(comparison_df.to_string())
            f.write("\n\n")

        logger.info(f"å®Œæ•´æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        return str(report_path)

    def get_position_weights_by_per_weight(self, holding_signals):

        # 1. ã€æ–°å¢ã€‘åˆ›å»ºç›®æ ‡æƒé‡çŸ©é˜µ
        # 1.1 è®¡ç®—æ¯æ—¥åº”æŒæœ‰çš„è‚¡ç¥¨æ€»æ•°
        num_positions = holding_signals.sum(axis=1)

        # 1.2 è®¡ç®—æ¯æ—¥çš„ç­‰æƒæƒé‡ (ä¾‹å¦‚ï¼ŒæŒæœ‰10åªè‚¡ç¥¨ï¼Œæ¯åªæƒé‡ä¸º 1/10 = 0.1)
        #     ä¸ºé¿å…é™¤ä»¥é›¶ (åœ¨æ²¡æœ‰æŒä»“çš„æ—¥å­)ï¼Œä½¿ç”¨ .replace(np.inf, 0)
        target_weights = (1 / num_positions).replace([np.inf, -np.inf], 0)

        # 1.3 å°†æ¯æ—¥æƒé‡å€¼å¹¿æ’­åˆ°å½“å¤©çš„æŒä»“è‚¡ç¥¨ä¸Šï¼Œå½¢æˆä¸€ä¸ªä¸ aholding_signals å½¢çŠ¶ç›¸åŒçš„æƒé‡çŸ©é˜µ
        #     ä¸æŒæœ‰(False)çš„è‚¡ç¥¨ï¼Œæƒé‡è‡ªç„¶ä¸º 0
        weights_df = holding_signals.mul(target_weights, axis=0)
        return weights_df

    def debug_signal_generation(self, holding_signals, config,entry_signals, exit_signals,weights_df,sidx,eidx):
        logger.info("ğŸ” ä¿¡å·è°ƒè¯•åˆ†æå¼€å§‹")
        # æ£€æŸ¥å‰å‡ å¤©çš„ä¿¡å·æƒ…å†µ
        sample_dates = generate_rebalance_dates(holding_signals.index,config.rebalancing_freq)

        # --- æ ¸å¿ƒæ”¹è¿›ï¼šå‘é‡åŒ–è®¡ç®—æ¯æ—¥çš„â€œå®é™…æŒä»“æ•°é‡â€ ---
        # 1. è®¡ç®—æ¯æ—¥æŒä»“æ•°é‡çš„â€œå‡€å˜åŒ–â€
        position_net_change = entry_signals.astype(int) - exit_signals.astype(int)
        # 2. ä½¿ç”¨ç´¯ç§¯æ±‚å’Œï¼Œå¾—åˆ°æ¯æ—¥ç»ˆç‚¹çš„å®é™…æŒä»“æ•°é‡
        actual_positions_count = position_net_change.cumsum(axis=0).sum(axis=1)
        # ----------------------------------------------------
        sample_dates = holding_signals.index[sidx:eidx]

        for date in sample_dates:
            # â€œç†æƒ³â€çš„è®¡åˆ’æŒä»“æ•°
            intended_holdings_count = holding_signals.loc[date].sum()
            # å½“å¤©å®é™…å‘ç”Ÿçš„äº¤æ˜“
            entry_count = entry_signals.loc[date].sum()
            exit_count = exit_signals.loc[date].sum()

            # å½“å¤©æ”¶ç›˜åçš„â€œç°å®â€æŒä»“æ•°
            actual_holding_count = actual_positions_count.loc[date]

            log_msg = (
                f"{date.strftime('%Y-%m-%d')}: "
                f"è®¡åˆ’æŒä»“({intended_holdings_count}), "
                f"å®é™…æŒä»“({actual_holding_count}), "
                f"å–å‡º({exit_count}), "
                f"ä¹°å…¥({entry_count})"
            )
            logger.info(log_msg)

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä¿¡å·éƒ½æ˜¯False
        total_entries = entry_signals.sum().sum()
        total_exits = exit_signals.sum().sum()

        if total_entries == 0 or total_exits==0:
            raise ValueError("âŒ ä¸¥é‡é—®é¢˜ï¼šæ²¡æœ‰ç”Ÿæˆä»»ä½•ä¹°å…¥ä¿¡å·ï¼ æˆ–è€… æ²¡æœ‰ä»»ä½•å–å‡ºä¿¡å·")
        logger.info(f"âœ… ä¿¡å·ç”Ÿæˆæ­£å¸¸: ä¹°å…¥{total_entries}ä¸ª, å–å‡º{total_exits}ä¸ª")



        logger.info(f"  å¹³å‡æ¯å¤©æŒä»“è‚¡ç¥¨æ•°é‡: {holding_signals.sum(axis=1).mean()}")
        ##æ£€æŸ¥æŒä»“æƒé‡ æ˜¯å¦ç­‰äº1
        logger.info(f"  æ¯å¤©å¹³å‡æŒä»“æ¯”ä¾‹: {weights_df.sum(axis=1).mean()}")
        self._debug_holding_days(holding_signals, entry_signals, exit_signals)

    def about_cash(self, portfolio):
        logger.info(f"ç°é‡‘å˜åŒ–æƒ…å†µ:")
        cash_flows  = portfolio.cash()
        initial_cash = float(cash_flows.iloc[0])
        final_cash = float(cash_flows.iloc[-1])
        pass

    def plot_cumulative_returns_curve(self,portfolio):
        cumulative_returns_builtin = (1 + portfolio.returns()).cumprod() - 1

        # ä½¿ç”¨å†…ç½®å‡½æ•°è¿˜æœ‰ä¸€ä¸ªå·¨å¤§çš„å¥½å¤„ï¼šå¯ä»¥ç›´æ¥è°ƒç”¨ vbt çš„ç»˜å›¾åŠŸèƒ½
        print("\næ­£åœ¨ç»˜åˆ¶æƒç›Šæ›²çº¿...")
        cumulative_returns_builtin.vbt.plot(title='Equity Curve').show()

    def myself_debug_data(self, origin_weights_df):
        #æŒ‰åˆ— æ•´åˆ—è‡³å°‘æœ‰ä¸€ä¸ªå€¼ä¸ä¸º0ï¼
        origin_weights_df = origin_weights_df.loc[:, origin_weights_df.any(axis=0)]
        pass

    def _debug_holding_days(self, holding_signals, entry_signals, exit_signals):
        """
        åˆ†ææŒä»“å¤©æ•°åˆ†å¸ƒï¼Œè¯†åˆ«"è€å¦–è‚¡"ï¼ˆé•¿æœŸæŒæœ‰çš„è‚¡ç¥¨ï¼‰
        
        Args:
            holding_signals: æŒä»“ä¿¡å·çŸ©é˜µ
            entry_signals: ä¹°å…¥ä¿¡å·çŸ©é˜µ  
            exit_signals: å–å‡ºä¿¡å·çŸ©é˜µ
        """
        logger.info("ğŸ•µï¸ å¼€å§‹åˆ†ææŒä»“å¤©æ•°åˆ†å¸ƒ...")
        
        # åˆ›å»ºæŒä»“å¤©æ•°ç»Ÿè®¡å­—å…¸
        stock_holding_stats = {}
        all_holding_periods = []
        
        # éå†æ¯åªè‚¡ç¥¨
        for stock in holding_signals.columns:
            stock_entries = entry_signals[stock]
            stock_exits = exit_signals[stock]
            stock_holdings = holding_signals[stock]
            
            # æ‰¾åˆ°æ‰€æœ‰ä¹°å…¥æ—¶ç‚¹
            entry_dates = stock_entries[stock_entries].index.tolist()
            exit_dates = stock_exits[stock_exits].index.tolist()
            
            if len(entry_dates) == 0:
                continue
                
            # è®¡ç®—æ¯æ¬¡æŒä»“å‘¨æœŸ
            holding_periods = []
            
            for entry_date in entry_dates:
                # æ‰¾åˆ°å¯¹åº”çš„å–å‡ºæ—¥æœŸ
                matching_exits = [exit_date for exit_date in exit_dates if exit_date > entry_date]
                
                if matching_exits:
                    exit_date = min(matching_exits)  # æœ€è¿‘çš„å–å‡ºæ—¥æœŸ
                    # è®¡ç®—æŒä»“å¤©æ•°
                    holding_days = (exit_date - entry_date).days
                    holding_periods.append(holding_days)
                    all_holding_periods.append(holding_days)
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å–å‡ºä¿¡å·ï¼Œè®¡ç®—åˆ°æœ€åä¸€å¤©çš„æŒä»“å¤©æ•°
                    last_date = holding_signals.index[-1]
                    holding_days = (last_date - entry_date).days
                    holding_periods.append(holding_days)
                    all_holding_periods.append(holding_days)
            
            if holding_periods:
                stock_holding_stats[stock] = {
                    'total_trades': len(holding_periods),
                    'min_holding_days': min(holding_periods),
                    'max_holding_days': max(holding_periods),
                    'avg_holding_days': np.mean(holding_periods),
                    'holding_periods': holding_periods
                }
        
        if not all_holding_periods:
            logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æŒä»“è®°å½•")
            return
            
        # æ•´ä½“ç»Ÿè®¡
        total_trades = len(all_holding_periods)
        avg_holding = np.mean(all_holding_periods)
        median_holding = np.median(all_holding_periods)
        max_holding = max(all_holding_periods)
        min_holding = min(all_holding_periods)
        
        logger.info(f"ğŸ“Š æŒä»“å¤©æ•°æ€»ä½“ç»Ÿè®¡:")
        logger.info(f"  æ€»äº¤æ˜“æ¬¡æ•°: {total_trades}")
        logger.info(f"  å¹³å‡æŒä»“å¤©æ•°: {avg_holding:.1f}å¤©")
        logger.info(f"  ä¸­ä½æ•°æŒä»“å¤©æ•°: {median_holding:.1f}å¤©")
        logger.info(f"  æœ€çŸ­æŒä»“: {min_holding}å¤©")
        logger.info(f"  æœ€é•¿æŒä»“: {max_holding}å¤©")
        
        # æŒä»“å¤©æ•°åˆ†å¸ƒ
        bins = [0, 7, 30, 60, 120, 240, float('inf')]
        bin_labels = ['<7å¤©', '7-30å¤©', '30-60å¤©', '60-120å¤©', '120-240å¤©', '>240å¤©']
        
        for i, (bin_start, bin_end) in enumerate(zip(bins[:-1], bins[1:])):
            if bin_end == float('inf'):
                count = sum(1 for days in all_holding_periods if days >= bin_start)
            else:
                count = sum(1 for days in all_holding_periods if bin_start <= days < bin_end)
            
            percentage = count / total_trades * 100
            logger.info(f"  {bin_labels[i]}: {count}æ¬¡ ({percentage:.1f}%)")
        
        # æ‰¾å‡º"è€å¦–è‚¡" - æŒä»“è¶…è¿‡120å¤©çš„è‚¡ç¥¨
        long_holding_threshold = 120
        old_monster_stocks = []
        
        for stock, stats in stock_holding_stats.items():
            max_days = stats['max_holding_days']
            if max_days >= long_holding_threshold:
                old_monster_stocks.append({
                    'stock': stock,
                    'max_holding_days': max_days,
                    'avg_holding_days': stats['avg_holding_days'],
                    'total_trades': stats['total_trades']
                })
        
        # æŒ‰æœ€é•¿æŒä»“å¤©æ•°æ’åº
        old_monster_stocks.sort(key=lambda x: x['max_holding_days'], reverse=True)
        
        if old_monster_stocks:
            logger.info(f"ğŸ‰ å‘ç°{len(old_monster_stocks)}åªè€å¦–è‚¡ (æŒä»“>{long_holding_threshold}å¤©):")
            
            # æ˜¾ç¤ºå‰10åªæœ€"å¦–"çš„è‚¡ç¥¨
            top_monsters = old_monster_stocks[:10]
            for i, stock_info in enumerate(top_monsters, 1):
                logger.info(f"  {i:2d}. {stock_info['stock']}: æœ€é•¿{stock_info['max_holding_days']}å¤©, "
                           f"å¹³å‡{stock_info['avg_holding_days']:.1f}å¤©, å…±{stock_info['total_trades']}æ¬¡äº¤æ˜“")
                
            if len(old_monster_stocks) > 10:
                logger.info(f"  ... è¿˜æœ‰{len(old_monster_stocks) - 10}åªè€å¦–è‚¡")
                
            # è¶…çº§å¦–è‚¡ - æŒä»“è¶…è¿‡240å¤©
            super_monsters = [s for s in old_monster_stocks if s['max_holding_days'] >= 240]
            if super_monsters:
                logger.info(f"ğŸ‘¹ å…¶ä¸­{len(super_monsters)}åªè¶…çº§å¦–è‚¡ (æŒä»“>240å¤©):")
                for stock_info in super_monsters:
                    logger.info(f"     {stock_info['stock']}: {stock_info['max_holding_days']}å¤©")
        else:
            logger.info(f"âœ… æ²¡æœ‰å‘ç°è€å¦–è‚¡ (æ‰€æœ‰è‚¡ç¥¨æŒä»“éƒ½<{long_holding_threshold}å¤©)")
        
        logger.info("ğŸ•µï¸ æŒä»“å¤©æ•°åˆ†æå®Œæˆ")

    def today_need_exit(self, prev_holdings, curr_holdings, not_finishied_exit):
        today_exit_signal =  ~curr_holdings & prev_holdings
        if not_finishied_exit is not None: #æ˜¨å¤©æ²¡å–å‡ºå»ï¼Œä»Šå¤©èµ¶ç´§å–ï¼
            today_exit_signal = today_exit_signal | not_finishied_exit

        return today_exit_signal


# ä¾¿æ·å‡½æ•°
def quick_factor_backtest(
        price_df: pd.DataFrame,
        factor_dict: Dict[str, pd.DataFrame],
        config: Optional[BacktestConfig] = None
) -> Tuple[Dict[str, any], pd.DataFrame]:
    """
    å¿«é€Ÿå› å­å›æµ‹å‡½æ•°
    
    Args:
        price_df: ä»·æ ¼æ•°æ®
        factor_dict: å› å­æ•°æ®å­—å…¸
        config: å›æµ‹é…ç½®
        
    Returns:
        Tuple: (å›æµ‹ç»“æœå­—å…¸, å¯¹æ¯”è¡¨)
    """
    backtester = QuantBacktester(config)
    portfolios = backtester.run_backtest(price_df, factor_dict)
    comparison_table = backtester.get_comparison_table()

    return portfolios, comparison_table


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    logger.info("QuantBacktester ç¤ºä¾‹è¿è¡Œ")

    # è¿™é‡Œéœ€è¦ä½ æä¾›çœŸå®çš„æ•°æ®
    # price_df = load_price_data()
    # factor_dict = {
    #     'volatility_40d': load_factor_data('volatility_40d'),
    #     'composite_factor': load_factor_data('composite_factor')
    # }

    # # é…ç½®å›æµ‹å‚æ•°
    # config = BacktestConfig(
    #     top_quantile=0.2,
    #     rebalancing_freq='M',
    #     commission_rate=0.0003,
    #     slippage_rate=0.001
    # )

    # # è¿è¡Œå›æµ‹
    # backtester = QuantBacktester(config)
    # portfolios = backtester.run_backtest(price_df, factor_dict)

    # # ç”Ÿæˆå¯¹æ¯”å’ŒæŠ¥å‘Š
    # comparison_table = backtester.get_comparison_table()
    # print(comparison_table)

    # backtester.plot_cumulative_returns()
    # report_path = backtester.generate_full_report()

    logger.info("QuantBacktester ç¤ºä¾‹å®Œæˆ")
