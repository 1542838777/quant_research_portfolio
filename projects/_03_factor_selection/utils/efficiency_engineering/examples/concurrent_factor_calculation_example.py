"""
å¹¶å‘å› å­è®¡ç®—ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ConcurrentExecutor è¿›è¡Œé«˜æ•ˆçš„æ‰¹é‡å› å­è®¡ç®—
"""

import pandas as pd
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.append(str(project_root))

from projects._03_factor_selection.utils.efficiency_engineering.concurrent_executor import (
    run_concurrent_factors,
    FactorCalculationExecutor,
    ConcurrentConfig
)
from quant_lib.config.logger_config import setup_logger

logger = setup_logger(__name__)


def example_basic_concurrent_execution():
    """åŸºç¡€å¹¶å‘æ‰§è¡Œç¤ºä¾‹"""
    logger.info("=== åŸºç¡€å¹¶å‘æ‰§è¡Œç¤ºä¾‹ ===")
    
    # è¯»å–å› å­åˆ—è¡¨
    factor_file = r'/projects/_03_factor_selection/factor_manager/selector/v3æœªç»è¿‡æ®‹å·®åŒ–ç‰ˆæœ¬.csv'
    df = pd.read_csv(factor_file)
    factor_names = df['factor_name'].unique().tolist()[:5]  # æµ‹è¯•å‰5ä¸ªå› å­
    
    snapshot_config_id = '20250825_091622_98ed2d08'
    
    # ä½¿ç”¨ä¾¿æ·å‡½æ•°è¿›è¡Œå¹¶å‘æ‰§è¡Œ
    successful_results, failed_factors = run_concurrent_factors(
        factor_names=factor_names,
        snapshot_config_id=snapshot_config_id,
        max_workers=3,
        execution_mode="single"
    )
    
    logger.info(f"æˆåŠŸè®¡ç®—: {len(successful_results)} ä¸ªå› å­")
    logger.info(f"å¤±è´¥å› å­: {len(failed_factors)} ä¸ª")


def example_custom_config_execution():
    """è‡ªå®šä¹‰é…ç½®å¹¶å‘æ‰§è¡Œç¤ºä¾‹"""
    logger.info("=== è‡ªå®šä¹‰é…ç½®å¹¶å‘æ‰§è¡Œç¤ºä¾‹ ===")
    
    # è¯»å–å› å­åˆ—è¡¨
    factor_file = r'/projects/_03_factor_selection/factor_manager/selector/v3æœªç»è¿‡æ®‹å·®åŒ–ç‰ˆæœ¬.csv'
    df = pd.read_csv(factor_file)
    factor_names = df['factor_name'].unique().tolist()[:8]
    
    snapshot_config_id = '20250825_091622_98ed2d08'
    
    # è‡ªå®šä¹‰å¹¶å‘é…ç½®
    config = ConcurrentConfig(
        max_workers=6,      # æ›´å¤šå¹¶å‘çº¿ç¨‹
        timeout=7200,       # 30åˆ†é’Ÿè¶…æ—¶
        retry_count=3,      # æ›´å¤šé‡è¯•æ¬¡æ•°
        log_interval=5      # æ›´é¢‘ç¹çš„è¿›åº¦æ—¥å¿—
    )
    
    # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®çš„æ‰§è¡Œå™¨
    executor = FactorCalculationExecutor(config)
    
    successful_results, failed_factors = executor.execute_factor_batch(
        factor_names=factor_names,
        snapshot_config_id=snapshot_config_id
    )
    
    logger.info(f"è‡ªå®šä¹‰é…ç½®æ‰§è¡Œå®Œæˆ")
    logger.info(f"æˆåŠŸ: {len(successful_results)}, å¤±è´¥: {len(failed_factors)}")


def example_chunked_execution():
    """åˆ†ç»„å¹¶å‘æ‰§è¡Œç¤ºä¾‹"""
    logger.info("=== åˆ†ç»„å¹¶å‘æ‰§è¡Œç¤ºä¾‹ ===")
    
    # è¯»å–å› å­åˆ—è¡¨
    factor_file = r'/projects/_03_factor_selection/factor_manager/selector/v3æœªç»è¿‡æ®‹å·®åŒ–ç‰ˆæœ¬.csv'
    df = pd.read_csv(factor_file)
    factor_names = df['factor_name'].unique().tolist()[:12]  # æµ‹è¯•12ä¸ªå› å­
    
    snapshot_config_id = '20250825_091622_98ed2d08'
    
    # åˆ†ç»„å¹¶å‘æ‰§è¡Œ(é€‚åˆå†…å­˜å—é™çš„æƒ…å†µ)
    successful_results, failed_chunks = run_concurrent_factors(
        factor_names=factor_names,
        snapshot_config_id=snapshot_config_id,
        max_workers=2,
        execution_mode="chunked"
    )
    
    logger.info(f"åˆ†ç»„æ‰§è¡Œå®Œæˆ")
    logger.info(f"æˆåŠŸåˆ†ç»„: {len(successful_results)}")
    logger.info(f"å¤±è´¥åˆ†ç»„: {len(failed_chunks)}")


def example_production_batch():
    """ç”Ÿäº§ç¯å¢ƒæ‰¹é‡è®¡ç®—ç¤ºä¾‹"""
    logger.info("=== ç”Ÿäº§ç¯å¢ƒæ‰¹é‡è®¡ç®—ç¤ºä¾‹ ===")
    
    # è¯»å–å®Œæ•´å› å­åˆ—è¡¨
    factor_file = r'/projects/_03_factor_selection/factor_manager/selector/v3æœªç»è¿‡æ®‹å·®åŒ–ç‰ˆæœ¬.csv'
    df = pd.read_csv(factor_file)
    factor_names = df['factor_name'].unique().tolist()
    
    snapshot_config_id = '20250825_091622_98ed2d08'
    
    logger.info(f"å‡†å¤‡è®¡ç®— {len(factor_names)} ä¸ªå› å­çš„æ»šåŠ¨IC")
    
    # åˆ†æ‰¹å¤„ç†,é¿å…å†…å­˜å ç”¨è¿‡é«˜
    batch_size = 10
    total_successful = 0
    total_failed = 0
    
    for i in range(0, len(factor_names), batch_size):
        batch_factors = factor_names[i:i + batch_size]
        batch_num = i // batch_size + 1
        total_batches = (len(factor_names) + batch_size - 1) // batch_size
        
        logger.info(f"ğŸš€ å¼€å§‹ç¬¬ {batch_num}/{total_batches} æ‰¹æ¬¡è®¡ç®— ({len(batch_factors)} ä¸ªå› å­)")
        
        try:
            successful_results, failed_factors = run_concurrent_factors(
                factor_names=batch_factors,
                snapshot_config_id=snapshot_config_id,
                max_workers=3,
                execution_mode="single"
            )
            
            total_successful += len(successful_results)
            total_failed += len(failed_factors)
            
            logger.info(f"âœ… ç¬¬ {batch_num} æ‰¹æ¬¡å®Œæˆ: æˆåŠŸ {len(successful_results)}, å¤±è´¥ {len(failed_factors)}")
            
        except Exception as e:
            logger.error(f"âŒ ç¬¬ {batch_num} æ‰¹æ¬¡æ‰§è¡Œå¼‚å¸¸: {e}")
            total_failed += len(batch_factors)
    
    logger.info(f"ğŸ‰ å…¨éƒ¨æ‰¹æ¬¡è®¡ç®—å®Œæˆ!")
    logger.info(f"ğŸ“Š æ€»è®¡: æˆåŠŸ {total_successful}, å¤±è´¥ {total_failed}")
    logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {(total_successful/(total_successful+total_failed)*100):.1f}%")


if __name__ == "__main__":
    # æ ¹æ®éœ€è¦é€‰æ‹©è¿è¡Œçš„ç¤ºä¾‹
    
    # 1. åŸºç¡€ç¤ºä¾‹
    example_basic_concurrent_execution()
    
    # 2. è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹
    # example_custom_config_execution()
    
    # 3. åˆ†ç»„æ‰§è¡Œç¤ºä¾‹
    # example_chunked_execution()
    
    # 4. ç”Ÿäº§ç¯å¢ƒæ‰¹é‡è®¡ç®—ç¤ºä¾‹
    # example_production_batch()