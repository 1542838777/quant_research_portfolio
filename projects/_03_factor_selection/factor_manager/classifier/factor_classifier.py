"""
å› å­åˆ†ç±»å™¨æ¨¡å—

è´Ÿè´£å¯¹å› å­è¿›è¡Œè‡ªåŠ¨åˆ†ç±»ã€èšç±»å’Œç‰¹å¾æå–ã€‚
"""
import re

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any, Union, Tuple
import logging
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# å¯¼å…¥å› å­æ³¨å†Œè¡¨
from ..registry.factor_registry import FactorCategory

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class FactorClassifier:
    """å› å­åˆ†ç±»å™¨ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–å› å­åˆ†ç±»å™¨"""
        self.feature_cache = {}  # ç¼“å­˜å› å­ç‰¹å¾
    
    def extract_factor_features(self, 
                               factor_data: pd.DataFrame,
                               returns_data: pd.DataFrame = None) -> Dict[str, float]:
        """
        æå–å› å­ç‰¹å¾
        
        Args:
            factor_data: å› å­æ•°æ®
            returns_data: æ”¶ç›Šç‡æ•°æ®ï¼Œç”¨äºè®¡ç®—ä¸æ”¶ç›Šç›¸å…³çš„ç‰¹å¾
            
        Returns:
            ç‰¹å¾å­—å…¸
        """
        features = {}
        
        # åŸºæœ¬ç»Ÿè®¡ç‰¹å¾
        features['mean'] = factor_data.mean().mean()
        features['std'] = factor_data.std().mean()
        features['skew'] = factor_data.skew().mean()
        features['kurtosis'] = factor_data.kurtosis().mean()
        
        # æ—¶é—´åºåˆ—ç‰¹å¾
        if len(factor_data) > 1:
            # è‡ªç›¸å…³æ€§
            autocorr = factor_data.apply(lambda x: x.autocorr(lag=1)).mean()
            features['autocorr'] = autocorr if not pd.isna(autocorr) else 0
            
            # è¶‹åŠ¿æ€§
            factor_means = factor_data.mean(axis=1)
            features['trend'] = np.polyfit(np.arange(len(factor_means)), factor_means, 1)[0]
            
            # æ³¢åŠ¨æ€§
            features['volatility'] = factor_data.pct_change().std().mean()
        
        # æ¨ªæˆªé¢ç‰¹å¾
        features['cross_dispersion'] = factor_data.std(axis=1).mean()
        
        # ä¸æ”¶ç›Šç‡ç›¸å…³çš„ç‰¹å¾
        if returns_data is not None:
            # å¯¹é½æ•°æ®
            common_dates = factor_data.index.intersection(returns_data.index)
            common_stocks = factor_data.columns.intersection(returns_data.columns)
            
            if len(common_dates) > 0 and len(common_stocks) > 0:
                aligned_factor = factor_data.loc[common_dates, common_stocks]
                aligned_returns = returns_data.loc[common_dates, common_stocks]
                
                # è®¡ç®—IC
                ic_values = []
                for date in common_dates:
                    f = aligned_factor.loc[date].dropna()
                    r = aligned_returns.loc[date].dropna()
                    common = f.index.intersection(r.index)
                    if len(common) > 10:  # è‡³å°‘éœ€è¦10åªè‚¡ç¥¨
                        corr = f[common].corr(r[common])
                        if not pd.isna(corr):
                            ic_values.append(corr)
                
                if ic_values:
                    features['ic_mean'] = np.mean(ic_values)
                    features['ic_std'] = np.std(ic_values)
                    features['ic_ir'] = features['ic_mean'] / features['ic_std'] if features['ic_std'] > 0 else 0
        
        return features
    
    def classify_factor(self, 
                       factor_data: pd.DataFrame,
                       returns_data: pd.DataFrame = None) -> FactorCategory:
        """
        è‡ªåŠ¨åˆ†ç±»å› å­
        
        Args:
            factor_data: å› å­æ•°æ®
            returns_data: æ”¶ç›Šç‡æ•°æ®
            
        Returns:
            å› å­ç±»åˆ«
        """
        # æå–ç‰¹å¾
        features = self.extract_factor_features(factor_data, returns_data)
        
        # åŸºäºè§„åˆ™çš„åˆ†ç±»
        # è¿™é‡Œæ˜¯ä¸€ä¸ªç®€åŒ–çš„åˆ†ç±»é€»è¾‘ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„è§„åˆ™æˆ–æœºå™¨å­¦ä¹ æ¨¡å‹
        
        # æ£€æŸ¥è¶‹åŠ¿æ€§å’Œè‡ªç›¸å…³æ€§ -> åŠ¨é‡ç±»
        if features.get('trend', 0) > 0.01 and features.get('autocorr', 0) > 0.7:
            return FactorCategory.MOMENTUM
        
        # æ£€æŸ¥æ³¢åŠ¨æ€§ -> æ³¢åŠ¨ç‡ç±»
        if features.get('volatility', 0) > 0.05:
            return FactorCategory.VOLATILITY
        
        # æ£€æŸ¥ååº¦ -> æƒ…ç»ªç±»
        if abs(features.get('skew', 0)) > 1.0:
            return FactorCategory.SENTIMENT
        
        # é»˜è®¤åˆ†ç±»
        return FactorCategory.CUSTOM
    
    def cluster_factors(self, 
                       factor_data_dict: Dict[str, pd.DataFrame],
                       n_clusters: int = 5) -> Dict[str, int]:
        """
        èšç±»å› å­
        
        Args:
            factor_data_dict: å› å­æ•°æ®å­—å…¸
            n_clusters: èšç±»æ•°é‡
            
        Returns:
            å› å­èšç±»ç»“æœå­—å…¸
        """
        # æå–æ‰€æœ‰å› å­çš„ç‰¹å¾
        feature_matrix = []
        factor_names = []
        
        for name, data in factor_data_dict.items():
            if name not in self.feature_cache:
                self.feature_cache[name] = self.extract_factor_features(data)
            
            # å°†ç‰¹å¾è½¬æ¢ä¸ºå‘é‡
            feature_vector = [v for k, v in sorted(self.feature_cache[name].items())]
            feature_matrix.append(feature_vector)
            factor_names.append(name)
        
        if not feature_matrix:
            raise  ValueError("æ²¡æœ‰å¯ç”¨çš„å› å­ç‰¹å¾è¿›è¡Œèšç±»")

        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        scaled_features = scaler.fit_transform(feature_matrix)
        
        # æ‰§è¡ŒK-meansèšç±»
        kmeans = KMeans(n_clusters=min(n_clusters, len(feature_matrix)), random_state=42)
        clusters = kmeans.fit_predict(scaled_features)
        
        # è¿”å›èšç±»ç»“æœ
        return {name: int(cluster) for name, cluster in zip(factor_names, clusters)}
    
    def visualize_factor_clusters(self, 
                                factor_data_dict: Dict[str, pd.DataFrame],
                                n_clusters: int = 5,
                                method: str = 'pca',
                                figsize: Tuple[int, int] = (12, 10)) -> plt.Figure:
        """
        å¯è§†åŒ–å› å­èšç±»
        
        Args:
            factor_data_dict: å› å­æ•°æ®å­—å…¸
            n_clusters: èšç±»æ•°é‡
            method: é™ç»´æ–¹æ³•ï¼Œ'pca'æˆ–'tsne'
            figsize: å›¾è¡¨å¤§å°
            
        Returns:
            å›¾è¡¨å¯¹è±¡
        """
        # æå–ç‰¹å¾å¹¶èšç±»
        clusters = self.cluster_factors(factor_data_dict, n_clusters)
        if not clusters:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„å› å­è¿›è¡Œå¯è§†åŒ–")

        
        # å‡†å¤‡æ•°æ®
        feature_matrix = []
        factor_names = []
        
        for name in clusters.keys():
            feature_vector = [v for k, v in sorted(self.feature_cache[name].items())]
            feature_matrix.append(feature_vector)
            factor_names.append(name)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        scaled_features = scaler.fit_transform(feature_matrix)
        
        # é™ç»´
        if method == 'pca':
            pca = PCA(n_components=2)
            reduced_data = pca.fit_transform(scaled_features)
            explained_var = pca.explained_variance_ratio_
            
            # åˆ›å»ºå›¾è¡¨
            fig, ax = plt.subplots(figsize=figsize)
            
            # ç»˜åˆ¶æ•£ç‚¹å›¾
            scatter = ax.scatter(
                reduced_data[:, 0], 
                reduced_data[:, 1], 
                c=[clusters[name] for name in factor_names],
                cmap='viridis', 
                alpha=0.8,
                s=100
            )
            
            # æ·»åŠ æ ‡ç­¾
            for i, name in enumerate(factor_names):
                ax.annotate(
                    name, 
                    (reduced_data[i, 0], reduced_data[i, 1]),
                    fontsize=9,
                    alpha=0.8
                )
            
            # æ·»åŠ å›¾ä¾‹
            legend = ax.legend(
                *scatter.legend_elements(),
                title="èšç±»",
                loc="upper right"
            )
            
            # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
            ax.set_title('å› å­èšç±»å¯è§†åŒ– (PCAé™ç»´)', fontsize=14)
            ax.set_xlabel(f'ä¸»æˆåˆ†1 ({explained_var[0]:.2%} æ–¹å·®)', fontsize=12)
            ax.set_ylabel(f'ä¸»æˆåˆ†2 ({explained_var[1]:.2%} æ–¹å·®)', fontsize=12)
            
            # æ·»åŠ ç½‘æ ¼çº¿
            ax.grid(True, linestyle='--', alpha=0.7)
            
            return fig
        else:
            # å¯ä»¥æ·»åŠ å…¶ä»–é™ç»´æ–¹æ³•ï¼Œå¦‚t-SNE
            raise ValueError(f"ä¸æ”¯æŒçš„é™ç»´æ–¹æ³•: {method}")

    
    def analyze_factor_correlation(self, 
                                 factor_data_dict: Dict[str, pd.DataFrame],
                                 figsize: Tuple[int, int] = (12, 10)) -> Tuple[pd.DataFrame, plt.Figure]:
        """
        åˆ†æå› å­ç›¸å…³æ€§
        
        Args:
            factor_data_dict: å› å­æ•°æ®å­—å…¸
            figsize: å›¾è¡¨å¤§å°
            
        Returns:
            (ç›¸å…³æ€§çŸ©é˜µ, çƒ­åŠ›å›¾)
        """
        # å¯¹é½æ‰€æœ‰å› å­æ•°æ®
        aligned_data = {}
        common_index = None
        
        for name, data in factor_data_dict.items():
            if common_index is None:
                common_index = data.index
            else:
                common_index = common_index.intersection(data.index)
        
        for name, data in factor_data_dict.items():
            aligned_data[name] = data.reindex(common_index).stack().dropna()
        
        # æ„å»ºDataFrameå¹¶è®¡ç®—ç›¸å…³æ€§
        factor_df = pd.DataFrame(aligned_data)
        correlation_matrix = factor_df.corr()
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        fig, ax = plt.subplots(figsize=figsize)
        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
        
        sns.heatmap(
            correlation_matrix, 
            mask=mask,
            cmap='coolwarm', 
            annot=True, 
            fmt='.2f',
            linewidths=0.5,
            ax=ax,
            vmin=-1, 
            vmax=1,
            center=0
        )
        
        ax.set_title('å› å­ç›¸å…³æ€§çƒ­åŠ›å›¾', fontsize=14)
        
        return correlation_matrix, fig

    """
       ã€V2ç‰ˆã€‘å› å­åˆ†ç±»å™¨
       æ ¹æ®å› å­åç§°çš„é€šç”¨æ¨¡å¼å’Œç‰¹å®šå…³é”®å­—ï¼Œåˆ¤æ–­å…¶æ‰€å±çš„é£é™©ç±»åˆ«ã€‚
       """

    """
       ä¸€ä¸ªç”¨äºæ ¹æ®åç§°å¯¹å› å­è¿›è¡Œåˆ†ç±»çš„å·¥å…·ç±»ã€‚
       """

    @staticmethod
    def _match(name: str,
               exact: list[str] | None = None,
               prefixes: list[str] | None = None,
               suffixes: list[str] | None = None,
               keywords: list[str] | None = None,
               patterns: list[str] | None = None) -> bool:
        """
        ã€é€šç”¨åŒ¹é…å¼•æ“ã€‘æ ¹æ®å¤šç§è§„åˆ™åˆ¤æ–­å› å­åç§°æ˜¯å¦åŒ¹é…ã€‚

        Args:
            name (str): å¾…æ£€æŸ¥çš„å› å­åç§°ã€‚
            exact (list): ç²¾ç¡®åŒ¹é…åˆ—è¡¨ã€‚
            prefixes (list): å‰ç¼€åŒ¹é…åˆ—è¡¨ã€‚
            suffixes (list): åç¼€åŒ¹é…åˆ—è¡¨ã€‚
            keywords (list): åŒ…å«çš„å…³é”®å­—åˆ—è¡¨ã€‚
            patterns (list): æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼åˆ—è¡¨ã€‚

        Returns:
            bool: å¦‚æœåŒ¹é…ä»»ä½•è§„åˆ™ï¼Œåˆ™è¿”å›Trueã€‚
        """
        # ä½¿ç”¨ç©ºåˆ—è¡¨ä½œä¸ºé»˜è®¤å€¼ï¼Œé¿å…å¤„ç†Noneçš„æƒ…å†µ
        exact = exact or []
        prefixes = prefixes or []
        suffixes = suffixes or []
        keywords = keywords or []
        patterns = patterns or []

        # 1. ç²¾ç¡®åŒ¹é…
        if name in exact:
            return True
        # 2. å‰ç¼€/åç¼€åŒ¹é…
        if any(name.startswith(p) for p in prefixes) or \
                any(name.endswith(s) for s in suffixes):
            return True
        # 3. å…³é”®å­—åŒ¹é…
        if any(k in name for k in keywords):
            return True
        # 4. æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… (æœ€å¼ºå¤§ã€æœ€ç²¾ç¡®)
        #    ä½¿ç”¨ \bbeta\b åŒ¹é…ç‹¬ç«‹çš„å•è¯ "beta"
        if any(re.search(pat, name) for pat in patterns):
            return True

        return False

    @classmethod
    def is_size_factor(cls, factor_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¸‚å€¼ç±»å› å­ (æ›´å®‰å…¨)"""
        return cls._match(
            name=factor_name,
            exact=['size', 'log_circ_mv','circ_mv','log_market_cap','log_total_mv','total_mv'],
            keywords=['market_cap'],
            suffixes=['_mv', '_log_mv']  # ä½¿ç”¨åç¼€åŒ¹é…æ›¿ä»£å…³é”®å­—'_mv'ï¼Œæ›´å®‰å…¨
        )

    @classmethod
    def is_industry_factor(cls, factor_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè¡Œä¸šç±»å› å­ (é€»è¾‘ä¸å˜ï¼Œç»Ÿä¸€æ¨¡å¼)"""
        return cls._match(
            name=factor_name,
            exact=['industry'],
            prefixes=['industry_'],
            keywords=['sw_l1', 'sw_l2']  # ğŸ”‘ æ–°å¢ï¼šå…³é”®å­—åŒ¹é…
        )
    @classmethod
    def is_beta_factor(cls, factor_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºBetaç±»å› å­ (ä½¿ç”¨æ­£åˆ™ï¼Œæ›´ç²¾ç¡®)"""
        # \b æ˜¯å•è¯è¾¹ç•Œï¼Œç¡®ä¿åªåŒ¹é…ç‹¬ç«‹çš„ "beta" å•è¯
        # ä¾‹å¦‚ï¼šåŒ¹é… 'beta', 'equity_beta', 'beta_60d'
        # ä¸ä¼šåŒ¹é…ï¼š'alphabet'
        return cls._match(
            name=factor_name,
            patterns=[r'\bbeta\b']
        )
