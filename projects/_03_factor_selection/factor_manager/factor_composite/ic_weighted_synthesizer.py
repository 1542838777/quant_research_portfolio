"""
ICåŠ æƒå› å­åˆæˆå™¨ - ä¸“ä¸šçº§å› å­åˆæˆå¼•æ“

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. åŸºäºå†å²ICè¡¨ç°çš„æ™ºèƒ½æƒé‡åˆ†é…
2. å¤šç»´åº¦å› å­ç­›é€‰æœºåˆ¶
3. åŠ¨æ€æƒé‡è°ƒæ•´
4. é£é™©æ§åˆ¶å’Œç¨³å¥æ€§æ£€éªŒ

è®¾è®¡ç†å¿µï¼š
- ä»¥å®ç›˜ç›ˆåˆ©ä¸ºç»ˆæç›®æ ‡
- å¹³è¡¡å› å­é¢„æµ‹èƒ½åŠ›ä¸ç¨³å®šæ€§
- é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæ³¨é‡æ³›åŒ–èƒ½åŠ›
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
import json
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

from projects._03_factor_selection.factor_manager.factor_composite.factor_synthesizer import FactorSynthesizer
from projects._03_factor_selection.factor_manager.storage.result_load_manager import ResultLoadManager
from projects._03_factor_selection.factor_manager.storage.rolling_ic_manager import (
    ICCalculationConfig, run_cal_and_save_rolling_ic_by_snapshot_config_id
)
from projects._03_factor_selection.factor_manager.selector.rolling_ic_factor_selector import (
    RollingICFactorSelector, RollingICSelectionConfig
)
from projects._03_factor_selection.config_manager.config_snapshot.config_snapshot_manager import ConfigSnapshotManager
from quant_lib.config.logger_config import setup_logger

logger = setup_logger(__name__)


@dataclass
class FactorWeightingConfig:
    """å› å­æƒé‡é…ç½®"""
    # ICç­›é€‰æ ‡å‡†
    min_ic_mean: float = 0.015  # æœ€å°ICå‡å€¼é˜ˆå€¼
    min_ic_ir: float = 0.183  # æœ€å°ICä¿¡æ¯æ¯”ç‡é˜ˆå€¼
    min_ic_win_rate: float = 0.52  # æœ€å°ICèƒœç‡é˜ˆå€¼
    max_ic_p_value: float = 0.10  # æœ€å¤§ICæ˜¾è‘—æ€§på€¼

    # æƒé‡è®¡ç®—å‚æ•°
    ic_decay_halflife: int = 60  # ICæƒé‡è¡°å‡åŠè¡°æœŸ(å¤©)
    max_single_weight: float = 0.5  # å•ä¸ªå› å­æœ€å¤§æƒé‡
    min_single_weight: float = 0.05  # å•ä¸ªå› å­æœ€å°æƒé‡

    # é£é™©æ§åˆ¶
    max_factors_count: int = 8  # æœ€å¤§å› å­æ•°é‡
    correlation_threshold: float = 0.70  # å› å­é—´ç›¸å…³æ€§é˜ˆå€¼

    # å›çœ‹æœŸè®¾ç½®
    lookback_periods: List[str] = None  # ICè®¡ç®—å‘¨æœŸ

    def __post_init__(self):
        if self.lookback_periods is None:
            self.lookback_periods = ['5d', '21d']


@dataclass
class FactorQualityReport:
    """å› å­è´¨é‡æŠ¥å‘Š"""
    factor_name: str
    ic_stats: Dict[str, float]
    weight: float
    selection_reason: str
    risk_flags: List[str]


class ICWeightCalculator:
    """ICæƒé‡è®¡ç®—å™¨ - æ ¸å¿ƒç®—æ³•å¼•æ“"""

    def __init__(self, config: FactorWeightingConfig):
        self.config = config

    def calculate_ic_based_weights(
            self,
            factor_ic_stats: Dict[str, Dict[str, float]]
    ) -> Dict[str, float]:
        """
        åŸºäºICç»Ÿè®¡çš„æ™ºèƒ½æƒé‡åˆ†é…
        
        Args:
            factor_ic_stats: {factor_name: {period: ic_stats_dict}}
            
        Returns:
            Dict[factor_name, weight]: æ ‡å‡†åŒ–åçš„æƒé‡åˆ†é…
        """
        logger.info("ğŸ§® å¼€å§‹è®¡ç®—ICåŠ æƒæƒé‡...")

        # 1. è®¡ç®—ç»¼åˆICå¾—åˆ†
        factor_scores = {}
        for factor_name, periods_stats in factor_ic_stats.items():
            score = self._calculate_composite_ic_score(periods_stats)
            factor_scores[factor_name] = score
            logger.debug(f"  {factor_name}: ç»¼åˆICå¾—åˆ† = {score:.4f}")

        # 2. åŸºäºå¾—åˆ†è®¡ç®—åŸå§‹æƒé‡
        raw_weights = self._convert_scores_to_weights(factor_scores)

        # 3. åº”ç”¨çº¦æŸå’Œæ ‡å‡†åŒ–
        final_weights = self._apply_weight_constraints(raw_weights)

        logger.info(f"âœ… ICæƒé‡è®¡ç®—å®Œæˆï¼Œå…±{len(final_weights)}ä¸ªå› å­è¢«åˆ†é…æƒé‡")
        return final_weights

    def _calculate_composite_ic_score(self, periods_stats: Dict[str, Dict]) -> float:
        """è®¡ç®—å› å­çš„ç»¼åˆICå¾—åˆ†"""
        period_scores = []

        for period, stats in periods_stats.items():
            if not stats or 'ic_mean' not in stats:
                continue

            # æå–å…³é”®æŒ‡æ ‡
            ic_mean = abs(stats.get('ic_mean', 0))  # ä½¿ç”¨ç»å¯¹å€¼
            ic_ir = stats.get('ic_ir', 0)
            ic_win_rate = stats.get('ic_win_rate', 0.5)
            ic_t_stat = abs(stats.get('ic_t_stat', 0))

            # å¤šç»´åº¦è¯„åˆ†æ¨¡å‹
            # 1. ICå‡å€¼å¾—åˆ† (40%æƒé‡)
            ic_mean_score = min(ic_mean / 0.05, 1.0) * 0.4

            # 2. ICç¨³å®šæ€§å¾—åˆ† (30%æƒé‡) 
            ic_stability_score = min(ic_ir / 0.5, 1.0) * 0.3

            # 3. ICèƒœç‡å¾—åˆ† (20%æƒé‡)
            ic_win_score = max(0, (ic_win_rate - 0.5) * 2) * 0.2

            # 4. ICæ˜¾è‘—æ€§å¾—åˆ† (10%æƒé‡)
            ic_sig_score = min(ic_t_stat / 2.0, 1.0) * 0.1

            period_score = ic_mean_score + ic_stability_score + ic_win_score + ic_sig_score
            period_scores.append(period_score)

        # å¤šå‘¨æœŸå¹³å‡ï¼Œç»™çŸ­æœŸç¨é«˜æƒé‡
        if not period_scores:
            return 0.0

        if len(period_scores) == 1:
            return period_scores[0]
        else:
            # --- æ”¹è¿›çš„åŠ æƒæ–¹æ¡ˆ ---
            # ä½¿ç”¨æŒ‡æ•°è¡°å‡æƒé‡ï¼Œç»™çŸ­æœŸæ›´é«˜æƒé‡ï¼Œä½†ä¾ç„¶è€ƒè™‘æ‰€æœ‰å‘¨æœŸ
            # decay_rate è¶Šå°ï¼Œæƒé‡è¡°å‡è¶Šæ…¢
            decay_rate = 0.75
            weights = np.array([decay_rate ** i for i in range(len(period_scores))])
            weights /= weights.sum()  # æƒé‡å½’ä¸€åŒ–

            logger.debug(f"  å¤šå‘¨æœŸæƒé‡ (ä»1dåˆ°120d): {[f'{w:.2f}' for w in weights]}")
            return np.average(period_scores, weights=weights)

    def _convert_scores_to_weights(self, factor_scores: Dict[str, float]) -> Dict[str, float]:
        """å°†å¾—åˆ†è½¬æ¢ä¸ºæƒé‡"""
        if not factor_scores:
            return {}

        # ä½¿ç”¨ softmax å‡½æ•°å°†å¾—åˆ†è½¬æ¢ä¸ºæƒé‡ï¼Œå¢å¼ºåŒºåˆ†åº¦
        scores = np.array(list(factor_scores.values()))

        # è¿‡æ»¤æ‰å¾—åˆ†è¿‡ä½çš„å› å­
        valid_mask = scores > 0.1
        if not valid_mask.any():
            logger.warning("âš ï¸ æ‰€æœ‰å› å­å¾—åˆ†éƒ½è¿‡ä½ï¼Œä½¿ç”¨ç­‰æƒé‡")
            return {name: 1.0 / len(factor_scores) for name in factor_scores.keys()}

        # å¯¹æœ‰æ•ˆå› å­åº”ç”¨ softmax
        valid_scores = scores[valid_mask]
        valid_names = [name for i, name in enumerate(factor_scores.keys()) if valid_mask[i]]

        # æ¸©åº¦å‚æ•°æ§åˆ¶æƒé‡é›†ä¸­åº¦ï¼Œæ¸©åº¦è¶Šé«˜è¶Šå¹³å‡
        temperature = 2.0
        exp_scores = np.exp(valid_scores / temperature)
        softmax_weights = exp_scores / exp_scores.sum()

        return dict(zip(valid_names, softmax_weights))

    def _apply_weight_constraints(self, raw_weights: Dict[str, float]) -> Dict[str, float]:
        """åº”ç”¨æƒé‡çº¦æŸ"""
        if not raw_weights:
            return {}

        weights = raw_weights.copy()

        # 1. åº”ç”¨å•å› å­æƒé‡ä¸Šä¸‹é™
        for name in weights:
            weights[name] = np.clip(weights[name],
                                    self.config.min_single_weight,
                                    self.config.max_single_weight)

        # 2. é‡æ–°æ ‡å‡†åŒ–
        total_weight = sum(weights.values())
        if total_weight > 0:
            weights = {name: w / total_weight for name, w in weights.items()}

        return weights


class FactorQualityFilter:
    """å› å­è´¨é‡ç­›é€‰å™¨"""

    def __init__(self, config: FactorWeightingConfig):
        self.config = config

    def filter_factors_by_quality(
            self,
            factor_ic_stats: Dict[str, Dict[str, Dict]]
    ) -> Tuple[Dict[str, Dict[str, Dict]], List[FactorQualityReport]]:
        """
        åŸºäºå¤šç»´åº¦è´¨é‡æŒ‡æ ‡ç­›é€‰å› å­
        
        Returns:
            (filtered_factor_stats, quality_reports)
        """
        logger.info("ğŸ” å¼€å§‹å› å­è´¨é‡ç­›é€‰...")

        filtered_stats = {}
        quality_reports = []

        for factor_name, periods_stats in factor_ic_stats.items():
            report = self._evaluate_single_factor(factor_name, periods_stats)
            quality_reports.append(report)

            if self._passes_quality_filter(report):
                filtered_stats[factor_name] = periods_stats
                logger.info(f"âœ… {factor_name}: é€šè¿‡ç­›é€‰ (æƒé‡={report.weight:.3f}) - {report.selection_reason}")
            else:
                logger.info(f"âŒ {factor_name}: æœªé€šè¿‡ç­›é€‰ - {report.selection_reason}")

        logger.info(f"ğŸ“Š ç­›é€‰ç»“æœ: {len(filtered_stats)}/{len(factor_ic_stats)} ä¸ªå› å­é€šè¿‡è´¨é‡æ£€éªŒ")
        return filtered_stats, quality_reports

    def _evaluate_single_factor(self, factor_name: str, periods_stats: Dict) -> FactorQualityReport:
        """è¯„ä¼°å•ä¸ªå› å­è´¨é‡"""
        # è®¡ç®—å…³é”®æŒ‡æ ‡çš„ç»¼åˆè¡¨ç°
        ic_means = []
        ic_irs = []
        ic_win_rates = []
        ic_p_values = []

        for period, stats in periods_stats.items():
            if stats and 'ic_mean' in stats:
                ic_means.append(abs(stats.get('ic_mean', 0)))
                ic_irs.append(stats.get('ic_ir', 0))
                ic_win_rates.append(stats.get('ic_win_rate', 0.5))
                ic_p_values.append(stats.get('ic_p_value', 1.0))

        if not ic_means:
            return FactorQualityReport(
                factor_name=factor_name,
                ic_stats={},
                weight=0.0,
                selection_reason="ç¼ºå°‘æœ‰æ•ˆçš„ICç»Ÿè®¡æ•°æ®",
                risk_flags=["æ•°æ®ä¸è¶³"]
            )

        # ç»¼åˆç»Ÿè®¡ ä¹‹å‰å¯¹ æ¯ä¸ªperiod ç”¨ä¸åŒçš„æ—¶é—´è¿›è¡Œaverï¼Œç°åœ¨å¯¹ä¸åŒçš„periodè¿›è¡Œaver
        avg_ic_mean = np.mean(ic_means)
        avg_ic_ir = np.mean(ic_irs)
        avg_win_rate = np.mean(ic_win_rates)
        min_p_value = min(ic_p_values)

        ic_stats = {
            'avg_ic_mean': avg_ic_mean,
            'avg_ic_ir': avg_ic_ir,
            'avg_win_rate': avg_win_rate,
            'min_p_value': min_p_value
        }

        # è´¨é‡è¯„ä¼°
        risk_flags = []
        selection_reason = ""
        if avg_ic_mean < self.config.min_ic_mean:
            risk_flags.append(f"ICå‡å€¼è¿‡ä½({avg_ic_mean:.3f})")
        if avg_ic_ir < self.config.min_ic_ir:
            risk_flags.append(f"ICä¿¡æ¯æ¯”ç‡è¿‡ä½({avg_ic_ir:.3f})")
        if avg_win_rate < self.config.min_ic_win_rate:
            risk_flags.append(f"ICèƒœç‡è¿‡ä½({avg_win_rate:.3f})")
        if min_p_value > self.config.max_ic_p_value:
            risk_flags.append(f"ICæ˜¾è‘—æ€§ä¸è¶³(p={min_p_value:.3f})")

        if not risk_flags:
            selection_reason = f"é«˜è´¨é‡å› å­ (IC={avg_ic_mean:.3f}, IR={avg_ic_ir:.2f}, èƒœç‡={avg_win_rate:.1%})"
            weight = self._calculate_factor_weight(ic_stats)
        else:
            selection_reason = "; ".join(risk_flags)
            weight = 0.0

        return FactorQualityReport(
            factor_name=factor_name,
            ic_stats=ic_stats,
            weight=weight,
            selection_reason=selection_reason,
            risk_flags=risk_flags
        )

    def _passes_quality_filter(self, report: FactorQualityReport) -> bool:
        """åˆ¤æ–­å› å­æ˜¯å¦é€šè¿‡è´¨é‡ç­›é€‰"""
        return len(report.risk_flags) == 0 and report.weight > 0

    def _calculate_factor_weight(self, ic_stats: Dict[str, float]) -> float:
        """åŸºäºICç»Ÿè®¡è®¡ç®—å› å­åˆæ­¥æƒé‡"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„æƒé‡é€»è¾‘
        # ç›®å‰ä½¿ç”¨ç®€å•çš„ç»¼åˆå¾—åˆ†
        ic_mean = ic_stats.get('avg_ic_mean', 0)
        ic_ir = ic_stats.get('avg_ic_ir', 0)
        win_rate = ic_stats.get('avg_win_rate', 0.5)

        # ç»¼åˆå¾—åˆ†
        score = ic_mean * 0.5 + ic_ir * 0.3 + max(0, win_rate - 0.5) * 0.2
        return min(score, 1.0)


class ICWeightedSynthesizer(FactorSynthesizer):
    """ICåŠ æƒå› å­åˆæˆå™¨ - ç»§æ‰¿å¹¶æ‰©å±•ç°æœ‰åŠŸèƒ½"""

    def __init__(self, factor_manager, factor_analyzer, factor_processor,
                 config: Optional[FactorWeightingConfig] = None, 
                 selector_config: Optional[RollingICSelectionConfig] = None):
        super().__init__(factor_manager, factor_analyzer, factor_processor)

        self.config = config or FactorWeightingConfig()
        self.weight_calculator = ICWeightCalculator(self.config)
        self.quality_filter = FactorQualityFilter(self.config)

        # è®¾ç½®å·¥ä½œè·¯å¾„
        self.main_work_path = Path(r"D:\lqs\codeAbout\py\Quantitative\quant_research_portfolio\projects\_03_factor_selection\workspace\result")

        # æ»šåŠ¨ICç®¡ç†å™¨ - æ ¸å¿ƒæ”¹è¿›
        rolling_ic_config = ICCalculationConfig(
            lookback_months=12,
            forward_periods=self.config.lookback_periods,
            calculation_frequency='M'
        )

        # é›†æˆä¸“ä¸šçš„æ»šåŠ¨ICå› å­ç­›é€‰å™¨
        self.selector_config = selector_config or RollingICSelectionConfig()
        self.factor_selector = None  # å»¶è¿Ÿåˆå§‹åŒ–ï¼Œéœ€è¦snap_config_id

        # ç¼“å­˜ICç»Ÿè®¡æ•°æ®ï¼Œé¿å…é‡å¤è®¡ç®—
        self._ic_stats_cache = {}

    def synthesize_ic_weighted_factor(
            self,
            composite_factor_name: str,
            stock_pool_index: str,
            candidate_factor_names: List[str],
            force_recalculate_ic: bool = False,
            snap_config_id: str = None

    ) -> Tuple[pd.DataFrame, Dict]:
        """
        ICåŠ æƒå› å­åˆæˆä¸»æµç¨‹
        
        Args:
            composite_factor_name: å¤åˆå› å­åç§°
            stock_pool_index: è‚¡ç¥¨æ± åç§°
            candidate_factor_names: å€™é€‰å› å­åˆ—è¡¨
            force_recalculate_ic: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¡ç®—IC
            
        Returns:
            (composite_factor_df, synthesis_report)
        """
        logger.info(f"\nğŸš€ å¼€å§‹ICåŠ æƒå› å­åˆæˆ: {composite_factor_name}")
        logger.info(f"ğŸ“Š å€™é€‰å› å­æ•°é‡: {len(candidate_factor_names)}")
        logger.info(f"ğŸ“ˆ ç›®æ ‡è‚¡ç¥¨æ± INDEX: {stock_pool_index}")

        # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†å€™é€‰å› å­çš„ICç»Ÿè®¡æ•°æ®
        factor_ic_stats = self._collect_factor_ic_stats(
            candidate_factor_names,
            stock_pool_index,
            force_recalculate_ic,
            snap_config_id

        )

        # ç¬¬äºŒæ­¥ï¼šè´¨é‡ç­›é€‰
        qualified_factor_stats, quality_reports = self.quality_filter.filter_factors_by_quality(
            factor_ic_stats
        )

        if not qualified_factor_stats:
            raise ValueError("âŒ æ²¡æœ‰å› å­é€šè¿‡è´¨é‡ç­›é€‰ï¼Œæ— æ³•è¿›è¡Œåˆæˆ")

        # ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—æƒé‡
        factor_weights = self.weight_calculator.calculate_ic_based_weights(
            qualified_factor_stats
        )

        # ç¬¬å››æ­¥ï¼šæ‰§è¡ŒåŠ æƒåˆæˆ
        composite_factor_df = self._execute_weighted_synthesis(
            composite_factor_name,
            stock_pool_index,
            factor_weights,
            snap_config_id
        )

        # ç”ŸæˆåˆæˆæŠ¥å‘Š
        synthesis_report = self._generate_synthesis_report(
            composite_factor_name,
            candidate_factor_names,
            factor_weights,
            quality_reports
        )


        logger.info(f"âœ… ICåŠ æƒå› å­åˆæˆå®Œæˆ: {composite_factor_name}")
        return composite_factor_df, synthesis_report
    #å¼€å§‹ç­›é€‰ æœ€ç»ˆç‰ˆ
    def synthesize_with_professional_selection(
            self,
            composite_factor_name: str,
            candidate_factor_names: List[str],
            snap_config_id: str,
            force_generate_ic: bool = False
    ) -> Tuple[pd.DataFrame, Dict]:
        """
        ä½¿ç”¨ä¸“ä¸šæ»šåŠ¨ICç­›é€‰å™¨è¿›è¡Œå› å­åˆæˆ
        
        Args:
            composite_factor_name: å¤åˆå› å­åç§°
            candidate_factor_names: å€™é€‰å› å­åˆ—è¡¨
            snap_config_id: é…ç½®å¿«ç…§ID
            force_generate_ic: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”ŸæˆICæ•°æ®
            
        Returns:
            (composite_factor_df, synthesis_report)
        """
        logger.info(f"\nğŸš€ å¯åŠ¨ä¸“ä¸šICç­›é€‰å› å­åˆæˆ: {composite_factor_name}")
        logger.info(f"ğŸ“Š å€™é€‰å› å­æ•°é‡: {len(candidate_factor_names)}")
        
        # 1. åˆå§‹åŒ–ä¸“ä¸šç­›é€‰å™¨
        if self.factor_selector is None:
            self.factor_selector = RollingICFactorSelector(snap_config_id, self.selector_config)
            logger.info("âœ… æ»šåŠ¨ICå› å­ç­›é€‰å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # 2. æ‰§è¡Œå®Œæ•´çš„ä¸“ä¸šç­›é€‰æµç¨‹
        selected_factors, selection_report = self.factor_selector.run_complete_selection(
            candidate_factor_names, force_generate_ic
        )
        
        if not selected_factors:
            raise ValueError("âŒ ä¸“ä¸šç­›é€‰æœªé€‰å‡ºä»»ä½•å› å­ï¼Œæ— æ³•è¿›è¡Œåˆæˆ")
        
        logger.info(f"ğŸ¯ ä¸“ä¸šç­›é€‰ç»“æœ: {len(selected_factors)} ä¸ªä¼˜è´¨å› å­")
        for i, factor in enumerate(selected_factors, 1):
            logger.info(f"  {i}. {factor}")
        
        # 3. è·å–è‚¡ç¥¨æ± ä¿¡æ¯
        config_manager = ConfigSnapshotManager()
        pool_index, start_date, end_date, config_evaluation = config_manager.get_snapshot_config_content_details(snap_config_id)
        
        # 4. åŸºäºç­›é€‰ç»“æœè®¡ç®—ICæƒé‡
        factor_ic_stats = {}
        for factor_name in selected_factors:
            try:
                ic_stats = self._load_factor_ic_stats(
                    factor_name, pool_index, snap_config_id=snap_config_id
                )
                if ic_stats:
                    factor_ic_stats[factor_name] = ic_stats
                    logger.debug(f"  âœ… {factor_name}: åŠ è½½ICç»Ÿè®¡æˆåŠŸ")
                else:
                    logger.warning(f"  âš ï¸ {factor_name}: ICç»Ÿè®¡åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨ç­‰æƒé‡")
            except Exception as e:
                logger.error(f"  âŒ {factor_name}: ICç»Ÿè®¡åŠ è½½å¼‚å¸¸ - {e}")
        
        # 5. è®¡ç®—æœ€ç»ˆæƒé‡
        if factor_ic_stats:
            factor_weights = self.weight_calculator.calculate_ic_based_weights(factor_ic_stats)
        else:
            logger.warning("âš ï¸ æ— æ³•è·å–ICç»Ÿè®¡ï¼Œä½¿ç”¨ç­‰æƒé‡åˆæˆ")
            equal_weight = 1.0 / len(selected_factors)
            factor_weights = {name: equal_weight for name in selected_factors}
        
        # 6. æ‰§è¡ŒåŠ æƒåˆæˆ
        composite_factor_df = self._execute_weighted_synthesis(
            composite_factor_name,
            pool_index,
            factor_weights,
            snap_config_id
        )
        
        # 7. ç”Ÿæˆç»¼åˆæŠ¥å‘Šï¼ˆåŒ…å«ç­›é€‰å’Œåˆæˆä¿¡æ¯ï¼‰
        synthesis_report = self._generate_comprehensive_report(
            composite_factor_name,
            candidate_factor_names,
            selected_factors,
            factor_weights,
            selection_report
        )
        
        logger.info(f"âœ… ä¸“ä¸šICç­›é€‰å› å­åˆæˆå®Œæˆ: {composite_factor_name}")
        logger.info(f"ğŸ“Š æœ€ç»ˆåˆæˆæƒé‡åˆ†å¸ƒ:")
        for factor, weight in sorted(factor_weights.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"  {factor}: {weight:.1%}")
            
        return composite_factor_df, synthesis_report

    def calculate_rolling_weights(
            self,
            candidate_factor_names: List[str],
            stock_pool_index_name: str,
            calculation_date: str,
            resultLoadManager: ResultLoadManager
    ) -> Dict[str, float]:
        """
        æ»šåŠ¨æƒé‡è®¡ç®— - æ ¸å¿ƒæ”¹è¿›ï¼šå®Œå…¨é¿å…å‰è§†åå·®
        
        Args:
            candidate_factor_names: å€™é€‰å› å­åˆ—è¡¨
            stock_pool_index_name: è‚¡ç¥¨æ± åç§°
            calculation_date: æƒé‡è®¡ç®—æ—¶ç‚¹ï¼ˆä¸¥æ ¼ä¸ä½¿ç”¨æ­¤æ—¶ç‚¹ä¹‹åçš„æ•°æ®ï¼‰
            factor_data_source: å› å­æ•°æ®æº
            return_data_source: æ”¶ç›Šæ•°æ®æº
            
        Returns:
            Dict[factor_name, weight]: åŸºäºå†å²ICçš„æƒé‡åˆ†é…
        """
        logger.info(f"ğŸ”„ å¼€å§‹æ»šåŠ¨æƒé‡è®¡ç®— @ {calculation_date}")
        logger.info(f"ğŸ“Š å€™é€‰å› å­: {len(candidate_factor_names)} ä¸ª")

        # ç¬¬ä¸€æ­¥ï¼šè·å–æˆªæ­¢åˆ°calculation_dateçš„å†å²ICæ•°æ®
        historical_ic_stats = {}

        for factor_name in candidate_factor_names:
            try:
                # ä»æ»šåŠ¨ICç®¡ç†å™¨è·å–å†å²ICå¿«ç…§
                latest_snapshot = self.rolling_ic_manager.get_ic_at_timepoint(
                    factor_name, stock_pool_index_name, calculation_date
                )

                if latest_snapshot and latest_snapshot.ic_stats:
                    historical_ic_stats[factor_name] = latest_snapshot.ic_stats
                    logger.debug(f"  âœ… {factor_name}: è·å–å†å²IC @ {calculation_date}")
                else:
                    snapshot = self.rolling_ic_manager._calculate_ic_snapshot(
                        factor_name, stock_pool_index_name, calculation_date,
                        resultLoadManager
                    )
                    if snapshot and snapshot.ic_stats:
                        historical_ic_stats[factor_name] = snapshot.ic_stats
                        # ä¿å­˜å¿«ç…§ä»¥ä¾›åç»­ä½¿ç”¨
                        self.rolling_ic_manager._save_snapshot(snapshot)
                        logger.debug(f"  ğŸ”„ {factor_name}: å®æ—¶è®¡ç®—IC @ {calculation_date}")
                    else:
                        logger.warning(f"  âŒ {factor_name}: æ— æ³•è®¡ç®—å†å²IC--æ­£å¸¸ï¼šå› ä¸ºä¸æ»¡è¶³120ä¸ªè§‚æµ‹ç‚¹ï¼")
            except Exception as e:
                logger.error(f"  âŒ {factor_name}: ICè·å–å¤±è´¥ - {e}")
                continue

        if not historical_ic_stats:
            logger.error("âŒ æ— ä»»ä½•å› å­çš„å†å²ICæ•°æ®ï¼Œæ— æ³•è®¡ç®—æƒé‡")
            return {}

        logger.info(f"ğŸ“Š æˆåŠŸè·å– {len(historical_ic_stats)} ä¸ªå› å­çš„å†å²ICæ•°æ®")

        # ç¬¬äºŒæ­¥ï¼šåŸºäºå†å²ICè¿›è¡Œè´¨é‡ç­›é€‰
        qualified_factor_stats, quality_reports = self.quality_filter.filter_factors_by_quality(
            historical_ic_stats
        )

        if not qualified_factor_stats:
            logger.warning("âš ï¸ æ— å› å­é€šè¿‡è´¨é‡ç­›é€‰ï¼Œè¿”å›ç­‰æƒé‡")
            equal_weight = 1.0 / len(candidate_factor_names)
            return {name: equal_weight for name in candidate_factor_names}

        # ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—æƒé‡ï¼ˆä»…åŸºäºå†å²ICè¡¨ç°ï¼‰
        factor_weights = self.weight_calculator.calculate_ic_based_weights(
            qualified_factor_stats
        )

        # ç¬¬å››æ­¥ï¼šä¸ºæœªé€šè¿‡ç­›é€‰çš„å› å­åˆ†é…0æƒé‡
        final_weights = {}
        for factor_name in candidate_factor_names:
            final_weights[factor_name] = factor_weights.get(factor_name, 0.0)

        # æ—¥å¿—è®°å½•
        selected_factors = [name for name, weight in final_weights.items() if weight > 0]
        logger.info(f"âœ… æ»šåŠ¨æƒé‡è®¡ç®—å®Œæˆ @ {calculation_date}")
        logger.info(f"ğŸ“Š é€‰ä¸­å› å­: {len(selected_factors)}/{len(candidate_factor_names)}")

        for factor_name, weight in sorted(final_weights.items(), key=lambda x: x[1], reverse=True):
            if weight > 0:
                logger.info(f"  ğŸ¯ {factor_name}: {weight:.1%}")

        return final_weights

    # æ·±å…¥å‰–æ
    def _collect_factor_ic_stats(
            self,
            factor_names: List[str],
            stock_pool_index: str,
            force_recalculate: bool = False,
            snap_config_id: str = None
    ) -> Dict[str, Dict[str, Dict]]:
        """æ”¶é›†å› å­ICç»Ÿè®¡æ•°æ®"""
        logger.info("ğŸ“Š æ­£åœ¨æ”¶é›†å› å­ICç»Ÿè®¡æ•°æ®...")

        factor_ic_stats = {}

        for factor_name in factor_names:
            cache_key = f"{factor_name}_{stock_pool_index}"

            if not force_recalculate and cache_key in self._ic_stats_cache:
                factor_ic_stats[factor_name] = self._ic_stats_cache[cache_key]
                logger.debug(f"  ğŸ“¥ {factor_name}: ä½¿ç”¨ç¼“å­˜æ•°æ®")
                continue

            try:
                # ä»å·²ä¿å­˜çš„æµ‹è¯•ç»“æœä¸­è¯»å–ICç»Ÿè®¡
                ic_stats = self._load_factor_ic_stats(factor_name=factor_name,stock_pool_index= stock_pool_index, snap_config_id=snap_config_id)

                if ic_stats:
                    factor_ic_stats[factor_name] = ic_stats
                    self._ic_stats_cache[cache_key] = ic_stats
                    logger.debug(f"  âœ… {factor_name}: ICæ•°æ®åŠ è½½æˆåŠŸ")
                else:
                    logger.warning(f"  âš ï¸ {factor_name}: æœªæ‰¾åˆ°ICç»Ÿè®¡æ•°æ®ï¼Œè·³è¿‡")

            except Exception as e:
                raise ValueError(f"  âŒ {factor_name}: åŠ è½½ICæ•°æ®å¤±è´¥ - {e}")

        logger.info(f"ğŸ“Š ICæ•°æ®æ”¶é›†å®Œæˆ: {len(factor_ic_stats)}/{len(factor_names)} ä¸ªå› å­")
        return factor_ic_stats

    def _load_factor_ic_stats(self, factor_name: str, stock_pool_index: str, calcu_type='c2c', snap_config_id: str = None) -> Optional[Dict]:
        """
        ä»æ»šåŠ¨ICå­˜å‚¨ä¸­æå–å› å­çš„ICç»Ÿè®¡æ•°æ®
        Args:
            factor_name: å› å­åç§°
            stock_pool_index: è‚¡ç¥¨æ± ç´¢å¼•
            calcu_type: æ”¶ç›Šè®¡ç®—ç±»å‹ï¼Œé»˜è®¤'c2c'
            snap_config_id: é…ç½®å¿«ç…§IDï¼Œç”¨äºç¡®å®šç‰ˆæœ¬
            
        Returns:
            Dict[period, ic_stats]: å„å‘¨æœŸçš„ICç»Ÿè®¡æ•°æ®ï¼Œæ ¼å¼ä¸RollingICManagerä¸€è‡´
        """
        try:
            if snap_config_id is None:
                logger.warning(f"æœªæä¾›snap_config_idï¼Œæ— æ³•ç¡®å®šæ•°æ®ç‰ˆæœ¬")
                return None
                
            # 1. ä»é…ç½®å¿«ç…§è·å–ç‰ˆæœ¬ä¿¡æ¯
            config_manager = ConfigSnapshotManager()
            pool_index, start_date, end_date, config_evaluation = config_manager.get_snapshot_config_content_details(snap_config_id)
            version = f"{start_date}_{end_date}"
            
            # 2. æ„å»ºæ»šåŠ¨ICæ–‡ä»¶è·¯å¾„
            rolling_ic_dir = (self.main_work_path / stock_pool_index / factor_name / 
                             calcu_type / version / 'rolling_ic')
            
            if not rolling_ic_dir.exists():
                # å°±åœ°ç”ŸæˆICæ•°æ®å¹¶ä¿å­˜åˆ°æœ¬åœ°
                logger.info(f"æ»šåŠ¨ICç›®å½•ä¸å­˜åœ¨ï¼Œå¼€å§‹å°±åœ°ç”Ÿæˆ: {factor_name}")
                try:
                    # è°ƒç”¨ç”Ÿæˆå‡½æ•°ä¸ºå½“å‰å› å­ç”ŸæˆICæ•°æ®
                    run_cal_and_save_rolling_ic_by_snapshot_config_id(snap_config_id, [factor_name])
                    logger.info(f"âœ… æˆåŠŸç”Ÿæˆæ»šåŠ¨ICæ•°æ®: {factor_name}")
                    
                    # é‡æ–°æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
                    if not rolling_ic_dir.exists():
                        raise ValueError(f"for-{factor_name} ç”ŸæˆICæ•°æ®åç›®å½•ä»ä¸å­˜åœ¨: {rolling_ic_dir}")
                except Exception as e:
                    raise ValueError(f"ç”Ÿæˆæ»šåŠ¨ICæ•°æ®å¤±è´¥ {factor_name}: {e}")

            # 3. æŸ¥æ‰¾æ‰€æœ‰ICå¿«ç…§æ–‡ä»¶
            ic_files = list(rolling_ic_dir.glob("ic_snapshot_*.json"))
            if not ic_files:
                # å¦‚æœç›®å½•å­˜åœ¨ä½†æ— æ–‡ä»¶ï¼Œå¯èƒ½æ˜¯ICç”Ÿæˆä¸å®Œæ•´ï¼Œå°è¯•é‡æ–°ç”Ÿæˆ
                logger.warning(f"ICç›®å½•å­˜åœ¨ä½†æ— å¿«ç…§æ–‡ä»¶ï¼Œå°è¯•é‡æ–°ç”Ÿæˆ: {factor_name}")
                try:
                    run_cal_and_save_rolling_ic_by_snapshot_config_id(snap_config_id, [factor_name])
                    
                    # é‡æ–°æŸ¥æ‰¾æ–‡ä»¶
                    ic_files = list(rolling_ic_dir.glob("ic_snapshot_*.json"))
                    if not ic_files:
                        raise ValueError(f"é‡æ–°ç”Ÿæˆåä»æ— ICå¿«ç…§æ–‡ä»¶: {rolling_ic_dir}")
                    logger.info(f"âœ… é‡æ–°ç”ŸæˆICæ•°æ®æˆåŠŸ: {factor_name}")
                except Exception as e:
                    raise ValueError(f"é‡æ–°ç”ŸæˆICæ•°æ®å¤±è´¥ {factor_name}: {e}")

            logger.debug(f"æ‰¾åˆ° {len(ic_files)} ä¸ªICå¿«ç…§æ–‡ä»¶ for {factor_name}")
            
            # 4. åŠ è½½å¹¶èšåˆICç»Ÿè®¡æ•°æ®
            all_periods_stats = {}
            
            for ic_file in ic_files:
                try:
                    with open(ic_file, 'r', encoding='utf-8') as f:
                        snapshot_data = json.load(f)
                    
                    # æå–ic_statså­—æ®µ
                    ic_stats = snapshot_data.get('ic_stats', {})
                    
                    # èšåˆå„å‘¨æœŸçš„ç»Ÿè®¡æ•°æ®
                    for period, period_stats in ic_stats.items():
                        if period not in all_periods_stats:
                            all_periods_stats[period] = []
                        all_periods_stats[period].append(period_stats)
                        
                except Exception as e:
                    logger.warning(f"è¯»å–ICæ–‡ä»¶å¤±è´¥ {ic_file}: {e}")
                    continue
            
            if not all_periods_stats:
                logger.debug(f"æœªæ‰¾åˆ°æœ‰æ•ˆçš„ICç»Ÿè®¡æ•°æ®")
                return None
            
            # 5. è®¡ç®—èšåˆç»Ÿè®¡æŒ‡æ ‡
            aggregated_stats = {}
            for period, stats_list in all_periods_stats.items():
                if not stats_list:
                    continue
                    
                # è®¡ç®—æ—¶é—´åºåˆ—çš„å¹³å‡æŒ‡æ ‡
                ic_means = [s.get('ic_mean', 0) for s in stats_list if s.get('ic_mean') is not None]
                ic_stds = [s.get('ic_std', 0) for s in stats_list if s.get('ic_std') is not None]
                ic_irs = [s.get('ic_ir', 0) for s in stats_list if s.get('ic_ir') is not None]
                ic_win_rates = [s.get('ic_win_rate', 0.5) for s in stats_list if s.get('ic_win_rate') is not None]
                ic_p_values = [s.get('ic_p_value', 1.0) for s in stats_list if s.get('ic_p_value') is not None]
                ic_t_stats = [s.get('ic_t_stat', 0) for s in stats_list if s.get('ic_t_stat') is not None]
                
                if not ic_means:
                    continue
                
                # èšåˆç»Ÿè®¡
                aggregated_stats[period] = {
                    'ic_mean': np.mean(ic_means),
                    'ic_std': np.mean(ic_stds) if ic_stds else 0,
                    'ic_ir': np.mean(ic_irs) if ic_irs else 0,
                    'ic_win_rate': np.mean(ic_win_rates) if ic_win_rates else 0.5,
                    'ic_p_value': np.mean(ic_p_values) if ic_p_values else 1.0,
                    'ic_t_stat': np.mean(ic_t_stats) if ic_t_stats else 0,
                    'ic_count': len(ic_means),
                    'snapshot_count': len(stats_list),
                    'ic_mean_std': np.std(ic_means) if len(ic_means) > 1 else 0,  # ICå‡å€¼çš„ç¨³å®šæ€§
                    'ic_ir_std': np.std(ic_irs) if len(ic_irs) > 1 else 0  # IRçš„ç¨³å®šæ€§
                }
            
            if not aggregated_stats:
                logger.debug(f"èšåˆåæ— æœ‰æ•ˆç»Ÿè®¡æ•°æ®")
                return None
                
            logger.debug(f"æˆåŠŸæå–å› å­ {factor_name} çš„ICç»Ÿè®¡: {list(aggregated_stats.keys())} å‘¨æœŸ")
            return aggregated_stats
            
        except Exception as e:
            logger.error(f"åŠ è½½å› å­ICç»Ÿè®¡å¤±è´¥ {factor_name}: {e}")
            return None

    def _execute_weighted_synthesis(
            self,
            composite_factor_name: str,
            stock_pool_index_name: str,
            factor_weights: Dict[str, float],
            snap_config_id:str
    ) -> pd.DataFrame:
        """æ‰§è¡ŒåŠ æƒå› å­åˆæˆ"""
        logger.info(f"âš–ï¸ å¼€å§‹æ‰§è¡ŒåŠ æƒåˆæˆï¼Œä½¿ç”¨{len(factor_weights)}ä¸ªå› å­")

        processed_factors = []
        weights_list = []

        for factor_name, weight in factor_weights.items():#todo è¿™é‡Œ
            logger.info(f"  ğŸ”„ å¤„ç†å› å­: {factor_name} (æƒé‡: {weight:.3f})")

            # å¤„ç†å•ä¸ªå› å­
            processed_df = self.get_sub_factor_df_from_local(factor_name, stock_pool_index_name,snap_config_id)

            processed_factors.append(processed_df)
            weights_list.append(weight)

        if not processed_factors:
            raise ValueError("æ²¡æœ‰ä»»ä½•å› å­è¢«æˆåŠŸå¤„ç†")

        # åŠ æƒåˆæˆ
        composite_factor_df = self._weighted_combine_factors(processed_factors, weights_list)

        # æœ€ç»ˆæ ‡å‡†åŒ–
        composite_factor_df = self.processor._standardize_robust(composite_factor_df)

        logger.info(f"âœ… åŠ æƒåˆæˆå®Œæˆ: {composite_factor_name}")
        return composite_factor_df

    def _weighted_combine_factors(
            self,
            factor_dfs: List[pd.DataFrame],
            weights: List[float]
    ) -> pd.DataFrame:
        """åŠ æƒåˆå¹¶å› å­æ•°æ®æ¡†"""
        if len(factor_dfs) != len(weights):
            raise ValueError("å› å­æ•°é‡ä¸æƒé‡æ•°é‡ä¸åŒ¹é…")

        # ç¡®ä¿æƒé‡å½’ä¸€åŒ–
        weights = np.array(weights)
        weights = weights / weights.sum()

        # åŠ æƒåˆå¹¶
        result_df = None
        for i, (df, weight) in enumerate(zip(factor_dfs, weights)):
            weighted_df = df * weight

            if result_df is None:
                result_df = weighted_df
            else:
                result_df = result_df.add(weighted_df, fill_value=0)

        return result_df

    def _generate_synthesis_report(
            self,
            composite_factor_name: str,
            candidate_factors: List[str],
            final_weights: Dict[str, float],
            quality_reports: List[FactorQualityReport]
    ) -> Dict:
        """ç”Ÿæˆå› å­åˆæˆæŠ¥å‘Š"""

        # æŒ‰æƒé‡æ’åº
        sorted_weights = sorted(final_weights.items(), key=lambda x: x[1], reverse=True)

        report = {
            'composite_factor_name': composite_factor_name,
            'synthesis_timestamp': pd.Timestamp.now(),
            'candidate_factors_count': len(candidate_factors),
            'qualified_factors_count': len(final_weights),
            'final_weights': dict(sorted_weights),
            'top_3_factors': sorted_weights[:3],
            'config_used': {
                'min_ic_mean': self.config.min_ic_mean,
                'min_ic_ir': self.config.min_ic_ir,
                'min_ic_win_rate': self.config.min_ic_win_rate,
                'max_single_weight': self.config.max_single_weight
            },
            'quality_summary': {
                'passed': len([r for r in quality_reports if not r.risk_flags]),
                'failed': len([r for r in quality_reports if r.risk_flags]),
                'main_failure_reasons': self._summarize_failure_reasons(quality_reports)
            }
        }

        return report

    def _summarize_failure_reasons(self, quality_reports: List[FactorQualityReport]) -> Dict[str, int]:
        """æ±‡æ€»å¤±è´¥åŸå› ç»Ÿè®¡"""
        failure_counts = {}

        for report in quality_reports:
            for flag in report.risk_flags:
                reason = flag.split('(')[0]  # æå–åŸå› ä¸»ä½“
                failure_counts[reason] = failure_counts.get(reason, 0) + 1

        return failure_counts
    
    def _generate_comprehensive_report(
            self,
            composite_factor_name: str,
            candidate_factors: List[str],
            selected_factors: List[str],
            final_weights: Dict[str, float],
            selection_report: Dict
    ) -> Dict:
        """ç”ŸæˆåŒ…å«ç­›é€‰å’Œåˆæˆä¿¡æ¯çš„ç»¼åˆæŠ¥å‘Š"""
        
        # åŸºç¡€åˆæˆæŠ¥å‘Š
        base_report = self._generate_synthesis_report(
            composite_factor_name, candidate_factors, final_weights, []
        )
        
        # æ·»åŠ ä¸“ä¸šç­›é€‰ä¿¡æ¯
        comprehensive_report = {
            **base_report,
            'professional_selection': {
                'selection_method': 'RollingIC-based Professional Selection',
                'candidate_count': len(candidate_factors),
                'selected_count': len(selected_factors),
                'selection_rate': len(selected_factors) / len(candidate_factors) if candidate_factors else 0,
                'selected_factors': selected_factors,
                'selection_report': selection_report
            }
        }
        
        return comprehensive_report

    def print_synthesis_report(self, report: Dict):
        """æ‰“å°åˆæˆæŠ¥å‘Šï¼ˆæ”¯æŒä¸“ä¸šç­›é€‰å’Œä¼ ç»Ÿç­›é€‰ä¸¤ç§æ ¼å¼ï¼‰"""
        print(f"\n{'=' * 80}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸“ä¸šç­›é€‰æŠ¥å‘Š
        if 'professional_selection' in report:
            print(f"ğŸ“Š ä¸“ä¸šæ»šåŠ¨ICç­›é€‰+ICåŠ æƒåˆæˆæŠ¥å‘Š")
            print(f"{'=' * 80}")
            print(f"ğŸ¯ åˆæˆå› å­åç§°: {report['composite_factor_name']}")
            print(f"â° åˆæˆæ—¶é—´: {report['synthesis_timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
            
            # ä¸“ä¸šç­›é€‰ä¿¡æ¯
            prof_sel = report['professional_selection']
            print(f"\nğŸ” ä¸“ä¸šç­›é€‰ç»“æœ:")
            print(f"  ğŸ“ˆ å€™é€‰å› å­æ•°é‡: {prof_sel['candidate_count']}")
            print(f"  âœ… ç­›é€‰é€šè¿‡æ•°é‡: {prof_sel['selected_count']}")
            print(f"  ğŸ“Š ç­›é€‰é€šè¿‡ç‡: {prof_sel['selection_rate']:.1%}")
            print(f"  ğŸ† ç­›é€‰æ–¹æ³•: {prof_sel['selection_method']}")
            
            print(f"\nğŸ¯ æœ€ç»ˆé€‰ä¸­å› å­:")
            for i, factor in enumerate(prof_sel['selected_factors'], 1):
                weight = report['final_weights'].get(factor, 0)
                print(f"  {i:2d}. {factor:25s}: {weight:6.1%}")
                
        else:
            print(f"ğŸ“Š ICåŠ æƒå› å­åˆæˆæŠ¥å‘Š")
            print(f"{'=' * 80}")
            print(f"ğŸ¯ åˆæˆå› å­åç§°: {report['composite_factor_name']}")
            print(f"â° åˆæˆæ—¶é—´: {report['synthesis_timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"ğŸ“ˆ å€™é€‰å› å­æ•°é‡: {report['candidate_factors_count']}")
            print(f"âœ… é€šè¿‡ç­›é€‰æ•°é‡: {report['qualified_factors_count']}")

            print(f"\nğŸ† æœ€ç»ˆæƒé‡åˆ†é…:")
            for factor_name, weight in report['final_weights'].items():
                print(f"  {factor_name:25s}: {weight:6.1%}")

        print(f"\nğŸ¥‡ æƒé‡å‰ä¸‰å:")
        for i, (factor_name, weight) in enumerate(report.get('top_3_factors', []), 1):
            print(f"  {i}. {factor_name}: {weight:.1%}")

        # è´¨é‡ç­›é€‰ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'quality_summary' in report:
            quality_summary = report['quality_summary']
            print(f"\nğŸ“‹ è´¨é‡ç­›é€‰æ±‡æ€»:")
            print(f"  âœ… é€šè¿‡: {quality_summary['passed']} ä¸ª")
            print(f"  âŒ å¤±è´¥: {quality_summary['failed']} ä¸ª")

            if quality_summary['main_failure_reasons']:
                print(f"  ä¸»è¦å¤±è´¥åŸå› :")
                for reason, count in quality_summary['main_failure_reasons'].items():
                    print(f"    - {reason}: {count} ä¸ªå› å­")

        print(f"{'=' * 80}")

    def execute_orthogonalization_plan(
            self,
            orthogonalization_plan: List[Dict],
            stock_pool_index: str,
            snap_config_id: str
    ) -> Dict[str, pd.DataFrame]:
        """
        æ‰§è¡Œæ­£äº¤åŒ–æ”¹é€ è®¡åˆ’ - æ ¸å¿ƒåŠŸèƒ½ï¼šæˆªé¢çº¿æ€§å›å½’æ®‹å·®æå–
        
        Args:
            orthogonalization_plan: æ­£äº¤åŒ–è®¡åˆ’åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
                - original_factor: ç›®æ ‡å› å­åç§°
                - base_factor: åŸºå‡†å› å­åç§°
                - orthogonal_name: æ­£äº¤åŒ–åçš„æ–°å› å­åç§°
                - correlation: åŸå§‹ç›¸å…³æ€§
                - base_score: åŸºå‡†å› å­è¯„åˆ†
                - target_score: ç›®æ ‡å› å­è¯„åˆ†
            stock_pool_index: è‚¡ç¥¨æ± åç§°
            snap_config_id: é…ç½®å¿«ç…§ID
            
        Returns:
            Dict[orthogonal_name, orthogonal_factor_df]: æ­£äº¤åŒ–åçš„å› å­æ•°æ®
        """
        if not orthogonalization_plan:
            logger.info("âšª æ— æ­£äº¤åŒ–è®¡åˆ’ï¼Œè·³è¿‡æ‰§è¡Œ")
            return {}
            
        logger.info(f"ğŸ”§ å¼€å§‹æ‰§è¡Œæ­£äº¤åŒ–è®¡åˆ’ï¼Œå…± {len(orthogonalization_plan)} é¡¹")
        
        orthogonal_factors = {}
        
        for plan_item in orthogonalization_plan:
            try:
                orthogonal_factor_df, avg_r_squared = self._execute_single_orthogonalization(
                    plan_item, stock_pool_index, snap_config_id
                )
                
                if orthogonal_factor_df is not None:
                    orthogonal_factors[plan_item['orthogonal_name']] = orthogonal_factor_df
                    logger.info(f"âœ… æˆåŠŸç”Ÿæˆæ­£äº¤åŒ–å› å­: {plan_item['orthogonal_name']} (RÂ²={avg_r_squared:.3f})")
                else:
                    logger.warning(f"âš ï¸ æ­£äº¤åŒ–å¤±è´¥: {plan_item['orthogonal_name']}")
                    
            except Exception as e:
                logger.error(f"âŒ æ­£äº¤åŒ–æ‰§è¡Œå¼‚å¸¸ {plan_item['orthogonal_name']}: {e}")
                continue
        
        logger.info(f"ğŸ¯ æ­£äº¤åŒ–æ‰§è¡Œå®Œæˆï¼ŒæˆåŠŸç”Ÿæˆ {len(orthogonal_factors)} ä¸ªæ­£äº¤åŒ–å› å­")
        return orthogonal_factors

    def _execute_single_orthogonalization(
            self,
            plan_item: Dict,
            stock_pool_index: str,
            snap_config_id: str
    ) -> Tuple[Optional[pd.DataFrame], float]:
        """
        æ‰§è¡Œå•ä¸ªæ­£äº¤åŒ–æ”¹é€  - é€æ—¥æˆªé¢OLSå›å½’
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. åŠ è½½ç›®æ ‡å› å­å’ŒåŸºå‡†å› å­æ•°æ®
        2. é€æ—¥è¿›è¡Œæˆªé¢çº¿æ€§å›å½’ï¼štarget_factor = Î± + Î² * base_factor + Îµ
        3. æå–æ®‹å·®Îµä½œä¸ºæ­£äº¤åŒ–åçš„å› å­å€¼
        
        Args:
            plan_item: å•ä¸ªæ­£äº¤åŒ–è®¡åˆ’
            stock_pool_index: è‚¡ç¥¨æ± åç§°
            snap_config_id: é…ç½®å¿«ç…§ID
            
        Returns:
            (æ­£äº¤åŒ–åçš„å› å­DataFrame, å¹³å‡RÂ²): ç”¨äºICè°ƒæ•´
        """
        target_factor = plan_item['original_factor']
        base_factor = plan_item['base_factor']
        orthogonal_name = plan_item['orthogonal_name']
        
        logger.debug(f"  ğŸ”„ æ‰§è¡Œæ­£äº¤åŒ–: {target_factor} vs {base_factor} -> {orthogonal_name}")
        
        try:
            # 1. åŠ è½½å› å­æ•°æ®
            target_df = self.get_sub_factor_df_from_local(target_factor, stock_pool_index, snap_config_id)
            base_df = self.get_sub_factor_df_from_local(base_factor, stock_pool_index, snap_config_id)
            
            if target_df is None or base_df is None:
                logger.error(f"  âŒ æ— æ³•åŠ è½½å› å­æ•°æ®: target={target_df is not None}, base={base_df is not None}")
                return None, 0.0
            
            # 2. æ•°æ®å¯¹é½å’Œé¢„å¤„ç†
            aligned_target, aligned_base = self._align_factor_data(target_df, base_df)
            
            if aligned_target.empty or aligned_base.empty:
                logger.error("  âŒ å› å­æ•°æ®å¯¹é½åä¸ºç©º")
                return None, 0.0
            
            # 3. é€æ—¥æˆªé¢å›å½’ï¼Œè·å–RÂ²ç”¨äºICè°ƒæ•´
            orthogonal_df, avg_r_squared = self._daily_cross_sectional_orthogonalization(
                aligned_target, aligned_base, orthogonal_name
            )
            
            # è®°å½•RÂ²ä¿¡æ¯ç”¨äºåç»­ICè°ƒæ•´
            plan_item['avg_r_squared'] = avg_r_squared
            
            return orthogonal_df, avg_r_squared
            
        except Exception as e:
            logger.error(f"  âŒ å•é¡¹æ­£äº¤åŒ–å¤±è´¥ {orthogonal_name}: {e}")
            return None, 0.0

    def _align_factor_data(
            self,
            target_df: pd.DataFrame,
            base_df: pd.DataFrame
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        å› å­æ•°æ®å¯¹é½ - ç¡®ä¿æ—¶é—´å’Œè‚¡ç¥¨ç»´åº¦ä¸€è‡´
        
        Args:
            target_df: ç›®æ ‡å› å­æ•°æ®
            base_df: åŸºå‡†å› å­æ•°æ®
            
        Returns:
            (aligned_target, aligned_base): å¯¹é½åçš„æ•°æ®
        """
        # æ‰¾åˆ°å…±åŒçš„æ—¶é—´å’Œè‚¡ç¥¨
        common_dates = target_df.index.intersection(base_df.index)
        common_stocks = target_df.columns.intersection(base_df.columns)
        
        if len(common_dates) == 0 or len(common_stocks) == 0:
            logger.error(f"  âŒ æ— å…±åŒæ—¶é—´ç‚¹æˆ–è‚¡ç¥¨ï¼šæ—¥æœŸ={len(common_dates)}, è‚¡ç¥¨={len(common_stocks)}")
            return pd.DataFrame(), pd.DataFrame()
        
        # æ•°æ®å¯¹é½
        aligned_target = target_df.loc[common_dates, common_stocks]
        aligned_base = base_df.loc[common_dates, common_stocks]
        
        logger.debug(f"  ğŸ“Š æ•°æ®å¯¹é½å®Œæˆï¼š{len(common_dates)}ä¸ªäº¤æ˜“æ—¥, {len(common_stocks)}åªè‚¡ç¥¨")
        
        return aligned_target, aligned_base

    def _daily_cross_sectional_orthogonalization(
            self,
            target_df: pd.DataFrame,
            base_df: pd.DataFrame,
            orthogonal_name: str
    ) -> Tuple[pd.DataFrame, float]:
        """
        é€æ—¥æˆªé¢æ­£äº¤åŒ– - æ ¸å¿ƒç®—æ³•å®ç°
        
        å¯¹æ¯ä¸ªäº¤æ˜“æ—¥ï¼Œæ‰§è¡Œæˆªé¢å›å½’ï¼štarget[t,i] = Î±[t] + Î²[t] * base[t,i] + Îµ[t,i]
        æå–æ®‹å·®Îµ[t,i]ä½œä¸ºæ­£äº¤åŒ–åçš„å› å­å€¼
        
        Args:
            target_df: ç›®æ ‡å› å­æ•°æ® (æ—¥æœŸÃ—è‚¡ç¥¨)
            base_df: åŸºå‡†å› å­æ•°æ® (æ—¥æœŸÃ—è‚¡ç¥¨) 
            orthogonal_name: æ­£äº¤åŒ–å› å­åç§°
            
        Returns:
            (orthogonal_df, avg_r_squared): æ­£äº¤åŒ–åçš„å› å­DataFrame å’Œ å¹³å‡RÂ²
        """
        logger.debug(f"  ğŸ§® å¼€å§‹é€æ—¥æˆªé¢å›å½’ï¼Œå…±{len(target_df)}ä¸ªäº¤æ˜“æ—¥")
        
        # åˆå§‹åŒ–ç»“æœDataFrame
        orthogonal_df = pd.DataFrame(
            index=target_df.index,
            columns=target_df.columns,
            dtype=np.float64
        )
        
        successful_regressions = 0
        r_squared_list = []
        
        # é€æ—¥å›å½’
        for date in target_df.index:
            try:
                # æå–å½“æ—¥æˆªé¢æ•°æ®
                y_cross = target_df.loc[date]  # ç›®æ ‡å› å­çš„æ¨ªæˆªé¢
                x_cross = base_df.loc[date]    # åŸºå‡†å› å­çš„æ¨ªæˆªé¢
                
                # ç§»é™¤ç¼ºå¤±å€¼
                valid_mask = (~y_cross.isna()) & (~x_cross.isna())
                
                if valid_mask.sum() < 10:  # è‡³å°‘éœ€è¦10ä¸ªæœ‰æ•ˆè§‚æµ‹
                    logger.debug(f"    âš ï¸ {date}: æœ‰æ•ˆè§‚æµ‹ä¸è¶³({valid_mask.sum()}ä¸ª)ï¼Œè·³è¿‡")
                    continue
                
                y_valid = y_cross[valid_mask]
                x_valid = x_cross[valid_mask]
                
                # æ‰§è¡Œæˆªé¢OLSå›å½’ï¼šy = Î± + Î²*x + Îµ
                residuals, r_squared = self._perform_cross_sectional_ols(y_valid, x_valid, date)
                
                if residuals is not None:
                    # ç«‹å³è¿›è¡Œæˆªé¢æ ‡å‡†åŒ–ï¼ˆä¼˜åŒ–å»ºè®®ï¼‰
                    if len(residuals) >= 5:  # è‡³å°‘éœ€è¦5ä¸ªæœ‰æ•ˆå€¼
                        mean_val = residuals.mean()
                        std_val = residuals.std()
                        
                        if std_val > 1e-8:  # é¿å…é™¤é›¶
                            standardized_residuals = (residuals - mean_val) / std_val
                            orthogonal_df.loc[date, standardized_residuals.index] = standardized_residuals.values
                            successful_regressions += 1
                            
                            # æ”¶é›†RÂ²ç”¨äºICè°ƒæ•´
                            if r_squared is not None:
                                r_squared_list.append(r_squared)
                
            except Exception as e:
                logger.debug(f"    âŒ {date}: å›å½’å¤±è´¥ - {e}")
                continue
        
        if successful_regressions == 0:
            logger.error("  âŒ æ‰€æœ‰æ—¥æœŸçš„å›å½’éƒ½å¤±è´¥äº†")
            return pd.DataFrame(), 0.0
        
        success_rate = successful_regressions / len(target_df)
        avg_r_squared = np.mean(r_squared_list) if r_squared_list else 0.0
        
        logger.debug(f"  âœ… æˆªé¢å›å½’å®Œæˆï¼šæˆåŠŸç‡ {success_rate:.1%} ({successful_regressions}/{len(target_df)})")
        logger.debug(f"  ğŸ“Š å¹³å‡RÂ²: {avg_r_squared:.3f} (ç”¨äºICè°ƒæ•´)")
        
        return orthogonal_df, avg_r_squared

    def _perform_cross_sectional_ols(
            self,
            y: pd.Series,
            x: pd.Series,
            date: str = None
    ) -> Tuple[Optional[pd.Series], Optional[float]]:
        """
        æ‰§è¡Œå•æ—¥æˆªé¢OLSå›å½’å¹¶æå–æ®‹å·®
        
        å›å½’æ–¹ç¨‹ï¼šy = Î± + Î²*x + Îµ
        é‡è¦ï¼šæ‰‹åŠ¨ä¸ºè‡ªå˜é‡æ·»åŠ å¸¸æ•°é¡¹ï¼Œç¡®ä¿æˆªè·é¡¹æ­£ç¡®ä¼°è®¡
        
        Args:
            y: å› å˜é‡ï¼ˆç›®æ ‡å› å­çš„æˆªé¢æ•°æ®ï¼‰
            x: è‡ªå˜é‡ï¼ˆåŸºå‡†å› å­çš„æˆªé¢æ•°æ®ï¼‰
            date: äº¤æ˜“æ—¥æœŸï¼ˆç”¨äºè°ƒè¯•ï¼‰
            
        Returns:
            (æ®‹å·®åºåˆ—, RÂ²å€¼): æ­£äº¤åŒ–åçš„å› å­å€¼å’Œå›å½’æ‹Ÿåˆåº¦
        """
        try:
            # æ‰‹åŠ¨æ·»åŠ å¸¸æ•°é¡¹ - è¿™æ˜¯å…³é”®æ­¥éª¤ï¼
            X_with_const = sm.add_constant(x)
            
            # æ‰§è¡ŒOLSå›å½’
            model = sm.OLS(y, X_with_const).fit()
            
            # æå–æ®‹å·®å’ŒRÂ²
            residuals = model.resid
            r_squared = model.rsquared
            
            # æ£€æŸ¥å›å½’è´¨é‡
            if r_squared > 0.95:  # è¿‡é«˜çš„RÂ²å¯èƒ½è¡¨ç¤ºæ•°æ®é—®é¢˜
                logger.debug(f"    âš ï¸ {date}: RÂ²å¼‚å¸¸é«˜({r_squared:.3f})ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®é—®é¢˜")
            
            return residuals, r_squared
            
        except Exception as e:
            # å›é€€åˆ°sklearnå®ç°
            logger.debug(f"    âš ï¸ statsmodelså›å½’å¤±è´¥ï¼Œå°è¯•sklearn: {e}")
            try:
                residuals, r_squared = self._perform_ols_sklearn_fallback(y, x)
                return residuals, r_squared
            except Exception as e2:
                logger.debug(f"    âŒ sklearnå›å½’ä¹Ÿå¤±è´¥: {e2}")
                return None, None

    def _perform_ols_sklearn_fallback(
            self,
            y: pd.Series,
            x: pd.Series
    ) -> Tuple[Optional[pd.Series], Optional[float]]:
        """
        sklearnå›å½’å¤‡ç”¨æ–¹æ¡ˆ
        
        Args:
            y: å› å˜é‡
            x: è‡ªå˜é‡
            
        Returns:
            (æ®‹å·®åºåˆ—, RÂ²å€¼): æ®‹å·®å’Œæ‹Ÿåˆåº¦
        """
        try:
            # sklearnä¼šè‡ªåŠ¨æ·»åŠ æˆªè·é¡¹ï¼ˆå¦‚æœfit_intercept=Trueï¼‰
            reg = LinearRegression(fit_intercept=True)
            
            # reshapeæ•°æ®
            X = x.values.reshape(-1, 1)
            y_values = y.values
            
            # æ‹Ÿåˆæ¨¡å‹
            reg.fit(X, y_values)
            
            # è®¡ç®—é¢„æµ‹å€¼å’Œæ®‹å·®
            y_pred = reg.predict(X)
            residuals = y_values - y_pred
            
            # è®¡ç®—RÂ²
            r_squared = reg.score(X, y_values)
            
            # è¿”å›pandas Seriesæ ¼å¼å’ŒRÂ²
            residuals_series = pd.Series(residuals, index=y.index)
            return residuals_series, r_squared
            
        except Exception as e:
            logger.debug(f"    âŒ sklearnå›å½’å¤±è´¥: {e}")
            return None, None

    def _standardize_orthogonal_factor(
            self,
            orthogonal_df: pd.DataFrame
    ) -> pd.DataFrame:
        """
        æ ‡å‡†åŒ–æ­£äº¤åŒ–å› å­
        
        åº”ç”¨æˆªé¢æ ‡å‡†åŒ–ï¼šæ¯ä¸ªäº¤æ˜“æ—¥å†…ï¼Œå› å­å€¼æ ‡å‡†åŒ–ä¸ºå‡å€¼0ã€æ ‡å‡†å·®1
        
        Args:
            orthogonal_df: åŸå§‹æ­£äº¤åŒ–å› å­
            
        Returns:
            æ ‡å‡†åŒ–åçš„æ­£äº¤åŒ–å› å­
        """
        if orthogonal_df.empty:
            return orthogonal_df
        
        logger.debug("  ğŸ“ å¼€å§‹å› å­æ ‡å‡†åŒ–")
        
        standardized_df = orthogonal_df.copy()
        
        # é€æ—¥æ ‡å‡†åŒ–
        for date in orthogonal_df.index:
            date_values = orthogonal_df.loc[date]
            valid_values = date_values.dropna()
            
            if len(valid_values) < 5:  # è‡³å°‘éœ€è¦5ä¸ªæœ‰æ•ˆå€¼
                continue
            
            # Z-Scoreæ ‡å‡†åŒ–
            mean_val = valid_values.mean()
            std_val = valid_values.std()
            
            if std_val > 1e-8:  # é¿å…é™¤é›¶
                standardized_values = (valid_values - mean_val) / std_val
                standardized_df.loc[date, valid_values.index] = standardized_values
        
        logger.debug("  âœ… å› å­æ ‡å‡†åŒ–å®Œæˆ")
        return standardized_df

    def _adjust_ic_stats_by_r_squared(
            self,
            original_ic_stats: Dict[str, Dict],
            avg_r_squared: float,
            orthogonal_factor_name: str
    ) -> Dict[str, Dict]:
        """
        åŸºäºRÂ²è°ƒæ•´æ­£äº¤åŒ–å› å­çš„ICç»Ÿè®¡ - æ ¸å¿ƒä¿®æ­£æ–¹æ³•
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        æ­£äº¤åŒ–åçš„å› å­æ˜¯æ®‹å·®ï¼Œå…¶é¢„æµ‹èƒ½åŠ›çº¦ç­‰äº (1 - RÂ²) * åŸå§‹é¢„æµ‹èƒ½åŠ›
        è¿™æ˜¯å› ä¸ºRÂ²è¡¨ç¤ºè¢«åŸºå‡†å› å­è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹
        
        Args:
            original_ic_stats: åŸå§‹å› å­çš„ICç»Ÿè®¡æ•°æ®
            avg_r_squared: å¹³å‡RÂ²å€¼ï¼ˆæ¥è‡ªé€æ—¥å›å½’ï¼‰
            orthogonal_factor_name: æ­£äº¤åŒ–å› å­åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            è°ƒæ•´åçš„ICç»Ÿè®¡æ•°æ®
        """
        if avg_r_squared <= 0 or avg_r_squared >= 1:
            logger.warning(f"  âš ï¸ {orthogonal_factor_name}: å¼‚å¸¸RÂ²å€¼({avg_r_squared:.3f})ï¼Œä½¿ç”¨åŸå§‹IC")
            return original_ic_stats
        
        # ICè°ƒæ•´å› å­ï¼šæ®‹å·®çš„é¢„æµ‹èƒ½åŠ› â‰ˆ (1 - RÂ²) * åŸå§‹é¢„æµ‹èƒ½åŠ›
        ic_adjustment_factor = 1 - avg_r_squared
        
        logger.debug(f"  ğŸ“Š {orthogonal_factor_name}: RÂ²={avg_r_squared:.3f}, ICè°ƒæ•´ç³»æ•°={ic_adjustment_factor:.3f}")
        
        adjusted_ic_stats = {}
        
        for period, period_stats in original_ic_stats.items():
            adjusted_period_stats = {}
            
            # è°ƒæ•´ä¸»è¦ICæŒ‡æ ‡
            for key, value in period_stats.items():
                if key in ['ic_mean', 'ic_ir']:
                    # ICå‡å€¼å’ŒIRéœ€è¦æŒ‰è°ƒæ•´ç³»æ•°ç¼©æ”¾
                    adjusted_value = value * ic_adjustment_factor
                    adjusted_period_stats[key] = adjusted_value
                elif key in ['ic_win_rate']:
                    # èƒœç‡çš„è°ƒæ•´æ›´å¤æ‚ï¼šå‘50%å›å½’
                    original_win_rate = value
                    # æ­£äº¤åŒ–ä¼šé™ä½èƒœç‡çš„æç«¯æ€§
                    adjusted_win_rate = 0.5 + (original_win_rate - 0.5) * ic_adjustment_factor
                    adjusted_period_stats[key] = adjusted_win_rate
                elif key in ['ic_std', 'ic_volatility']:
                    # æ³¢åŠ¨ç‡å¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ï¼Œä½†é€šå¸¸å‡å°‘ï¼ˆå› ä¸ºå»é™¤äº†éƒ¨åˆ†ç³»ç»Ÿæ€§ä¿¡æ¯ï¼‰
                    adjusted_period_stats[key] = value * np.sqrt(ic_adjustment_factor)
                elif key in ['ic_p_value', 't_stat']:
                    # ç»Ÿè®¡æ˜¾è‘—æ€§ä¼šé™ä½ï¼ˆå› ä¸ºä¿¡å·å¼ºåº¦å‡å¼±ï¼‰
                    if key == 't_stat':
                        adjusted_period_stats[key] = value * ic_adjustment_factor
                    else:  # p_value
                        # på€¼å˜å¤§ï¼ˆæ˜¾è‘—æ€§é™ä½ï¼‰
                        adjusted_period_stats[key] = min(1.0, value / ic_adjustment_factor) if ic_adjustment_factor > 0 else 1.0
                else:
                    # å…¶ä»–æŒ‡æ ‡ä¿æŒä¸å˜
                    adjusted_period_stats[key] = value
            
            adjusted_ic_stats[period] = adjusted_period_stats
        
        # è®°å½•è°ƒæ•´æ•ˆæœ
        original_main_ic = original_ic_stats.get('5d', {}).get('ic_mean', 0)
        adjusted_main_ic = adjusted_ic_stats.get('5d', {}).get('ic_mean', 0)
        
        logger.info(f"  ğŸ”„ {orthogonal_factor_name}: ICè°ƒæ•´ {original_main_ic:.4f} -> {adjusted_main_ic:.4f} "
                   f"(è°ƒæ•´å¹…åº¦: {(1-ic_adjustment_factor)*100:.1f}%)")
        
        return adjusted_ic_stats

    def synthesize_with_orthogonalization(
            self,
            composite_factor_name: str,
            candidate_factor_names: List[str],
            snap_config_id: str,
            force_generate_ic: bool = False
    ) -> Tuple[pd.DataFrame, Dict]:
        """
        å¸¦æ­£äº¤åŒ–çš„ä¸“ä¸šå› å­åˆæˆæµç¨‹
        
        å®Œæ•´æµç¨‹ï¼š
        1. ä¸“ä¸šç­›é€‰ï¼ˆçº¢è‰²åŒºåŸŸæ·˜æ±° + é»„è‰²åŒºåŸŸæ­£äº¤åŒ–è®¡åˆ’ï¼‰
        2. æ‰§è¡Œæ­£äº¤åŒ–æ”¹é€ è®¡åˆ’
        3. åŸºäºå¤„ç†åçš„å› å­è¿›è¡ŒICåŠ æƒåˆæˆ
        
        Args:
            composite_factor_name: å¤åˆå› å­åç§°
            candidate_factor_names: å€™é€‰å› å­åˆ—è¡¨
            snap_config_id: é…ç½®å¿«ç…§ID
            force_generate_ic: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”ŸæˆICæ•°æ®
            
        Returns:
            (composite_factor_df, synthesis_report)
        """
        logger.info(f"\nğŸš€ å¯åŠ¨å¸¦æ­£äº¤åŒ–çš„ä¸“ä¸šå› å­åˆæˆ: {composite_factor_name}")
        logger.info(f"ğŸ“Š å€™é€‰å› å­æ•°é‡: {len(candidate_factor_names)}")
        
        # 1. åˆå§‹åŒ–ä¸“ä¸šç­›é€‰å™¨
        if self.factor_selector is None:
            self.factor_selector = RollingICFactorSelector(snap_config_id, self.selector_config)
            logger.info("âœ… æ»šåŠ¨ICå› å­ç­›é€‰å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # 2. æ‰§è¡Œå®Œæ•´çš„ä¸“ä¸šç­›é€‰æµç¨‹ï¼ˆåŒ…å«æ­£äº¤åŒ–è®¡åˆ’ç”Ÿæˆï¼‰
        selected_factors, selection_report = self.factor_selector.run_complete_selection(
            candidate_factor_names, force_generate_ic
        )
        
        if not selected_factors:
            raise ValueError("âŒ ä¸“ä¸šç­›é€‰æœªé€‰å‡ºä»»ä½•å› å­ï¼Œæ— æ³•è¿›è¡Œåˆæˆ")
        
        # 3. è·å–æ­£äº¤åŒ–è®¡åˆ’
        orthogonalization_plan = selection_report.get('orthogonalization_plan', [])
        logger.info(f"ğŸ“‹ è·å–åˆ° {len(orthogonalization_plan)} é¡¹æ­£äº¤åŒ–è®¡åˆ’")
        
        # 4. æ‰§è¡Œæ­£äº¤åŒ–æ”¹é€ 
        config_manager = ConfigSnapshotManager()
        pool_index, start_date, end_date, config_evaluation = config_manager.get_snapshot_config_content_details(snap_config_id)
        
        orthogonal_factors = {}
        if orthogonalization_plan:
            orthogonal_factors = self.execute_orthogonalization_plan(
                orthogonalization_plan, pool_index, snap_config_id
            )
        
        # 5. æ„å»ºæœ€ç»ˆå› å­åˆ—è¡¨ï¼ˆåŸå§‹ç­›é€‰å› å­ + æ­£äº¤åŒ–å› å­ï¼‰
        final_factor_list = selected_factors.copy()
        
        # æ›¿æ¢è¢«æ­£äº¤åŒ–çš„å› å­
        for plan_item in orthogonalization_plan:
            original_factor = plan_item['original_factor']
            orthogonal_name = plan_item['orthogonal_name']
            
            if original_factor in final_factor_list and orthogonal_name in orthogonal_factors:
                final_factor_list.remove(original_factor)
                final_factor_list.append(orthogonal_name)
                logger.info(f"ğŸ”„ å› å­æ›¿æ¢: {original_factor} -> {orthogonal_name}")
        
        logger.info(f"ğŸ¯ æœ€ç»ˆå› å­åˆ—è¡¨: {len(final_factor_list)} ä¸ªå› å­")
        
        # 6. åŸºäºæœ€ç»ˆå› å­åˆ—è¡¨è®¡ç®—ICæƒé‡ï¼ˆä¿®æ­£åçš„é€»è¾‘ï¼‰
        factor_ic_stats = {}
        for factor_name in final_factor_list:
            try:
                # æ£€æŸ¥æ˜¯å¦ä¸ºæ­£äº¤åŒ–å› å­
                is_orthogonal_factor = False
                original_factor = factor_name
                avg_r_squared = 0.0
                
                # æŸ¥æ‰¾å¯¹åº”çš„æ­£äº¤åŒ–è®¡åˆ’é¡¹
                for plan_item in orthogonalization_plan:
                    if plan_item['orthogonal_name'] == factor_name:
                        is_orthogonal_factor = True
                        original_factor = plan_item['original_factor']
                        avg_r_squared = plan_item.get('avg_r_squared', 0.0)
                        break
                
                # åŠ è½½åŸå§‹å› å­çš„ICç»Ÿè®¡
                ic_stats = self._load_factor_ic_stats(
                    original_factor, pool_index, snap_config_id=snap_config_id
                )
                
                if ic_stats:
                    if is_orthogonal_factor and avg_r_squared > 0:
                        # ğŸ¯ æ ¸å¿ƒä¿®æ­£ï¼šåŸºäºRÂ²è°ƒæ•´æ­£äº¤åŒ–å› å­çš„ICç»Ÿè®¡
                        logger.info(f"  ğŸ”§ æ­£äº¤åŒ–å› å­ICè°ƒæ•´: {factor_name}")
                        adjusted_ic_stats = self._adjust_ic_stats_by_r_squared(
                            ic_stats, avg_r_squared, factor_name
                        )
                        factor_ic_stats[factor_name] = adjusted_ic_stats
                    else:
                        # åŸå§‹å› å­ç›´æ¥ä½¿ç”¨
                        factor_ic_stats[factor_name] = ic_stats
                        logger.debug(f"  ğŸ“Š åŸå§‹å› å­: {factor_name}")
                    
            except Exception as e:
                logger.error(f"  âŒ {factor_name}: ICç»Ÿè®¡å¤„ç†å¼‚å¸¸ - {e}")
        
        # 7. è®¡ç®—æœ€ç»ˆæƒé‡
        if factor_ic_stats:
            factor_weights = self.weight_calculator.calculate_ic_based_weights(factor_ic_stats)
        else:
            logger.warning("âš ï¸ æ— æ³•è·å–ICç»Ÿè®¡ï¼Œä½¿ç”¨ç­‰æƒé‡åˆæˆ")
            equal_weight = 1.0 / len(final_factor_list)
            factor_weights = {name: equal_weight for name in final_factor_list}
        
        # 8. æ‰§è¡ŒåŠ æƒåˆæˆï¼ˆæ”¯æŒæ­£äº¤åŒ–å› å­ï¼‰
        composite_factor_df = self._execute_weighted_synthesis_with_orthogonal(
            composite_factor_name, pool_index, factor_weights, orthogonal_factors, snap_config_id
        )
        
        # 9. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        synthesis_report = self._generate_orthogonalization_report(
            composite_factor_name,
            candidate_factor_names,
            selected_factors,
            final_factor_list,
            factor_weights,
            orthogonalization_plan,
            selection_report
        )
        
        logger.info(f"âœ… å¸¦æ­£äº¤åŒ–çš„ä¸“ä¸šå› å­åˆæˆå®Œæˆ: {composite_factor_name}")
        return composite_factor_df, synthesis_report

    def _execute_weighted_synthesis_with_orthogonal(
            self,
            composite_factor_name: str,
            stock_pool_index_name: str,
            factor_weights: Dict[str, float],
            orthogonal_factors: Dict[str, pd.DataFrame],
            snap_config_id: str
    ) -> pd.DataFrame:
        """
        æ”¯æŒæ­£äº¤åŒ–å› å­çš„åŠ æƒåˆæˆ
        
        Args:
            composite_factor_name: å¤åˆå› å­åç§°
            stock_pool_index_name: è‚¡ç¥¨æ± åç§°
            factor_weights: å› å­æƒé‡
            orthogonal_factors: æ­£äº¤åŒ–å› å­æ•°æ® {name: df}
            snap_config_id: é…ç½®å¿«ç…§ID
            
        Returns:
            åˆæˆåçš„å› å­DataFrame
        """
        logger.info(f"âš–ï¸ å¼€å§‹æ‰§è¡Œæ”¯æŒæ­£äº¤åŒ–çš„åŠ æƒåˆæˆï¼Œä½¿ç”¨{len(factor_weights)}ä¸ªå› å­")
        
        processed_factors = []
        weights_list = []
        
        for factor_name, weight in factor_weights.items():
            logger.info(f"  ğŸ”„ å¤„ç†å› å­: {factor_name} (æƒé‡: {weight:.3f})")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ­£äº¤åŒ–å› å­
            if factor_name in orthogonal_factors:
                logger.debug(f"    ğŸ“ ä½¿ç”¨æ­£äº¤åŒ–å› å­æ•°æ®: {factor_name}")
                processed_df = orthogonal_factors[factor_name]
            else:
                logger.debug(f"    ğŸ“Š ä»æœ¬åœ°åŠ è½½åŸå§‹å› å­: {factor_name}")
                processed_df = self.get_sub_factor_df_from_local(factor_name, stock_pool_index_name, snap_config_id)
            
            if processed_df is not None and not processed_df.empty:
                processed_factors.append(processed_df)
                weights_list.append(weight)
            else:
                logger.warning(f"    âš ï¸ å› å­æ•°æ®æ— æ•ˆï¼Œè·³è¿‡: {factor_name}")
        
        if not processed_factors:
            raise ValueError("æ²¡æœ‰ä»»ä½•å› å­è¢«æˆåŠŸå¤„ç†")
        
        # åŠ æƒåˆæˆ
        composite_factor_df = self._weighted_combine_factors(processed_factors, weights_list)
        
        # æœ€ç»ˆæ ‡å‡†åŒ–
        composite_factor_df = self.processor._standardize_robust(composite_factor_df)
        
        logger.info(f"âœ… æ”¯æŒæ­£äº¤åŒ–çš„åŠ æƒåˆæˆå®Œæˆ: {composite_factor_name}")
        return composite_factor_df

    def _generate_orthogonalization_report(
            self,
            composite_factor_name: str,
            candidate_factors: List[str],
            selected_factors: List[str],
            final_factor_list: List[str],
            factor_weights: Dict[str, float],
            orthogonalization_plan: List[Dict],
            selection_report: Dict
    ) -> Dict:
        """
        ç”ŸæˆåŒ…å«æ­£äº¤åŒ–ä¿¡æ¯çš„ç»¼åˆæŠ¥å‘Š
        
        Args:
            composite_factor_name: å¤åˆå› å­åç§°
            candidate_factors: å€™é€‰å› å­åˆ—è¡¨
            selected_factors: åˆæ­¥ç­›é€‰å› å­åˆ—è¡¨
            final_factor_list: æœ€ç»ˆå› å­åˆ—è¡¨ï¼ˆç»æ­£äº¤åŒ–å¤„ç†åï¼‰
            factor_weights: æœ€ç»ˆæƒé‡
            orthogonalization_plan: æ­£äº¤åŒ–è®¡åˆ’
            selection_report: ç­›é€‰æŠ¥å‘Š
            
        Returns:
            ç»¼åˆæŠ¥å‘Š
        """
        # åŸºç¡€æŠ¥å‘Š
        base_report = self._generate_comprehensive_report(
            composite_factor_name, candidate_factors, final_factor_list, factor_weights, selection_report
        )
        
        # æ·»åŠ æ­£äº¤åŒ–ä¿¡æ¯
        orthogonalization_info = {
            'orthogonalization_enabled': True,
            'orthogonalization_plan_count': len(orthogonalization_plan),
            'orthogonalization_details': []
        }
        
        for plan_item in orthogonalization_plan:
            orthogonalization_info['orthogonalization_details'].append({
                'original_factor': plan_item['original_factor'],
                'base_factor': plan_item['base_factor'],
                'orthogonal_name': plan_item['orthogonal_name'],
                'original_correlation': plan_item.get('correlation', 0),
                'base_score': plan_item.get('base_score', 0),
                'target_score': plan_item.get('target_score', 0)
            })
        
        # åˆå¹¶æŠ¥å‘Š
        comprehensive_report = {
            **base_report,
            'orthogonalization': orthogonalization_info,
            'factor_transformation_summary': {
                'initial_selected_count': len(selected_factors),
                'orthogonalized_count': len(orthogonalization_plan),
                'final_factor_count': len(final_factor_list),
                'replacement_mapping': {
                    plan['original_factor']: plan['orthogonal_name']
                    for plan in orthogonalization_plan
                }
            }
        }
        
        return comprehensive_report
