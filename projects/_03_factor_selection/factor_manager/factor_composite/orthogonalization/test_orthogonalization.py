"""
æ­£äº¤åŒ–åŠŸèƒ½æµ‹è¯•è„šæœ¬

æœ¬è„šæœ¬ç”¨äºæµ‹è¯•æ–°å®ç°çš„æ­£äº¤åŒ–åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. æ­£äº¤åŒ–è®¡åˆ’çš„æ‰§è¡Œ
2. æˆªé¢çº¿æ€§å›å½’çš„æ­£ç¡®æ€§
3. æ®‹å·®æå–çš„æœ‰æ•ˆæ€§
4. å®Œæ•´çš„æ­£äº¤åŒ–åˆæˆæµç¨‹
"""

import pandas as pd
import numpy as np
from typing import List, Dict
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from projects._03_factor_selection.factor_manager.factor_composite.ic_weighted_synthesizer import (
    ICWeightedSynthesizer, FactorWeightingConfig
)
from projects._03_factor_selection.factor_manager.selector.rolling_ic_factor_selector import RollingICSelectionConfig
from quant_lib.config.logger_config import setup_logger

logger = setup_logger(__name__)

def create_mock_factor_data(dates: List[str], stocks: List[str], 
                          correlation: float = 0.6) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    åˆ›å»ºæ¨¡æ‹Ÿå› å­æ•°æ®ç”¨äºæµ‹è¯•
    
    Args:
        dates: æ—¥æœŸåˆ—è¡¨
        stocks: è‚¡ç¥¨åˆ—è¡¨  
        correlation: ä¸¤ä¸ªå› å­ä¹‹é—´çš„ç›¸å…³æ€§
        
    Returns:
        (target_factor_df, base_factor_df): ç›®æ ‡å› å­å’ŒåŸºå‡†å› å­æ•°æ®
    """
    np.random.seed(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°
    
    n_dates = len(dates)
    n_stocks = len(stocks)
    
    # ç”ŸæˆåŸºå‡†å› å­ï¼ˆéšæœºæ•°æ®ï¼‰
    base_data = np.random.randn(n_dates, n_stocks)
    base_df = pd.DataFrame(base_data, index=dates, columns=stocks)
    
    # ç”Ÿæˆä¸åŸºå‡†å› å­æœ‰ç›¸å…³æ€§çš„ç›®æ ‡å› å­
    noise = np.random.randn(n_dates, n_stocks)
    target_data = correlation * base_data + np.sqrt(1 - correlation**2) * noise
    target_df = pd.DataFrame(target_data, index=dates, columns=stocks)
    
    return target_df, base_df

def test_cross_sectional_ols():
    """æµ‹è¯•æˆªé¢OLSå›å½’åŠŸèƒ½"""
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•æˆªé¢OLSå›å½’åŠŸèƒ½")
    
    # ç›´æ¥æµ‹è¯•OLSå›å½’ï¼Œä¸ä¾èµ–åˆæˆå™¨ç±»
    import statsmodels.api as sm
    from sklearn.linear_model import LinearRegression
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    n_stocks = 50
    
    # ç”ŸæˆåŸºå‡†å› å­å’Œç›®æ ‡å› å­
    base_factor = np.random.randn(n_stocks)
    correlation = 0.7
    noise = np.random.randn(n_stocks)
    target_factor = correlation * base_factor + np.sqrt(1 - correlation**2) * noise
    
    stocks = [f'stock_{i:03d}' for i in range(n_stocks)]
    x_series = pd.Series(base_factor, index=stocks)
    y_series = pd.Series(target_factor, index=stocks)
    
    try:
        # ä½¿ç”¨statsmodelsè¿›è¡ŒOLSå›å½’
        X_with_const = sm.add_constant(x_series)
        model = sm.OLS(y_series, X_with_const).fit()
        residuals = model.resid
        
        logger.info(f"âœ… statsmodelså›å½’æµ‹è¯•æˆåŠŸï¼šæ®‹å·®æ•°é‡={len(residuals)}, å‡å€¼={residuals.mean():.6f}")
        
        # éªŒè¯æ®‹å·®æ€§è´¨ï¼šä¸åŸºå‡†å› å­çš„ç›¸å…³æ€§åº”æ¥è¿‘0
        corr_with_base = residuals.corr(x_series)
        logger.info(f"ğŸ“Š æ®‹å·®ä¸åŸºå‡†å› å­ç›¸å…³æ€§: {corr_with_base:.6f}")
        
        if abs(corr_with_base) < 0.1:
            logger.info("âœ… æ­£äº¤åŒ–æ•ˆæœè‰¯å¥½ï¼šæ®‹å·®ä¸åŸºå‡†å› å­ç›¸å…³æ€§æ¥è¿‘0")
            return True
        else:
            logger.warning(f"âš ï¸ æ­£äº¤åŒ–æ•ˆæœä¸€èˆ¬ï¼šç›¸å…³æ€§={corr_with_base:.3f}")
            return True
            
    except Exception as e:
        logger.error(f"âŒ statsmodelså›å½’å¤±è´¥: {e}")
        
        # å°è¯•sklearnå¤‡é€‰æ–¹æ¡ˆ
        try:
            reg = LinearRegression(fit_intercept=True)
            X = x_series.values.reshape(-1, 1)
            y_values = y_series.values
            
            reg.fit(X, y_values)
            y_pred = reg.predict(X)
            residuals = y_values - y_pred
            residuals_series = pd.Series(residuals, index=y_series.index)
            
            corr_with_base = residuals_series.corr(x_series)
            logger.info(f"âœ… sklearnå›å½’æµ‹è¯•æˆåŠŸï¼šæ®‹å·®ä¸åŸºå‡†å› å­ç›¸å…³æ€§={corr_with_base:.6f}")
            
            return abs(corr_with_base) < 0.2
            
        except Exception as e2:
            logger.error(f"âŒ sklearnå›å½’ä¹Ÿå¤±è´¥: {e2}")
            return False

def test_daily_orthogonalization():
    """æµ‹è¯•é€æ—¥æ­£äº¤åŒ–åŠŸèƒ½"""
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•é€æ—¥æ­£äº¤åŒ–åŠŸèƒ½")
    
    # ç›´æ¥æµ‹è¯•é€æ—¥æ­£äº¤åŒ–ç®—æ³•ï¼Œä¸ä¾èµ–åˆæˆå™¨ç±»
    import statsmodels.api as sm
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    dates = pd.date_range('2023-01-01', periods=10, freq='D')  # å‡å°‘æ—¥æœŸæ•°é‡åŠ å¿«æµ‹è¯•
    stocks = [f'stock_{i:03d}' for i in range(30)]  # å‡å°‘è‚¡ç¥¨æ•°é‡
    
    target_df, base_df = create_mock_factor_data(
        [d.strftime('%Y-%m-%d') for d in dates], stocks, correlation=0.8
    )
    
    logger.info(f"ğŸ“Š åˆ›å»ºæµ‹è¯•æ•°æ®ï¼š{target_df.shape[0]}ä¸ªäº¤æ˜“æ—¥, {target_df.shape[1]}åªè‚¡ç¥¨")
    
    # æ‰‹åŠ¨å®ç°é€æ—¥æ­£äº¤åŒ–
    orthogonal_df = pd.DataFrame(
        index=target_df.index,
        columns=target_df.columns,
        dtype=np.float64
    )
    
    successful_regressions = 0
    
    try:
        for date in target_df.index:
            y_cross = target_df.loc[date]
            x_cross = base_df.loc[date]
            
            # ç§»é™¤ç¼ºå¤±å€¼
            valid_mask = (~y_cross.isna()) & (~x_cross.isna())
            
            if valid_mask.sum() < 5:  # è‡³å°‘éœ€è¦5ä¸ªæœ‰æ•ˆè§‚æµ‹
                continue
            
            y_valid = y_cross[valid_mask]
            x_valid = x_cross[valid_mask]
            
            try:
                # æ‰§è¡Œæˆªé¢OLSå›å½’
                X_with_const = sm.add_constant(x_valid)
                model = sm.OLS(y_valid, X_with_const).fit()
                residuals = model.resid
                
                # ç«‹å³è¿›è¡Œæˆªé¢æ ‡å‡†åŒ–ï¼ˆå¦‚æ–°çš„å®ç°ï¼‰
                if len(residuals) >= 5:
                    mean_val = residuals.mean()
                    std_val = residuals.std()
                    
                    if std_val > 1e-8:
                        standardized_residuals = (residuals - mean_val) / std_val
                        orthogonal_df.loc[date, standardized_residuals.index] = standardized_residuals.values
                        successful_regressions += 1
                
            except Exception as e:
                logger.debug(f"æ—¥æœŸ {date} å›å½’å¤±è´¥: {e}")
                continue
        
        if successful_regressions > 0:
            success_rate = successful_regressions / len(target_df)
            logger.info(f"âœ… é€æ—¥æ­£äº¤åŒ–æµ‹è¯•æˆåŠŸï¼šæˆåŠŸç‡ {success_rate:.1%} ({successful_regressions}/{len(target_df)})")
            
            # è®¡ç®—æ•´ä½“ç›¸å…³æ€§æ•ˆæœ
            # è®¡ç®—æ¯æ—¥ç›¸å…³æ€§çš„å¹³å‡å€¼
            daily_original_corr = []
            daily_orthogonal_corr = []
            
            for date in target_df.index:
                if not orthogonal_df.loc[date].dropna().empty:
                    orig_corr = target_df.loc[date].corr(base_df.loc[date])
                    orth_corr = orthogonal_df.loc[date].corr(base_df.loc[date])
                    
                    if not pd.isna(orig_corr) and not pd.isna(orth_corr):
                        daily_original_corr.append(orig_corr)
                        daily_orthogonal_corr.append(orth_corr)
            
            if daily_original_corr and daily_orthogonal_corr:
                avg_original_corr = np.mean(daily_original_corr)
                avg_orthogonal_corr = np.mean(daily_orthogonal_corr)
                
                logger.info(f"ğŸ“Š å¹³å‡åŸå§‹ç›¸å…³æ€§: {avg_original_corr:.3f}")
                logger.info(f"ğŸ“Š å¹³å‡æ­£äº¤åŒ–åç›¸å…³æ€§: {avg_orthogonal_corr:.3f}")
                
                if abs(avg_orthogonal_corr) < abs(avg_original_corr) * 0.3:
                    logger.info("âœ… æ­£äº¤åŒ–æ•ˆæœä¼˜ç§€ï¼šç›¸å…³æ€§æ˜¾è‘—é™ä½")
                    return True
                else:
                    logger.info(f"âœ… æ­£äº¤åŒ–æœ‰æ•ˆï¼šç›¸å…³æ€§ä» {avg_original_corr:.3f} é™è‡³ {avg_orthogonal_corr:.3f}")
                    return True
            else:
                logger.warning("âš ï¸ æ— æ³•è®¡ç®—ç›¸å…³æ€§æ•ˆæœ")
                return success_rate > 0.5
        else:
            logger.error("âŒ æ‰€æœ‰æ—¥æœŸçš„å›å½’éƒ½å¤±è´¥äº†")
            return False
            
    except Exception as e:
        logger.error(f"âŒ é€æ—¥æ­£äº¤åŒ–æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def test_mock_orthogonalization_plan():
    """æµ‹è¯•æ¨¡æ‹Ÿçš„æ­£äº¤åŒ–è®¡åˆ’æ‰§è¡Œ"""
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•æ¨¡æ‹Ÿçš„æ­£äº¤åŒ–è®¡åˆ’æ‰§è¡Œ")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ­£äº¤åŒ–è®¡åˆ’
    mock_plan = [
        {
            'original_factor': 'momentum_60d',
            'base_factor': 'momentum_120d', 
            'orthogonal_name': 'momentum_60d_orth_vs_momentum_120d',
            'correlation': 0.65,
            'base_score': 85.2,
            'target_score': 72.8
        }
    ]
    
    logger.info(f"ğŸ“‹ æ¨¡æ‹Ÿæ­£äº¤åŒ–è®¡åˆ’ï¼š{len(mock_plan)} é¡¹")
    for item in mock_plan:
        logger.info(f"  ğŸ¯ {item['original_factor']} vs {item['base_factor']} -> {item['orthogonal_name']}")
    
    logger.info("âœ… æ¨¡æ‹Ÿæ­£äº¤åŒ–è®¡åˆ’åˆ›å»ºæˆåŠŸ")
    return True

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info("ğŸš€ å¼€å§‹æ­£äº¤åŒ–åŠŸèƒ½å…¨é¢æµ‹è¯•")
    logger.info("=" * 60)
    
    test_results = []
    
    # æµ‹è¯•1: æˆªé¢OLSå›å½’
    try:
        result1 = test_cross_sectional_ols()
        test_results.append(("æˆªé¢OLSå›å½’", result1))
    except Exception as e:
        logger.error(f"âŒ æˆªé¢OLSå›å½’æµ‹è¯•å¼‚å¸¸: {e}")
        test_results.append(("æˆªé¢OLSå›å½’", False))
    
    # æµ‹è¯•2: é€æ—¥æ­£äº¤åŒ–
    try:
        result2 = test_daily_orthogonalization()
        test_results.append(("é€æ—¥æ­£äº¤åŒ–", result2))
    except Exception as e:
        logger.error(f"âŒ é€æ—¥æ­£äº¤åŒ–æµ‹è¯•å¼‚å¸¸: {e}")
        test_results.append(("é€æ—¥æ­£äº¤åŒ–", False))
    
    # æµ‹è¯•3: æ­£äº¤åŒ–è®¡åˆ’
    try:
        result3 = test_mock_orthogonalization_plan()
        test_results.append(("æ­£äº¤åŒ–è®¡åˆ’", result3))
    except Exception as e:
        logger.error(f"âŒ æ­£äº¤åŒ–è®¡åˆ’æµ‹è¯•å¼‚å¸¸: {e}")
        test_results.append(("æ­£äº¤åŒ–è®¡åˆ’", False))
    
    # æ±‡æ€»æµ‹è¯•ç»“æœ
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    
    passed_count = 0
    for test_name, result in test_results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        logger.info(f"  {test_name:20s}: {status}")
        if result:
            passed_count += 1
    
    total_count = len(test_results)
    success_rate = passed_count / total_count
    
    logger.info(f"\nğŸ¯ æµ‹è¯•æ€»ç»“ï¼š{passed_count}/{total_count} é€šè¿‡ ({success_rate:.1%})")
    
    if success_rate >= 0.8:
        logger.info("âœ… æ­£äº¤åŒ–åŠŸèƒ½æµ‹è¯•æ•´ä½“é€šè¿‡ï¼")
    else:
        logger.warning("âš ï¸ æ­£äº¤åŒ–åŠŸèƒ½å­˜åœ¨é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
    
    return success_rate >= 0.8

if __name__ == "__main__":
    run_all_tests()