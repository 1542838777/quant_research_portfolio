"""
æ•°æ®ç®¡ç†å™¨ - å•å› å­æµ‹è¯•ç»ˆæä½œæˆ˜æ‰‹å†Œ
ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®åŠ è½½ä¸è‚¡ç¥¨æ± æ„å»º

å®ç°é…ç½®é©±åŠ¨çš„æ•°æ®åŠ è½½å’ŒåŠ¨æ€è‚¡ç¥¨æ± æ„å»ºåŠŸèƒ½
"""

import pandas as pd
import numpy as np
import yaml
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import warnings
import sys
import os

from pandas import DatetimeIndex

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from quant_lib.data_loader import DataLoader, logger
from quant_lib.config.constant_config import LOCAL_PARQUET_DATA_DIR

warnings.filterwarnings('ignore')


class DataManager:
    """
    æ•°æ®ç®¡ç†å™¨ - è´Ÿè´£æ•°æ®åŠ è½½å’Œè‚¡ç¥¨æ± æ„å»º
    
    æŒ‰ç…§é…ç½®æ–‡ä»¶çš„è¦æ±‚ï¼Œå®ç°ï¼š
    1. åŸå§‹æ•°æ®åŠ è½½
    2. åŠ¨æ€è‚¡ç¥¨æ± æ„å»º
    3. æ•°æ®è´¨é‡æ£€æŸ¥
    4. æ•°æ®å¯¹é½å’Œé¢„å¤„ç†
    """

    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        self.data_loader = DataLoader(data_path=LOCAL_PARQUET_DATA_DIR)
        self.raw_data = {}
        self.universe_df = None

    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config

    def load_all_data(self) -> Dict[str, pd.DataFrame]:
        """
        æ ¹æ®é…ç½®æ–‡ä»¶åŠ è½½æ‰€æœ‰éœ€è¦çš„æ•°æ®
        
        Returns:
            æ•°æ®å­—å…¸
        """
        print("=" * 80)
        print("ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®åŠ è½½ä¸è‚¡ç¥¨æ± æ„å»º")
        print("=" * 80)

        # 1. ç¡®å®šéœ€è¦åŠ è½½çš„å­—æ®µ
        required_fields = self._get_required_fields()
        print(f"\n1. ç¡®å®šéœ€è¦åŠ è½½çš„å­—æ®µ: {required_fields}")

        # 2. åŠ è½½åŸå§‹æ•°æ®
        print("\n2. åŠ è½½åŸå§‹æ•°æ®...")
        start_date = self.config['backtest']['start_date']
        end_date = self.config['backtest']['end_date']

        self.raw_data = self.data_loader.load_data(
            fields=required_fields,
            start_date=start_date,
            end_date=end_date
        )

        print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…±åŠ è½½ {len(self.raw_data)} ä¸ªå­—æ®µ")

        # 3. æ•°æ®è´¨é‡æ£€æŸ¥
        print("\n3. æ•°æ®è´¨é‡æ£€æŸ¥...")
        self._check_data_quality()

        # 4. æ„å»ºåŠ¨æ€è‚¡ç¥¨æ± 
        print("\n4. æ„å»ºåŠ¨æ€è‚¡ç¥¨æ± ...")
        # è·å–æ‰€æœ‰è‚¡ç¥¨
        ts_codes = list(set(self.get_price_data().columns))
        namechange = self.get_namechange_data()
        trading_dates = self.data_loader.get_trading_dates(start_date=start_date, end_date=end_date)

        self.build_st_period_from_namechange(ts_codes, namechange, trading_dates)
        self.universe_df = self._build_universe()

        # 5. åº”ç”¨è‚¡ç¥¨æ± è¿‡æ»¤
        print("\n5. åº”ç”¨è‚¡ç¥¨æ± è¿‡æ»¤...")
        self._apply_universe_filter()

        return self.raw_data

    def _get_required_fields(self) -> List[str]:
        """è·å–æ‰€æœ‰éœ€è¦çš„å­—æ®µ"""
        required_fields = set()

        # åŸºç¡€å­—æ®µ
        required_fields.update(['close',
                                'pb',#ä¸ºäº†è®¡ç®—ä»·å€¼ç±»å› å­
                                'total_mv', 'turnover_rate',  # ä¸ºäº†è¿‡æ»¤ å¾ˆå·®åŠ²çš„è‚¡ç¥¨ ä»…æ­¤è€Œå·²ï¼Œä¸ä¼šä½œå…¶ä»–è®¡ç®— ã€'total_mv'è¿˜å¯ ç”¨äºè®¡ç®—ä¸­æ€§åŒ–
                                'industry',  # ç”¨äºè®¡ç®—ä¸­æ€§åŒ–
                                'circ_mv',  # æµé€šå¸‚å€¼ ç”¨äºWOSï¼ŒåŠ æƒæœ€å°äºŒæ–¹è·Ÿ  ï¼Œå›å½’æ³•ä¼šç”¨åˆ°
                                'list_date'  # ä¸Šå¸‚æ—¥æœŸ
                                ])

        # ç›®æ ‡å› å­å­—æ®µ
        target_factor = self.config['target_factor']
        required_fields.update(target_factor['fields'])

        # ä¸­æ€§åŒ–éœ€è¦çš„å­—æ®µ
        neutralization = self.config['preprocessing']['neutralization']
        if neutralization['enable']:
            if 'industry' in neutralization['factors']:
                required_fields.add('industry')
            if 'market_cap' in neutralization['factors']:
                required_fields.add('total_mv')

        # # è‚¡ç¥¨æ± è¿‡æ»¤éœ€è¦çš„å­—æ®µ
        # universe_filters = self.config['universe']['filters']
        # if universe_filters.get('remove_st', False):
        #     print()
        #     # required_fields.add('name')  # ç”¨äºè¯†åˆ«STè‚¡ç¥¨ æ”¹æˆ ä½¿ç”¨æ—¶ å½“åœºæ·»åŠ ï¼Œç°åœ¨è¿‡æ—©åŠ å…¥ï¼Œå‰æœŸè·Ÿç€åˆ«çš„å­—æ®µä¸€èµ·ç»å†é‚£ä¹ˆå¤š æ²¡å¿…è¦

        return list(required_fields)

    def _check_data_quality(self):
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        print("  æ£€æŸ¥æ•°æ®å®Œæ•´æ€§å’Œè´¨é‡...")

        for field_name, df in self.raw_data.items():
            # æ£€æŸ¥æ•°æ®å½¢çŠ¶
            print(f"  {field_name}: {df.shape}")

            # æ£€æŸ¥ç¼ºå¤±å€¼æ¯”ä¾‹
            missing_ratio = df.isnull().sum().sum() / (df.shape[0] * df.shape[1])
            print(f"    ç¼ºå¤±å€¼æ¯”ä¾‹: {missing_ratio:.2%}")

            # æ£€æŸ¥å¼‚å¸¸å€¼
            if field_name in ['close', 'total_mv','pb', 'pe_ttm']:
                negative_ratio = (df <= 0).sum().sum() / df.notna().sum().sum()
                print(f"  æå€¼(>99%åˆ†ä½) å æ¯”: {((df > df.quantile(0.99)).sum().sum())/(df.shape[0] * df.shape[1])}")

                if negative_ratio > 0:
                    print(f"    è­¦å‘Š: {field_name} å­˜åœ¨ {negative_ratio:.2%} çš„éæ­£å€¼")

    def _build_universe(self) -> pd.DataFrame:
        """
        æ„å»ºåŠ¨æ€è‚¡ç¥¨æ± 
        Returns:
            è‚¡ç¥¨æ± DataFrameï¼ŒTrueè¡¨ç¤ºè¯¥è‚¡ç¥¨åœ¨è¯¥æ—¥æœŸå¯ç”¨
        """
        print("  æ„å»ºåŸºç¡€è‚¡ç¥¨æ± ...")

        # ç¬¬ä¸€æ­¥ï¼šåŸºç¡€è‚¡ç¥¨æ±  - æœ‰ä»·æ ¼æ•°æ®çš„è‚¡ç¥¨
        if 'close' not in self.raw_data:
            raise ValueError("ç¼ºå°‘ä»·æ ¼æ•°æ®ï¼Œæ— æ³•æ„å»ºè‚¡ç¥¨æ± ")

        close_df = self.raw_data['close']
        universe_df = close_df.notna()
        print(f"    åŸºç¡€ï¼ˆæœªè¿‡æ»¤ï¼‰è‚¡ç¥¨æ± ï¼š{universe_df.sum(axis=1).mean():.0f} åªè‚¡ç¥¨/æ—¥")
        # ç¬¬äºŒæ­¥ï¼šæŒ‡æ•°æˆåˆ†è‚¡è¿‡æ»¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        index_config = self.config['universe'].get('index_filter', {})
        if index_config.get('enable', False):
            print(f"    åº”ç”¨æŒ‡æ•°è¿‡æ»¤: {index_config['index_code']}")
            universe_df = self._build_dynamic_index_universe(universe_df, index_config['index_code'])

        # åº”ç”¨å„ç§è¿‡æ»¤æ¡ä»¶
        universe_filters = self.config['universe']['filters']

        # 1. å‰”é™¤STè‚¡ç¥¨
        if universe_filters.get('remove_st', False):
            print("    åº”ç”¨STè‚¡ç¥¨è¿‡æ»¤...")
            universe_df = self._filter_st_stocks(universe_df)

        # 2. æµåŠ¨æ€§è¿‡æ»¤
        if 'min_liquidity_percentile' in universe_filters:
            print("    åº”ç”¨æµåŠ¨æ€§è¿‡æ»¤...")
            universe_df = self._filter_by_liquidity(
                universe_df,
                universe_filters['min_liquidity_percentile']
            )

        # 3. å¸‚å€¼è¿‡æ»¤
        if 'min_market_cap_percentile' in universe_filters:
            print("    åº”ç”¨å¸‚å€¼è¿‡æ»¤...")
            universe_df = self._filter_by_market_cap(
                universe_df,
                universe_filters['min_market_cap_percentile']
            )

        # 4. å‰”é™¤æ¬¡æ—¥åœç‰Œè‚¡ç¥¨
        if universe_filters.get('remove_next_day_suspended', False):
            print("    åº”ç”¨æ¬¡æ—¥åœç‰Œè‚¡ç¥¨è¿‡æ»¤...")
            universe_df = self._filter_next_day_suspended(universe_df)

        # ç»Ÿè®¡è‚¡ç¥¨æ± ä¿¡æ¯
        daily_count = universe_df.sum(axis=1)
        print(f"    è¿‡æ»¤åï¼ˆå¸‚å€¼ã€æ¢æ‰‹ç‡...)è‚¡ç¥¨æ± ç»Ÿè®¡:")
        print(f"      å¹³å‡æ¯æ—¥è‚¡ç¥¨æ•°: {daily_count.mean():.0f}")
        print(f"      æœ€å°‘æ¯æ—¥è‚¡ç¥¨æ•°: {daily_count.min():.0f}")
        print(f"      æœ€å¤šæ¯æ—¥è‚¡ç¥¨æ•°: {daily_count.max():.0f}")

        return universe_df

    def build_st_period_from_namechange(
            self,
            ts_codes: list,
            namechange_df: pd.DataFrame,
            trading_dates: pd.DatetimeIndex
    ) -> pd.DataFrame:
        """
        ã€ä¸“ä¸šç‰ˆã€‘æ ¹æ®namechangeå†å²æ•°æ®ï¼Œé‡å»ºæ¯æ—¥STçŠ¶æ€çš„å¸ƒå°”çŸ©é˜µã€‚
        æ­¤ç‰ˆæœ¬å·²å¢å¼ºï¼Œå¯åŒæ—¶è¯†åˆ«ST, *ST, S, SSTç­‰å¤šç§é£é™©è­¦ç¤ºçŠ¶æ€ã€‚

        Args:
            start_date: å›æµ‹å¼€å§‹æ—¥æœŸ
            end_date: å›æµ‹ç»“æŸæ—¥æœŸ
            ts_codes: æ‰€æœ‰è‚¡ç¥¨ä»£ç çš„åˆ—è¡¨
            namechange_df: ä»Tushareä¸‹è½½çš„namechangeå†å²æ•°æ®
            trade_cal: äº¤æ˜“æ—¥å†DataFrame

        Returns:
            pd.DataFrame: ä¸€ä¸ªå¸ƒå°”çŸ©é˜µï¼ŒTrueä»£è¡¨å½“å¤©è¯¥è‚¡ç¥¨å¤„äºé£é™©è­¦ç¤ºçŠ¶æ€ã€‚
        """
        print("æ­£åœ¨æ ¹æ®åç§°å˜æ›´å†å²ï¼Œé‡å»ºæ¯æ—¥é£é™©è­¦ç¤ºçŠ¶æ€çŸ©é˜µ...")
        # è·å–çœŸå®çš„äº¤æ˜“æ—¥

        # åˆ›å»ºä¸€ä¸ªç©ºçš„â€œç”»å¸ƒâ€
        st_matrix = pd.DataFrame(False, index=trading_dates, columns=ts_codes)

        # æ ¼å¼åŒ–æ—¥æœŸå¹¶æ’åºï¼Œç¡®ä¿äº‹ä»¶æŒ‰æ—¶é—´å‘ç”Ÿ
        namechange_df['start_date'] = pd.to_datetime(namechange_df['start_date'])
        namechange_df.sort_values(by='start_date', inplace=True)

        # æŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„ï¼Œå¤„ç†æ¯åªè‚¡ç¥¨çš„STå†å²
        for stock, group in namechange_df.groupby('ts_code'):
            if stock not in st_matrix.columns:
                continue
            group = group.sort_values(by='start_date')  # ä¿è¯ç»„å†…æœ‰åº

            for _, row in group.iterrows():
                # --- ã€æ ¸å¿ƒé€»è¾‘ä¿®æ­£ã€‘ ---
                # ä¸€ä¸ªæ›´å¥å£®çš„æ£€æŸ¥ï¼ŒåŒæ—¶å¤„ç†ST, *ST, S, SSTç­‰æƒ…å†µ
                name_upper = row['name'].upper()
                is_risk_stock = 'ST' in name_upper or name_upper.startswith('S')

                start = row['start_date']
                # ç»“æŸæ—¥æœŸæ˜¯ä¸‹ä¸€æ¬¡åç§°å˜æ›´çš„å¼€å§‹æ—¥æœŸï¼Œæˆ–è€…æ˜¯æ— ç©·è¿œ
                end = row['end_date'] if pd.notna(row['end_date']) else pd.to_datetime('2200-01-01')

                # æ ¹æ®is_risk_stockçš„å€¼ï¼Œæ¥æ ‡è®°æ•´ä¸ªåŒºé—´çš„çŠ¶æ€ ä¸ºtrueè¡¨ç¤ºstè‚¡ç¥¨
                st_matrix.loc[start:end, stock] = is_risk_stock

        # ã€é‡è¦ã€‘å‘å‰å¡«å……ï¼Œå› ä¸ºnamechangeåªè®°å½•å˜æ›´æ—¶ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦å¡«å……æœŸé—´çš„çŠ¶æ€
        st_matrix.ffill(inplace=True)#ok æ²¡é—®é¢˜

        print("æ¯æ—¥é£é™©è­¦ç¤ºçŠ¶æ€çŸ©é˜µé‡å»ºå®Œæ¯•ã€‚")
        self.st_matrix = st_matrix

    def _filter_st_stocks(self, universe_df: pd.DataFrame) -> pd.DataFrame:
        print("    åº”ç”¨STè‚¡ç¥¨è¿‡æ»¤...")
        if self.st_matrix is None:
            print("    è­¦å‘Š: æœªèƒ½æ„å»ºSTçŠ¶æ€çŸ©é˜µï¼Œæ— æ³•è¿‡æ»¤STè‚¡ç¥¨ã€‚")
            return universe_df

        # å¯¹é½ä¸¤ä¸ªDataFrameçš„ç´¢å¼•å’Œåˆ—ï¼Œç¡®ä¿ä¸‡æ— ä¸€å¤±
        # join='left' è¡¨ç¤ºä»¥universe_dfçš„å½¢çŠ¶ä¸ºå‡†
        aligned_universe, aligned_st_status = universe_df.align(self.st_matrix, join='left', fill_value=False)#è‡³å°‘åš è¡Œåˆ— ä¿æŒä¸€è‡´çš„å¯¹é½ã€‚ ä¸‹é¢æ‰åšèµ‹å€¼ï¼ #fill_value=False ï¼šst_Dfåªèƒ½å¯¹åº”ä¸€éƒ¨åˆ†çš„è‚¡ç¥¨æ± _Df.è‚¡ç¥¨æ± _Dfå‰©ä½™çš„è¡Œåˆ— ç”¨falseå¡«å……ï¼

        # å°†STçš„è‚¡ç¥¨ä»universeä¸­å‰”é™¤
        # aligned_st_statusä¸ºTrueçš„åœ°æ–¹ï¼Œåœ¨universeä¸­å°±åº”è¯¥ä¸ºFalse
        aligned_universe[aligned_st_status] = False

        # ç»Ÿè®¡è¿‡æ»¤æ•ˆæœ
        original_count = universe_df.sum(axis=1).mean()
        filtered_count = aligned_universe.sum(axis=1).mean()
        st_filtered_count = original_count - filtered_count
        print(f"      STè‚¡ç¥¨è¿‡æ»¤: å¹³å‡æ¯æ—¥å‰”é™¤ {st_filtered_count:.0f} åªSTè‚¡ç¥¨")

        return aligned_universe

    def _filter_by_liquidity(self, universe_df: pd.DataFrame, min_percentile: float) -> pd.DataFrame:
        """æŒ‰æµåŠ¨æ€§è¿‡æ»¤ """
        if 'turnover_rate' not in self.raw_data:
            raise RuntimeError("ç¼ºå°‘æ¢æ‰‹ç‡æ•°æ®ï¼Œæ— æ³•è¿›è¡ŒæµåŠ¨æ€§è¿‡æ»¤")

        turnover_df = self.raw_data['turnover_rate']

        # 1. ã€ç¡®å®šæ ·æœ¬ã€‘åªä¿ç•™ universe_df ä¸­ä¸º True çš„æ¢æ‰‹ç‡æ•°æ®
        # â€œåªå¯¹å½“å‰è‚¡ç¥¨æ± è®¡ç®—â€
        valid_turnover = turnover_df.where(universe_df)

        # 2. ã€è®¡ç®—æ ‡å‡†ã€‘æ²¿è¡Œï¼ˆaxis=1ï¼‰ä¸€æ¬¡æ€§è®¡ç®—å‡ºæ¯æ—¥çš„åˆ†ä½æ•°é˜ˆå€¼
        thresholds = valid_turnover.quantile(min_percentile, axis=1)

        # 3. ã€åº”ç”¨æ ‡å‡†ã€‘å°†åŸå§‹æ¢æ‰‹ç‡ä¸æ¯æ—¥é˜ˆå€¼è¿›è¡Œæ¯”è¾ƒï¼Œç”Ÿæˆè¿‡æ»¤æ©ç 
        low_liquidity_mask = turnover_df.lt(thresholds, axis=0)

        # 4. å°†éœ€è¦å‰”é™¤çš„è‚¡ç¥¨åœ¨ universe_df ä¸­è®¾ä¸º False
        universe_df[low_liquidity_mask] = False

        return universe_df

    def _filter_by_market_cap(self,
                                         universe_df: pd.DataFrame,
                                         min_percentile: float) -> pd.DataFrame:
        """
        æŒ‰å¸‚å€¼è¿‡æ»¤ -

        Args:
            universe_df: åŠ¨æ€è‚¡ç¥¨æ± 
            min_percentile: å¸‚å€¼æœ€ä½ç™¾åˆ†ä½é˜ˆå€¼

        Returns:
            è¿‡æ»¤åçš„åŠ¨æ€è‚¡ç¥¨æ± 
        """
        if 'total_mv' not in self.raw_data:
            raise RuntimeError("ç¼ºå°‘å¸‚å€¼æ•°æ®ï¼Œæ— æ³•è¿›è¡Œå¸‚å€¼è¿‡æ»¤")

        mv_df = self.raw_data['total_mv']

        # 1. ã€å±è”½ã€‘åªä¿ç•™åœ¨å½“å‰è‚¡ç¥¨æ± (universe_df)ä¸­çš„è‚¡ç¥¨å¸‚å€¼ï¼Œå…¶ä½™è®¾ä¸ºNaN
        valid_mv = mv_df.where(universe_df)

        # 2. ã€è®¡ç®—æ ‡å‡†ã€‘å‘é‡åŒ–è®¡ç®—æ¯æ—¥çš„å¸‚å€¼åˆ†ä½æ•°é˜ˆå€¼
        # axis=1 ç¡®ä¿äº†æˆ‘ä»¬æ˜¯æŒ‰è¡Œï¼ˆæ¯æ—¥ï¼‰è®¡ç®—åˆ†ä½æ•°
        thresholds = valid_mv.quantile(min_percentile, axis=1)

        # 3. ã€ç”Ÿæˆæ©ç ã€‘å°†åŸå§‹å¸‚å€¼ä¸æ¯æ—¥é˜ˆå€¼è¿›è¡Œæ¯”è¾ƒ
        # .lt() æ˜¯â€œå°äºâ€æ“ä½œï¼Œaxis=0 ç¡®ä¿äº† thresholds è¿™ä¸ªSeriesèƒ½æŒ‰è¡Œæ­£ç¡®åœ°å¹¿æ’­
        small_cap_mask = mv_df.lt(thresholds, axis=0)

        # 4. ã€åº”ç”¨è¿‡æ»¤ã€‘å°†æ‰€æœ‰å¸‚å€¼å°äºå½“æ—¥é˜ˆå€¼çš„è‚¡ç¥¨ï¼Œåœ¨è‚¡ç¥¨æ± ä¸­æ ‡è®°ä¸ºFalse
        # è¿™æ˜¯ä¸€ä¸ªè·¨è¶Šæ•´ä¸ªDataFrameçš„å¸ƒå°”è¿ç®—ï¼Œæå…¶é«˜æ•ˆ
        universe_df[small_cap_mask] = False

        return universe_df

    def _filter_next_day_suspended(self, universe_df: pd.DataFrame) -> pd.DataFrame:
        """
          å‰”é™¤æ¬¡æ—¥åœç‰Œè‚¡ç¥¨ -

          Args:
              universe_df: åŠ¨æ€è‚¡ç¥¨æ± DataFrame

          Returns:
              è¿‡æ»¤åçš„åŠ¨æ€è‚¡ç¥¨æ± DataFrame
          """
        if 'close' not in self.raw_data:
            raise RuntimeError(" ç¼ºå°‘ä»·æ ¼æ•°æ®ï¼Œæ— æ³•è¿‡æ»¤æ¬¡æ—¥åœç‰Œè‚¡ç¥¨")

        close_df = self.raw_data['close']

        # 1. åˆ›å»ºä¸€ä¸ªä»£è¡¨â€œå½“æ—¥æœ‰ä»·æ ¼â€çš„å¸ƒå°”çŸ©é˜µ
        today_has_price = close_df.notna()

        # 2. åˆ›å»ºä¸€ä¸ªä»£è¡¨â€œæ¬¡æ—¥æœ‰ä»·æ ¼â€çš„å¸ƒå°”çŸ©é˜µ
        #    shift(-1) å°† T+1 æ—¥çš„æ•°æ®ï¼Œç§»åŠ¨åˆ° T æ—¥çš„è¡Œã€‚è¿™å°±åœ¨ä¸€ç¬é—´å®Œæˆäº†æ‰€æœ‰â€œnext_dateâ€çš„æŸ¥æ‰¾
        #    fill_value=True ä¼˜é›…åœ°å¤„ç†äº†æœ€åä¸€å¤©ï¼Œæˆ‘ä»¬å‡è®¾æœ€åä¸€å¤©ä¹‹åä¸ä¼šåœç‰Œ
        tomorrow_has_price = close_df.notna().shift(-1, fill_value=True)

        # 3. è®¡ç®—å‡ºæ‰€æœ‰â€œæ¬¡æ—¥åœç‰Œâ€çš„æ©ç  (Mask)
        #    æ¬¡æ—¥åœç‰Œ = ä»Šæ—¥æœ‰ä»· & æ˜æ—¥æ— ä»·
        next_day_suspended_mask = today_has_price & (~tomorrow_has_price)

        # 4. ä¸€æ¬¡æ€§ä»è‚¡ç¥¨æ± ä¸­å‰”é™¤æ‰€æœ‰è¢«æ ‡è®°çš„è‚¡ç¥¨
        #    è¿™ä¸ªå¸ƒå°”è¿ç®—ä¼šè‡ªåŠ¨æŒ‰ç´¢å¼•å¯¹é½ï¼Œåº”ç”¨åˆ°æ•´ä¸ªDataFrame
        universe_df[next_day_suspended_mask] = False

        return universe_df

    def _filter_by_index_components(self, universe_df: pd.DataFrame,
                                    index_code: str) -> pd.DataFrame:
        """æ ¹æ®æŒ‡æ•°æˆåˆ†è‚¡è¿‡æ»¤"""
        try:
            # åŠ è½½æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®
            index_components = self._load_index_components(index_code)

            # åªä¿ç•™æˆåˆ†è‚¡
            valid_stocks = universe_df.columns.intersection(index_components)
            filtered_universe = universe_df[valid_stocks].copy()

            print(f"    æŒ‡æ•° {index_code} æˆåˆ†è‚¡è¿‡æ»¤å®Œæˆï¼Œä¿ç•™ {len(valid_stocks)} åªè‚¡ç¥¨")
            return filtered_universe

        except Exception as e:
            print(f"    æŒ‡æ•°æˆåˆ†è‚¡è¿‡æ»¤å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹è‚¡ç¥¨æ± ")
            return universe_df

    ## return ts_code list
    def _load_index_components(self, index_code: str) -> list:
        """åŠ è½½æŒ‡æ•°æˆåˆ†è‚¡åˆ—è¡¨"""
        # æ–¹æ¡ˆ1ï¼šä»æœ¬åœ°æ–‡ä»¶åŠ è½½
        components_file = LOCAL_PARQUET_DATA_DIR / 'index_weights' / f"{index_code.replace('.', '_')}"
        if components_file.exists():
            df = pd.read_parquet(components_file)
            return df['con_code'].unique().tolist()

        raise ValueError(f"æœªæ‰¾åˆ°æŒ‡æ•° {index_code} çš„æˆåˆ†è‚¡æ•°æ®")

    def _load_dynamic_index_components(self, index_code: str,
                                       start_date: str, end_date: str) -> pd.DataFrame:
        """åŠ è½½åŠ¨æ€æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®"""
        print(f"    åŠ è½½ {index_code} åŠ¨æ€æˆåˆ†è‚¡æ•°æ®...")

        index_file_name = index_code.replace('.', '_')
        index_data_path = LOCAL_PARQUET_DATA_DIR / 'index_weights' / index_file_name

        if not index_data_path.exists():
            raise ValueError(f"æœªæ‰¾åˆ°æŒ‡æ•° {index_code} çš„æˆåˆ†è‚¡æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œdownloaderä¸‹è½½")

        # ç›´æ¥è¯»å–åˆ†åŒºæ•°æ®ï¼Œpandasä¼šè‡ªåŠ¨åˆå¹¶æ‰€æœ‰year=*åˆ†åŒº
        components_df = pd.read_parquet(index_data_path)
        components_df['trade_date'] = pd.to_datetime(components_df['trade_date'])

        # æ—¶é—´èŒƒå›´è¿‡æ»¤
        #å¤§å‘å•Š ï¼Œstart_dateå¿…é¡»æå‰6ä¸ªæœˆï¼ï¼ï¼ å› ä¸ºæœ€åœº6ä¸ªæœˆæ‰æœ‰æ–°çš„æ•°æ®ï¼ ï¼ˆæ–°è€æ•°æ®é—´éš”æœ€é•¿å¯è¾¾6ä¸ªæœˆï¼ï¼‰ã€‚åé¢é€æ—¥å¡«å……æˆåˆ†è‚¡ä¿¡æ¯ï¼šåŸç†å°±æ˜¯å–ä¸Šæ¬¡æ•°æ®è¿›è¡Œå¡«å……çš„ï¼
        extended_start_date = pd.Timestamp(start_date) - pd.DateOffset(months=6)
        mask = (components_df['trade_date'] >= extended_start_date) & \
               (components_df['trade_date'] <= pd.Timestamp(end_date))
        components_df = components_df[mask]

        print(f"    æˆåŠŸåŠ è½½ç¬¦åˆå½“å‰å›æµ‹æ—¶é—´æ®µï¼š {len(components_df)} æ¡æˆåˆ†è‚¡è®°å½•")
        return components_df

    def _build_dynamic_index_universe(self, universe_df, index_code: str) -> pd.DataFrame:
        """æ„å»ºåŠ¨æ€æŒ‡æ•°è‚¡ç¥¨æ± """
        start_date = self.config['backtest']['start_date']
        end_date = self.config['backtest']['end_date']

        # åŠ è½½åŠ¨æ€æˆåˆ†è‚¡æ•°æ®
        components_df = self._load_dynamic_index_components(index_code, start_date, end_date)

        # è·å–äº¤æ˜“æ—¥åºåˆ—
        trading_dates = self.data_loader.get_trading_dates(start_date, end_date)

        # ğŸ”§ ä¿®å¤ï¼šåˆ›å»ºæ–°çš„DataFrameï¼Œè€Œä¸æ˜¯ä¿®æ”¹åŸæœ‰çš„
        index_universe_df = universe_df.copy()

        # é€æ—¥å¡«å……æˆåˆ†è‚¡ä¿¡æ¯
        for date in trading_dates:
            if date not in index_universe_df.index:
                continue

            # è·å–å½“æ—¥æˆåˆ†è‚¡
            daily_components = components_df[
                components_df['trade_date'] == date
                ]['con_code'].tolist()

            if daily_components:
                # ğŸ”§ ä¿®å¤ï¼šåœ¨åŸºç¡€è‚¡ç¥¨æ± çš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥ç­›é€‰æŒ‡æ•°æˆåˆ†è‚¡
                valid_stocks = index_universe_df.columns.intersection(daily_components)

                # åªä¿ç•™æ—¢åœ¨åŸºç¡€è‚¡ç¥¨æ± ä¸­ï¼Œåˆæ˜¯æŒ‡æ•°æˆåˆ†è‚¡çš„è‚¡ç¥¨
                current_universe = index_universe_df.loc[date]  # å½“å‰åŸºç¡€è‚¡ç¥¨æ± 
                index_universe_df.loc[date, :] = False  # å…ˆæ¸…é›¶

                # åŒæ—¶æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ï¼š1)åœ¨åŸºç¡€è‚¡ç¥¨æ± ä¸­ 2)æ˜¯æŒ‡æ•°æˆåˆ†è‚¡
                final_valid_stocks = []
                for stock in valid_stocks:
                    if current_universe[stock]:  # åœ¨åŸºç¡€è‚¡ç¥¨æ± ä¸­
                        final_valid_stocks.append(stock)

                index_universe_df.loc[date, final_valid_stocks] = True #ä»¥ä¸Š å¼ºè¡Œä¿è¯äº† ä¸€å®šæ˜¯æœ‰closeï¼ˆå³current_universe[stock]ä¸ºtrueï¼‰ è¿˜ä¿è¯ä¸€å®šæ˜¯ç›®æ ‡æˆåˆ†è‚¡

            else:
                # å½“æ—¥æ— æˆåˆ†è‚¡æ•°æ®ï¼Œä½¿ç”¨æœ€è¿‘ä¸€æ¬¡çš„æˆåˆ†è‚¡
                recent_components = components_df[
                    components_df['trade_date'] <= date
                    ]
                if not recent_components.empty:
                    latest_date = recent_components['trade_date'].max()
                    latest_components = recent_components[
                        recent_components['trade_date'] == latest_date
                        ]['con_code'].tolist()

                    valid_stocks = index_universe_df.columns.intersection(latest_components)
                    current_universe = index_universe_df.loc[date]

                    index_universe_df.loc[date, :] = False
                    final_valid_stocks = [stock for stock in valid_stocks if current_universe[stock]]
                    index_universe_df.loc[date, final_valid_stocks] = True

        daily_count = index_universe_df.sum(axis=1)
        print(f"    åŠ¨æ€æŒ‡æ•°è‚¡ç¥¨æ± æ„å»ºå®Œæˆ:")
        print(f"      å¹³å‡æ¯æ—¥è‚¡ç¥¨æ•°: {daily_count.mean():.0f}")
        print(f"      æœ€å°‘æ¯æ—¥è‚¡ç¥¨æ•°: {daily_count.min():.0f}")
        print(f"      æœ€å¤šæ¯æ—¥è‚¡ç¥¨æ•°: {daily_count.max():.0f}")

        return index_universe_df

    def _apply_universe_filter(self):
        """å°†è‚¡ç¥¨æ± è¿‡æ»¤åº”ç”¨åˆ°æ‰€æœ‰æ•°æ®"""
        print("  å°†è‚¡ç¥¨æ± è¿‡æ»¤åº”ç”¨åˆ°æ‰€æœ‰æ•°æ®...")
        # self.universe_df.sum(axis=1).plot()

        for field_name, df in self.raw_data.items():
            # å°†ä¸åœ¨è‚¡ç¥¨æ± ä¸­çš„æ•°æ®è®¾ä¸ºNaN
            self.raw_data[field_name] = df.where(self.universe_df)

    def get_factor_data(self) -> pd.DataFrame:
        """
        è®¡ç®—ç›®æ ‡å› å­æ•°æ®
        
        Returns:
            å› å­æ•°æ®DataFrame
        """
        target_factor = self.config['target_factor']
        factor_name = target_factor['name']
        fields = target_factor['fields']

        print(f"\nè®¡ç®—ç›®æ ‡å› å­: {factor_name}")

        # ç®€å•çš„å› å­è®¡ç®—é€»è¾‘
        if factor_name == 'pe_inv' and 'pe_ttm' in fields:
            # PEå€’æ•°å› å­
            pe_data = self.raw_data['pe_ttm']
            factor_data = 1 / pe_data
            factor_data = factor_data.replace([np.inf, -np.inf], np.nan)
        else:
            # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªå­—æ®µ
            factor_data = self.raw_data[fields[0]]

        return factor_data

    def get_universe(self) -> pd.DataFrame:
        """è·å–è‚¡ç¥¨æ± """
        return self.universe_df

    def get_price_data(self) -> pd.DataFrame:
        """è·å–ä»·æ ¼æ•°æ®"""
        return self.raw_data['close']

    def get_namechange_data(self) -> pd.DataFrame:
        """è·å–nameæ”¹å˜çš„æ•°æ®"""
        namechange_path = LOCAL_PARQUET_DATA_DIR / 'namechange.parquet'

        return pd.read_parquet(namechange_path)

    def save_data_summary(self, output_dir: str):
        """ä¿å­˜æ•°æ®æ‘˜è¦"""
        os.makedirs(output_dir, exist_ok=True)

        # ä¿å­˜è‚¡ç¥¨æ± ç»Ÿè®¡
        universe_stats = {
            'daily_count': self.universe_df.sum(axis=1),
            'stock_coverage': self.universe_df.sum(axis=0)
        }

        summary_path = os.path.join(output_dir, 'data_summary.xlsx')
        with pd.ExcelWriter(summary_path) as writer:
            # æ¯æ—¥è‚¡ç¥¨æ•°ç»Ÿè®¡
            universe_stats['daily_count'].to_frame('stock_count').to_excel(
                writer, sheet_name='daily_stock_count'
            )

            # è‚¡ç¥¨è¦†ç›–ç»Ÿè®¡
            universe_stats['stock_coverage'].to_frame('coverage_days').to_excel(
                writer, sheet_name='stock_coverage'
            )

            # æ•°æ®è´¨é‡æŠ¥å‘Š
            quality_report = []
            for field_name, df in self.raw_data.items():
                quality_report.append({
                    'field': field_name,
                    'shape': f"{df.shape[0]}x{df.shape[1]}",
                    'missing_ratio': f"{df.isnull().sum().sum() / (df.shape[0] * df.shape[1]):.2%}",
                    'valid_ratio': f"{df.notna().sum().sum() / (df.shape[0] * df.shape[1]):.2%}"
                })

            pd.DataFrame(quality_report).to_excel(
                writer, sheet_name='data_quality', index=False
            )

        print(f"æ•°æ®æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_path}")


def create_data_manager(config_path: str) -> DataManager:
    """
    åˆ›å»ºæ•°æ®ç®¡ç†å™¨å®ä¾‹
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        DataManagerå®ä¾‹
    """
    return DataManager(config_path)


