"""
æ•°æ®ç®¡ç†å™¨ - å•å› å­æµ‹è¯•ç»ˆæä½œæˆ˜æ‰‹å†Œ
ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®åŠ è½½ä¸è‚¡ç¥¨æ± æ„å»º

å®ç°é…ç½®é©±åŠ¨çš„æ•°æ®åŠ è½½å’ŒåŠ¨æ€è‚¡ç¥¨æ± æ„å»ºåŠŸèƒ½
"""

import pandas as pd
import numpy as np
import yaml
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import warnings
import sys
import os

from pandas import DatetimeIndex

from data.local_data_load import load_index_daily, load_suspend_d_df
from data.namechange_date_manager import fill_end_date_field
from projects._03_factor_selection.config.base_config import INDEX_CODES
from projects._03_factor_selection.config.config_file.load_config_file import _load_local_config
from projects._03_factor_selection.config.factor_info_config import FACTOR_FILL_CONFIG, FILL_STRATEGY_FFILL_UNLIMITED, \
    FILL_STRATEGY_CONDITIONAL_ZERO, FILL_STRATEGY_FFILL_LIMIT_5, FILL_STRATEGY_NONE, FILL_STRATEGY_FFILL_LIMIT_65

from projects._03_factor_selection.factor_manager.factor_technical_cal.factor_technical_cal import \
    calculate_rolling_beta
from quant_lib.data_loader import DataLoader

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from quant_lib.config.constant_config import LOCAL_PARQUET_DATA_DIR, permanent__day
from quant_lib.config.logger_config import setup_logger

warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logger = setup_logger(__name__)


def check_field_level_completeness(raw_df: Dict[str, pd.DataFrame]):
    dfs = raw_df.copy()
    for item_name, df in dfs.items():
        logger.info("åŸå§‹å­—æ®µç¼ºå¤±ç‡ä½“æ£€æŠ¥å‘Š:")
        # missing_rate_daily = df.isna().mean(axis=1)

        # logger.info(f"{item_name}å› å­ç¼ºå¤±ç‡æœ€é«˜çš„10å¤© between {first_date} and {end_date}")
        # logger.info(f"{missing_rate_daily.sort_values(ascending=False).head(10)}")  # å…¶å®ä¹Ÿä¸éœ€è¦å¤ªçœ‹é‡ï¼Œåªèƒ½è¯´æ˜¯è¾…åŠ©æ—¥å¿—ï¼Œå¦‚æœæ€»ç¼ºå¤±ç‡é«˜ å¯ä»¥çœ‹çœ‹æ•´ä¸ªè¾…åŠ©æ’æŸ¥è€Œå·²ï¼

        # è®¡ç®—æ¯åªè‚¡ç¥¨ï¼ˆæ¯ä¸€åˆ—ï¼‰çš„ç¼ºå¤±ç‡(ç›¸å½“äºçœ‹è¿™è‚¡ç¥¨ åœ¨è¿™ä¸€æ®µæ—¶é—´çš„å®Œæ•´ç‡ï¼---ã€‹æ¨å¯¼ï¼šæœ€åä¸€å¤©æ‰ä¸Šå¸‚ï¼ï¼Œé‚£ä¹ˆç¼ºå¤±ç‡å¯èƒ½é«˜è¾¾99.99% æ‰€ä»¥ä¸éœ€è¦çœ‹é‡è¿™ä¸ªï¼)  æ³¨é‡Šæ‰
        missing_rate_per_stock = df.isna().mean(axis=0)
        #
        # logger.info(f"{item_name}ï¼ˆä¸æ˜¯å¾ˆé‡è¦ï¼‰å› å­ç¼ºå¤±ç‡æœ€é«˜çš„10åªè‚¡ç¥¨ between {first_date} and {end_date}")
        # logger.info(f"{missing_rate_per_stock.sort_values(ascending=False).head(10)}")

        # è®¡ç®—æ•´ä¸ªDataFrameçš„ç¼ºå¤±ç‡
        total_cells = df.size
        df_all_cells = df.isna().sum().sum()
        global_na_ratio = df_all_cells / total_cells
        logger.info(_get_nan_comment(item_name, global_na_ratio))


def _get_nan_comment(field: str, rate: float) -> str:
    logger.info(f"fieldï¼š{field}åœ¨åŸå§‹raw_df ç¡®å®å æ¯”ä¸ºï¼š{rate}")
    if field in ['delist_date']:
        return f"{field} in ç™½åå•ï¼Œè¿™ç±»å› å­ç¼ºå¤±ç‡å¾ˆé«˜å¾ˆæ­£å¸¸"
    if rate >= 0.5:
        raise ValueError(f'field:{field}ç¼ºå¤±ç‡è¶…è¿‡50% å¿…é¡»æ£€æŸ¥')
    """æ ¹æ®å­—æ®µåç§°å’Œç¼ºå¤±ç‡ï¼Œæä¾›ä¸“å®¶è¯Šæ–­æ„è§"""
    if field in ['pe_ttm', 'pe', 'pb',
                 'pb_ttm'] and rate <= 0.4:  # äº²æµ‹ å¾ˆæ­£å¸¸ï¼Œæœ‰çš„åƒåœ¾è‚¡ç¥¨ price earning ä¸ºè´Ÿã€‚é‚£ä¹ˆtushareç»™æˆ‘çš„æ•°æ®å°±ç®—nanï¼Œåˆç†ï¼
        return " (æ­£å¸¸ç°è±¡: ä¸»è¦ä»£è¡¨å…¬å¸äºæŸ)"

    if field in ['dv_ttm', 'dv_ratio']:
        return " (æ­£å¸¸ç°è±¡: ä¸»è¦ä»£è¡¨å…¬å¸ä¸åˆ†çº¢, åç»­åº”å¡«å……ä¸º0)"

    if field in ['industry']:  # äº²æµ‹ industry å¯ä»¥ç›´æ¥æ”¾è¡Œï¼Œä¸éœ€è¦care å¤šå°‘ç¼ºå¤±ç‡ï¼å› ä¸ºä¹Ÿå°±300ä¸ªï¼Œè€Œä¸”å…¨æ˜¯é€€å¸‚çš„ï¼Œ
        return "æ­£å¸¸ç°è±¡ï¼šä¸éœ€è¦care å¤šå°‘ç¼ºå¤±ç‡"
    if field in ['circ_mv', 'close', 'total_mv',
                 'turnover_rate', 'open', 'high', 'low',
                 'pre_close', 'amount'] and rate < 0.2:  # äº²æµ‹ ä¸€å¤§æ®µæ—¶é—´ï¼Œå¯èƒ½æœ‰çš„è‚¡ç¥¨æœ€åä¸€ä¸ªæœˆæ‰ä¸Šå¸‚ï¼Œå¯¼è‡´å‰é¢ç©ºç¼ºï¼Œæœ‰ç¼ºå¤± é‚£å¾ˆæ­£å¸¸ï¼
        return "æ­£å¸¸ç°è±¡ï¼šä¸éœ€è¦care å¤šå°‘ç¼ºå¤±ç‡"
    if field in ['list_date'] and rate <= 0.01:
        return "æ­£å¸¸ç°è±¡ï¼šä¸éœ€è¦care å¤šå°‘ç¼ºå¤±ç‡"
    if field in ['pct_chg', 'beta'] and rate <= 0.20:
        return "æ­£å¸¸"
    if field in ['ps_ttm'] and rate <= 0.20:
        return "æ­£å¸¸"
    raise ValueError(f"(ğŸš¨ è­¦å‘Š: æ­¤å­—æ®µ{field}ç¼ºå¤±ratio:{rate}!) è¯·è‡ªè¡Œé…ç½®é€šè¿‡ratio æˆ–åˆ™æ˜¯ç¼ºå¤±ç‡å¤ªé«˜ï¼")


class DataManager:
    """
    æ•°æ®ç®¡ç†å™¨ - è´Ÿè´£æ•°æ®åŠ è½½å’Œè‚¡ç¥¨æ± æ„å»º
    
    æŒ‰ç…§é…ç½®æ–‡ä»¶çš„è¦æ±‚ï¼Œå®ç°ï¼š
    1. åŸå§‹æ•°æ®åŠ è½½
    2. åŠ¨æ€è‚¡ç¥¨æ± æ„å»º
    3. æ•°æ®è´¨é‡æ£€æŸ¥
    4. æ•°æ®å¯¹é½å’Œé¢„å¤„ç†
    """

    def __init__(self, config_path: str, need_data_deal: bool = True):
        """
        åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.st_matrix = None  # æ³¨æ„ åç»­ç”¨æ­¤å­—æ®µï¼Œéœ€è¦æ³¨æ„å‰è§†åå·®
        self._tradeable_matrix_by_suspend_resume = None
        self.config = _load_local_config(config_path)
        self.backtest_start_date = self.config['backtest']['start_date']
        self.backtest_end_date = self.config['backtest']['end_date']
        if need_data_deal:
            self.data_loader = DataLoader(data_path=LOCAL_PARQUET_DATA_DIR)
            self.raw_dfs = {}
            self.stock_pools_dict = None
            self.trading_dates = self.data_loader.get_trading_dates(self.backtest_start_date, self.backtest_end_date)
            self._existence_matrix = None

    def prepare_basic_data(self) -> Dict[str, pd.DataFrame]:
        """
        ä¼˜åŒ–çš„ä¸¤é˜¶æ®µæ•°æ®å¤„ç†æµæ°´çº¿ï¼ˆåªåŠ è½½ä¸€æ¬¡æ•°æ®ï¼‰
        Returns:
            å¤„ç†åçš„æ•°æ®å­—å…¸
        """
        # è·å–æ—¶é—´èŒƒå›´
        start_date = self.config['backtest']['start_date']
        end_date = self.config['backtest']['end_date']

        # ç¡®å®šæ‰€æœ‰éœ€è¦çš„å­—æ®µï¼ˆä¸€æ¬¡æ€§ç¡®å®šï¼‰
        all_required_fields = self._get_required_fields()

        # === ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰rawæ•°æ®(äº’ç›¸å¯¹é½) ===

        self.raw_dfs = self.data_loader.get_raw_dfs_by_require_fields(fields=all_required_fields,
                                                                      start_date=start_date, end_date=end_date)

        check_field_level_completeness(self.raw_dfs)
        logger.info(f"raw_dfsåŠ è½½å®Œæˆï¼Œå…±åŠ è½½ {len(self.raw_dfs)} ä¸ªå­—æ®µ")

        # === ç¬¬ä¸€é˜¶æ®µï¼šåŸºäºå·²åŠ è½½æ•°æ®æ„å»ºæƒå¨è‚¡ç¥¨æ±  ===
        logger.info("ç¬¬ä¸€é˜¶æ®µï¼šæ„å»ºä¸¤ä¸ªæƒå¨è‚¡ç¥¨æ± ï¼ˆå„ç§è¿‡æ»¤ï¼ï¼‰")
        self._build_stock_pools_from_loaded_data(start_date, end_date)
        # å¼ºè¡Œæ£€æŸ¥ä¸€ä¸‹æ•°æ®ï¼å®Œæ•´ç‡ï¼ ä¸åº”è¯¥åœ¨è¿™é‡Œæ£€æŸ¥ï¼ï¼Œå¤ªæ™šäº†ï¼Œ å·²ç»è¢«stock_pool_df åŠ¨äº†æ‰‹è„šäº†ï¼ˆä½å¸‚å€¼çš„ä¼šè¢«ç½®ä¸ºnanï¼Œ

    # ok
    def _build_stock_pools_from_loaded_data(self, start_date: str, end_date: str) -> pd.DataFrame:
        """
        ç¬¬ä¸€é˜¶æ®µï¼šåŸºäºå·²åŠ è½½çš„æ•°æ®æ„å»ºæƒå¨è‚¡ç¥¨æ± 

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            æƒå¨è‚¡ç¥¨æ± DataFrame
        """
        # print("1. éªŒè¯è‚¡ç¥¨æ± æ„å»ºæ‰€éœ€æ•°æ®...")

        # éªŒè¯å¿…éœ€å­—æ®µæ˜¯å¦å·²åŠ è½½
        required_fields_for_universe = ['close', 'total_mv', 'turnover_rate', 'industry', 'list_date']
        missing_fields = [field for field in required_fields_for_universe if field not in self.raw_dfs]

        if missing_fields:
            raise ValueError(f"æ„å»ºè‚¡ç¥¨æ± ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")

        self.build_diff_stock_pools()

    def build_diff_stock_pools(self):
        stock_pool_df_dict = {}
        stock_pool_profiles = self.config['stock_pool_profiles']
        for pool_name, pool_config in stock_pool_profiles.items():
            product_universe = self.create_stock_pool(pool_config, pool_name)
            stock_pool_df_dict[pool_name] = product_universe
        self.stock_pools_dict = stock_pool_df_dict

    # institutional_profile   = stock_pool_profiles['institutional_profile']#ä¸ºâ€œåŸºæœ¬é¢æ´¾â€å’Œâ€œè¶‹åŠ¿æ´¾â€å› å­ï¼Œæä¾›ä¸€ä¸ªé«˜å¸‚å€¼ã€é«˜æµåŠ¨æ€§çš„ç¯å¢ƒ
    # microstructure_profile = stock_pool_profiles['microstructure_profile']#ç”¨äº å¾®è§‚ï¼ˆé‡ä»·/æƒ…ç»ªï¼‰å› å­
    # product_universe =self.product_universe (microstructure_profile,trading_dates)

    # å¯¹äº æ˜¯å…ˆ fill è¿˜æ˜¯å…ˆwhere çš„è€ƒé‡ ï¼šè¿˜æ˜¯åˆ«å…ˆffilläº†ï¼šæç«¯ä¾‹å­ï¼šåœç‰Œäº†99å¤©çš„ï¼Œ100ã€‚ è‹¥å…ˆffillé‚£ä¹ˆ è¿™100å¤©éƒ½æ˜¯å€Ÿæ¥çš„æ•°æ®ï¼  å¦‚æœå…ˆwhereã€‚é‚£ä¹ˆç›´æ¥ç»Ÿç»Ÿnanäº†ã€‚åœ¨ffillä¹Ÿæ˜¯nanï¼Œæ›´å…·çœŸå®
    # ok
    def _align_many_raw_dfs_by_stock_pool_and_fill(self, raw_dfs: Dict[str, pd.DataFrame],
                                                   stock_pool_df: pd.DataFrame,
                                                   ) -> Dict[str, pd.DataFrame]:
        if stock_pool_df is None or stock_pool_df.empty:
            raise ValueError("stock_pool_param å¿…é¡»ä¼ å…¥ä¸”ä¸èƒ½ä¸ºç©ºçš„ DataFrame")
        """
        ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨æƒå¨è‚¡ç¥¨æ± å¯¹é½å’Œæ¸…æ´—æ‰€æœ‰æ•°æ®

        Args:
            raw_dfs: åŸå§‹æ•°æ®å­—å…¸
            stock_pool_df: æƒå¨è‚¡ç¥¨æ± DataFrame
        Returns:
            å¯¹é½å’Œæ¸…æ´—åçš„æ•°æ®å­—å…¸
        """
        aligned_data = {}
        for factor_name, raw_df in raw_dfs.items():
            # 1. ç¡®å®šå½“å‰å› å­éœ€è¦å“ªä¸ªè‚¡ç¥¨æ± ï¼
            aligned_df = align_one_df_by_stock_pool_and_fill(factor_name=factor_name, df=raw_df,
                                                             stock_pool_df=stock_pool_df)
            aligned_data[factor_name] = aligned_df
        return aligned_data

    def _get_required_fields(self) -> List[str]:
        """è·å–æ‰€æœ‰éœ€è¦çš„å­—æ®µ"""
        required_fields = set()

        # åŸºç¡€å­—æ®µ
        required_fields.update([
            'pct_chg',  # è‚¡ç¥¨æ”¶ç›Šä¸æŒ‡æ•°æ”¶ç›Šçš„è”åŠ¨beta (ç”¨äºä¸­æ€§åŒ– è¿›ä¸€æ­¥å‡€åŒ–å› å­ å®ƒèƒ½ä¸ºåŠ¨é‡å› å­â€œé™å™ªâ€ï¼Œé¢å¤–å‰”é™¤å¸‚åœºç³»ç»Ÿæ€§é£é™©ï¼ˆBetaï¼‰çš„å½±å“ã€‚

            'close',
            'pb',  # ä¸ºäº†è®¡ç®—ä»·å€¼ç±»å› å­
            'total_mv', 'turnover_rate',  # ä¸ºäº†è¿‡æ»¤ å¾ˆå·®åŠ²çš„è‚¡ç¥¨  ï¼Œ  ã€'total_mv'è¿˜å¯ ç”¨äºè®¡ç®—ä¸­æ€§åŒ–
            'industry',  # ç”¨äºè®¡ç®—ä¸­æ€§åŒ–
            'circ_mv',  # æµé€šå¸‚å€¼ ç”¨äºWOSï¼ŒåŠ æƒæœ€å°äºŒæ–¹è·Ÿ  ï¼Œå›å½’æ³•ä¼šç”¨åˆ°
            'list_date',  # ä¸Šå¸‚æ—¥æœŸ,
            'delist_date',  # é€€å¸‚æ—¥æœŸ,ç”¨äºæ„å»ºæ ‡å‡†åŠ¨æ€è‚¡ç¥¨æ± 

            'open', 'high', 'low', 'pre_close',  # ä¸ºäº†è®¡ç®—æ¬¡æ—¥æ˜¯å¦ä¸€å­—é©¬æ¶¨åœ
            'pe_ttm', 'ps_ttm',  # æ‡’å¾—å†™calcu ç›´æ¥åœ¨è¿™é‡Œç”Ÿæˆå°±å¥½
        ])
        # é‰´äº get_raw_dfs_by_require_fields é’ˆå¯¹æ²¡æœ‰trade_dateåˆ—çš„parquetï¼Œå¯¹æ•´ä¸ªparquetçš„å­—æ®µï¼Œæ˜¯è¿›è¡Œæ— è„‘ å¹¿æ’­çš„ã€‚ éœ€è¦æ³¨æ„ï¼šæŠ¥å‘ŠæœŸ(æ¯ä¸ªå­£åº¦æœ€åä¸€å¤©çš„æ—¥æœŸï¼‰ä¹Ÿå°±æ˜¯end_date ç°é‡‘æµé‡è¡¨ä¸¾ä¾‹æ¥è¯´ï¼Œå°±åªæœ‰end_Dateå­—æ®µï¼Œä¸é€‚åˆå¹¿æ’­ï¼
        # è§£å†³åŠæ³•ï¼š
        # æˆ‘å†³å®š è¿™ä¸éœ€è¦äº†ï¼Œè‡ªè¡Œåœ¨factor_calculatoré‡Œé¢ è‡ªå®šä¹‰_calcuâ€”å‡½æ•° æ›´æ¸…æ™°ï¼
        # æœ€æ–°è§£å†³åŠæ³• åŠ ä¸€ä¸ªcal_require_base_fields_from_dailyæ ‡è¯†å°±å¯ä»¥äº†
        target_factors_for_evaluation = self.config['target_factors_for_evaluation']
        required_fields.update(self.get_cal_base_factors(target_factors_for_evaluation['fields']))

        # ä¸­æ€§åŒ–éœ€è¦çš„å­—æ®µ
        neutralization = self.config['preprocessing']['neutralization']
        if neutralization['enable']:
            if 'industry' in neutralization['factors']:
                required_fields.add('industry')
            if 'market_cap' in neutralization['factors']:
                required_fields.add('total_mv')
        return list(required_fields)

    def _check_data_quality(self):
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        print("  æ£€æŸ¥æ•°æ®å®Œæ•´æ€§å’Œè´¨é‡...")

        for field_name, df in self.raw_dfs.items():
            # æ£€æŸ¥æ•°æ®å½¢çŠ¶
            print(f"  {field_name}: {df.shape}")

            # æ£€æŸ¥ç¼ºå¤±å€¼æ¯”ä¾‹
            missing_ratio = df.isnull().sum().sum() / (df.shape[0] * df.shape[1])
            print(f"    ç¼ºå¤±å€¼æ¯”ä¾‹: {missing_ratio:.2%}")

            # æ£€æŸ¥å¼‚å¸¸å€¼
            if field_name in ['close', 'total_mv', 'pb', 'pe_ttm']:
                negative_ratio = (df <= 0).sum().sum() / df.notna().sum().sum()
                print(f"  æå€¼(>99%åˆ†ä½) å æ¯”: {((df > df.quantile(0.99)).sum().sum()) / (df.shape[0] * df.shape[1])}")

                if negative_ratio > 0:
                    print(f"    è­¦å‘Š: {field_name} å­˜åœ¨ {negative_ratio:.2%} çš„éæ­£å€¼")

    # ok è¿™æ”¯è‚¡ç¥¨åœ¨è¿™ä¸€å¤©æ˜¯å¦å·²ä¸Šå¸‚ä¸”æœªé€€å¸‚_df
    def build_existence_matrix(self) -> pd.DataFrame:
        """
        æ ¹æ®æ¯æ—¥æ›´æ–°çš„ä¸Šå¸‚/é€€å¸‚æ—¥æœŸé¢æ¿ï¼Œæ„å»ºæ¯æ—¥â€œå­˜åœ¨æ€§â€çŸ©é˜µã€‚
        """
        logger.info("    æ­£åœ¨æ„å»ºè‚¡ç¥¨â€œå­˜åœ¨æ€§â€çŸ©é˜µ..")
        # 1. è·å–ä½œä¸ºè¾“å…¥çš„ä¸Šå¸‚å’Œé€€å¸‚æ—¥æœŸé¢æ¿
        list_date_panel = self.raw_dfs.get('list_date')
        delist_date_panel = self.raw_dfs.get('delist_date')

        if list_date_panel is None or delist_date_panel is None:
            raise ValueError("ç¼ºå°‘'list_date'æˆ–'delist_date'é¢æ¿æ•°æ®ï¼Œæ— æ³•æ„å»ºå­˜åœ¨æ€§çŸ©é˜µã€‚")

        # 2. ã€æ ¸å¿ƒã€‘å‘é‡åŒ–æ„å»ºå¸ƒå°”æ©ç  (Boolean Masks)

        # a. åˆ›å»ºä¸€ä¸ªâ€œåŸºå‡†æ—¥æœŸâ€çŸ©é˜µï¼Œç”¨äºæ¯”è¾ƒ
        #    è¯¥çŸ©é˜µçš„æ¯ä¸ªå•å…ƒæ ¼[date, stock]çš„å€¼ï¼Œå°±æ˜¯è¯¥å•å…ƒæ ¼çš„æ—¥æœŸ'date'
        #    è¿™å…è®¸æˆ‘ä»¬å°†æ¯ä¸ªå•å…ƒæ ¼çš„â€œå½“å‰æ—¥æœŸâ€ä¸å®ƒçš„ä¸Šå¸‚/é€€å¸‚æ—¥æœŸè¿›è¡Œæ¯”è¾ƒ
        dates_matrix = pd.DataFrame(
            data=np.tile(list_date_panel.index.values, (len(list_date_panel.columns), 1)).T,
            index=list_date_panel.index,
            columns=list_date_panel.columns
        )

        # b. æ„å»ºâ€œæ˜¯å¦å·²ä¸Šå¸‚â€çš„æ©ç  (after_listing_mask)
        #    ç›´æ¥æ¯”è¾ƒä¸¤ä¸ªç›¸åŒå½¢çŠ¶çš„DataFrame
        #    å¦‚æœ å½“å‰æ—¥æœŸ >= ä¸Šå¸‚æ—¥æœŸ, åˆ™ä¸ºTrue
        after_listing_mask = (dates_matrix >= list_date_panel)

        # c. æ„å»ºâ€œæ˜¯å¦æœªé€€å¸‚â€çš„æ©ç  (before_delisting_mask)
        #    åŒæ ·ï¼Œå…ˆç”¨ä¸€ä¸ªé¥è¿œçš„æœªæ¥æ—¥æœŸå¡«å……NaTï¼ˆæœªé€€å¸‚çš„æƒ…å†µï¼‰
        future_date = pd.Timestamp(permanent__day)
        delist_dates_filled = delist_date_panel.fillna(future_date)

        #    å¦‚æœ å½“å‰æ—¥æœŸ < é€€å¸‚æ—¥æœŸ, åˆ™ä¸ºTrue
        before_delisting_mask = (dates_matrix < delist_dates_filled)

        # 4. åˆå¹¶æ©ç ï¼Œå¾—åˆ°æœ€ç»ˆçš„â€œå­˜åœ¨æ€§â€çŸ©é˜µ
        #    ä¸€ä¸ªè‚¡ç¥¨å½“å¤©â€œå­˜åœ¨â€ï¼Œå½“ä¸”ä»…å½“å®ƒâ€œå·²ä¸Šå¸‚â€ AND â€œæœªé€€å¸‚â€
        existence_matrix = after_listing_mask & before_delisting_mask

        logger.info("    è‚¡ç¥¨â€œå­˜åœ¨æ€§â€çŸ©é˜µæ„å»ºå®Œæ¯•ã€‚")
        # ç¼“å­˜èµ·æ¥ï¼Œå› ä¸ºå®ƒåœ¨ä¸€æ¬¡å›æµ‹ä¸­æ˜¯ä¸å˜çš„
        self._existence_matrix = existence_matrix

    def build_tradeable_matrix_by_suspend_resume(
            self,
    ) -> pd.DataFrame:
        """
         æ ¹æ®å®Œæ•´çš„åœå¤ç‰Œå†å²ï¼Œæ„å»ºæ¯æ—¥â€œå¯äº¤æ˜“â€çŠ¶æ€çŸ©é˜µã€‚

        """
        if self._tradeable_matrix_by_suspend_resume is not None:
            logger.info(
                "self._tradeable_matrix_by_suspend_resume ä¹‹å‰ä»¥åŠè¢«åˆå§‹åŒ–ï¼Œæ— éœ€å†æ¬¡åŠ è½½ï¼ˆè¿™æ˜¯å…¨é‡æ•°æ®ï¼Œä¸€æ¬¡åŠ è½½å³å¯")
            return self._tradeable_matrix_by_suspend_resume
        # æ•°æ®å‡†å¤‡ è·å–æ‰€æœ‰è‚¡ç¥¨å’Œäº¤æ˜“æ—¥æœŸ
        ts_codes = list(set(self.get_price_data().columns))
        trading_dates = self.data_loader.get_trading_dates(start_date=self.backtest_start_date,
                                                           end_date=self.backtest_end_date)

        logger.info("ã€ä¸“ä¸šç‰ˆã€‘æ­£åœ¨é‡å»ºæ¯æ—¥â€˜å¯äº¤æ˜“â€™çŠ¶æ€çŸ©é˜µ...")
        suspend_df = load_suspend_d_df()  # ç›´æ¥ä¼ å…¥å®Œæ•´çš„åœå¤ç‰Œæ•°æ®

        # --- 1. æ•°æ®é¢„å¤„ç† ---
        # ç¡®ä¿suspend_dfä¸­çš„æ—¥æœŸæ˜¯datetimeç±»å‹ï¼Œå¹¶æŒ‰è‚¡ç¥¨å’Œæ—¥æœŸæ’åº
        suspend_df['trade_date'] = pd.to_datetime(suspend_df['trade_date'])
        suspend_df.sort_values(by=['ts_code', 'trade_date'], inplace=True)

        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„DataFrameï¼Œå‡†å¤‡é€åˆ—å¡«å……
        tradeable_matrix = pd.DataFrame(index=trading_dates, columns=ts_codes, dtype=bool)

        # --- 2. é€ä¸€å¤„ç†æ¯åªè‚¡ç¥¨çš„çŠ¶æ€åºåˆ— ---
        for ts_code in ts_codes:
            # a. è·å–è¯¥è‚¡ç¥¨çš„æ‰€æœ‰åœå¤ç‰Œäº‹ä»¶
            stock_events = suspend_df[suspend_df['ts_code'] == ts_code]

            # åˆ›å»ºä¸€ä¸ªç”¨äºçŠ¶æ€ä¼ æ’­çš„ä¸´æ—¶Seriesï¼Œåˆå§‹å€¼å…¨ä¸ºNaN
            status_series = pd.Series(np.nan, index=trading_dates)

            # b. ã€æ ¸å¿ƒã€‘ç¡®å®šåˆå§‹çŠ¶æ€
            # æŸ¥æ‰¾åœ¨å›æµ‹å¼€å§‹æ—¥æœŸä¹‹å‰å‘ç”Ÿçš„æœ€åä¸€ä¸ªäº‹ä»¶
            events_before_start = stock_events[stock_events['trade_date'] < trading_dates[0]]
            if not events_before_start.empty:
                # å¦‚æœå­˜åœ¨ï¼Œåˆ™æœ€åä¸€ä¸ªäº‹ä»¶çš„ç±»å‹å†³å®šäº†åˆå§‹çŠ¶æ€
                # 'R' (Resumed) -> True (å¯äº¤æ˜“), 'S' (Suspended) -> False (ä¸å¯äº¤æ˜“)
                initial_status = (events_before_start.iloc[-1]['suspend_type'] == 'R')
            else:
                # å¦‚æœä¹‹å‰æ²¡æœ‰ä»»ä½•åœå¤ç‰Œäº‹ä»¶ï¼Œåˆ™é»˜è®¤ä¸ºå¯äº¤æ˜“
                initial_status = True

            # åœ¨æˆ‘ä»¬çš„çŠ¶æ€åºåˆ—çš„ç¬¬ä¸€ä¸ªä½ç½®ï¼Œè®¾ç½®å¥½åˆå§‹çŠ¶æ€
            status_series.iloc[0] = initial_status

            # c. ã€æ ¸å¿ƒã€‘æ ‡è®°å›æµ‹æœŸå†…çš„çŠ¶æ€å˜åŒ–â€œæ‹ç‚¹â€
            events_in_period = stock_events[stock_events['trade_date'].isin(trading_dates)]
            for _, event in events_in_period.iterrows():
                event_date = event['trade_date']
                is_tradeable = (event['suspend_type'] == 'R')
                status_series[event_date] = is_tradeable

            # d. ã€æ ¸å¿ƒã€‘çŠ¶æ€ä¼ æ’­ (Forward Fill)
            # ffillä¼šç”¨å‰ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……åé¢çš„NaNï¼Œå®Œç¾æ¨¡æ‹Ÿäº†çŠ¶æ€çš„æŒç»­æ€§
            status_series.ffill(inplace=True)

            # å°†è¿™åªè‚¡ç¥¨è®¡ç®—å¥½çš„å®Œæ•´çŠ¶æ€åºåˆ—ï¼Œå¡«å……åˆ°æ€»çŸ©é˜µä¸­
            tradeable_matrix[ts_code] = status_series

        # e. æ”¶å°¾å·¥ä½œï¼šå¯¹äºæ²¡æœ‰ä»»ä½•åœå¤ç‰Œå†å²çš„è‚¡ç¥¨ï¼Œå®ƒä»¬åˆ—å¯èƒ½ä¾ç„¶æ˜¯NaNï¼Œé»˜è®¤ä¸ºå¯äº¤æ˜“
        tradeable_matrix.fillna(True, inplace=True)

        logger.info("æ¯æ—¥â€˜å¯äº¤æ˜“â€™çŠ¶æ€çŸ©é˜µé‡å»ºå®Œæ¯•ã€‚")
        self._tradeable_matrix_by_suspend_resume = tradeable_matrix.astype(bool)
        return self._tradeable_matrix_by_suspend_resume

    # ok
    def build_st_period_from_namechange(
            self,
    ) -> pd.DataFrame:
        """
         æ ¹æ®namechangeå†å²ï¼Œé‡å»ºæ¯æ—¥â€œå·²çŸ¥é£é™©â€çŠ¶æ€çŸ©é˜µã€‚
         æ­¤ç‰ˆæœ¬é€šè¿‡searchsortedéšå¼å¤„ç†åˆå§‹çŠ¶æ€ï¼Œé€»è¾‘æœ€ç®€ä¸”ç»“æœæ­£ç¡®ã€‚
         """
        if self.st_matrix is not None:
            logger.info("self.st_matrix ä¹‹å‰å·²ç»è¢«åˆå§‹åŒ–ï¼Œæ— éœ€å†æ¬¡åŠ è½½ï¼ˆè¿™æ˜¯å…¨é‡æ•°æ®ï¼Œä¸€æ¬¡åŠ è½½å³å¯")
            return self.st_matrix
        logger.info("æ­£åœ¨æ ¹æ®åç§°å˜æ›´å†å²ï¼Œé‡å»ºæ¯æ—¥â€˜å·²çŸ¥é£é™©â€™çŠ¶æ€stçŸ©é˜µ...")
        # æ•°æ®å‡†å¤‡ è·å–æ‰€æœ‰è‚¡ç¥¨å’Œäº¤æ˜“æ—¥æœŸ
        ts_codes = list(set(self.get_price_data().columns))
        trading_dates = self.data_loader.get_trading_dates(start_date=self.backtest_start_date,
                                                           end_date=self.backtest_end_date)
        namechange_df = self.get_namechange_data()

        # --- 1. å‡†å¤‡å·¥ä½œ ---
        if not trading_dates._is_monotonic_increasing:
            trading_dates = trading_dates.sort_values(ascending=True)

        # ã€å…³é”®ã€‘å¿…é¡»æŒ‰â€œç”Ÿæ•ˆæ—¥â€æ’åºï¼Œä»¥ç¡®ä¿çŠ¶æ€çš„æ­£ç¡®å»¶ç»­å’Œè¦†ç›–
        namechange_df['start_date'] = pd.to_datetime(namechange_df['start_date'])
        namechange_df.sort_values(by=['ts_code', 'start_date'], inplace=True)

        # ã€å…³é”®ã€‘å¿…é¡»ç”¨ np.nan åˆå§‹åŒ–ï¼Œä½œä¸ºâ€œæœªçŸ¥çŠ¶æ€â€
        st_matrix = pd.DataFrame(np.nan, index=trading_dates, columns=ts_codes)

        # --- 2. â€œæ‰“ç‚¹â€ï¼šä¸€ä¸ªå¾ªç¯å¤„ç†æ‰€æœ‰å†å²äº‹ä»¶ ---
        for ts_code, group in namechange_df.groupby('ts_code'):
            group_sorted = group.sort_values(by='start_date')
            for _, row in group_sorted.iterrows():
                start_date = row['start_date']

                # å‘ç”Ÿåœ¨å›æµ‹æœŸå‰çš„æ—¥æœŸï¼Œä¼šè¢«è‡ªåŠ¨æ˜ å°„åˆ°ä½ç½® 0  or å‘ç”Ÿåœ¨å›æµ‹æœŸå†…çš„æ—¥æœŸï¼Œä¼šè¢«æ˜ å°„åˆ°å®ƒå¯¹åº”çš„æ­£ç¡®ä½ç½®
                start_date_loc = trading_dates.searchsorted(start_date,
                                                            side='left')  # éå†trading_datesæ‰¾åˆ°é¦–ä¸ª>=start_dateçš„ä¸‹æ ‡ï¼ å¦‚æœæ˜¯rigths ï¼šåˆ™é¦–ä¸ª>çš„ä¸‹æ ‡

                # åªå¤„ç†é‚£äº›èƒ½å½±å“åˆ°æˆ‘ä»¬å›æµ‹å‘¨æœŸçš„äº‹ä»¶
                if start_date_loc < len(trading_dates):
                    name_upper = row['name'].upper()
                    is_risk_event = 'ST' in name_upper or name_upper.startswith('S')
                    # ä½¿ç”¨.ilocè¿›è¡Œèµ‹å€¼
                    start_trade_date = pd.DatetimeIndex(trading_dates)[start_date_loc]
                    st_matrix.loc[start_trade_date, ts_code] = is_risk_event

        # --- 3. â€œä¼ æ’­â€ä¸â€œæ”¶å°¾â€ ---
        st_matrix.ffill(inplace=True)
        st_matrix.fillna(False, inplace=True)

        logger.info("æ¯æ—¥â€˜å·²çŸ¥é£é™©â€™çŠ¶æ€çŸ©é˜µé‡å»ºå®Œæ¯•ã€‚")
        self.st_matrix = st_matrix.astype(bool)
        return self.st_matrix

    # ok ä¸ºä»€ä¹ˆä¸éœ€è¦shift1 å› ä¸ºä¼ä¸šä¸Šå¸‚ä¿¡æ¯ï¼Œå¾ˆå¾ˆæ—©çš„ä¿¡æ¯ï¼Œä¸å±äºåé¢ä¿¡æ¯
    def _filter_new_stocks(self, stock_pool_df: pd.DataFrame, months: int = 6) -> pd.DataFrame:
        """
        å‰”é™¤ä¸Šå¸‚æ—¶é—´å°äºæŒ‡å®šæœˆæ•°çš„è‚¡ç¥¨ã€‚
        """

        if 'list_date' not in self.raw_dfs:
            raise ValueError("ç¼ºå°‘ä¸Šå¸‚æ—¥æœŸæ•°æ®(list_date)ï¼Œè·³è¿‡æ–°è‚¡è¿‡æ»¤ã€‚")

        list_dates_df = self.raw_dfs['list_date']
        if list_dates_df.empty:
            return stock_pool_df

        # --- 1. å¯¹é½æ•°æ® ---
        aligned_universe, aligned_list_dates = stock_pool_df.align(list_dates_df, join='left')

        # --- 2. ã€æ ¸å¿ƒä¿®æ­£ã€‘å¼ºåˆ¶è½¬æ¢æ•°æ®ç±»å‹ ---
        # åœ¨æå– .values ä¹‹å‰ï¼Œç¡®ä¿æ•´ä¸ªDataFrameæ˜¯np.datetime64ç±»å‹
        # errors='coerce' ä¼šå°†ä»»ä½•æ— æ³•è½¬æ¢çš„å€¼ï¼ˆæ¯”å¦‚ç©ºå€¼æˆ–é”™è¯¯å­—ç¬¦ä¸²ï¼‰å˜æˆ NaT (Not a Time)
        try:
            list_dates_converted = aligned_list_dates.apply(pd.to_datetime, errors='raise')
        except Exception as e:
            raise ValueError(f"ä¸Šå¸‚æ—¥æœŸæ•°æ®æ— æ³•è½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼ï¼Œè¯·æ£€æŸ¥æ•°æ®æº: {e}")
            # return stock_pool_df  # Or handle error appropriately

        # --- 3. å‘é‡åŒ–è®¡ç®— ---
        dates_arr = aligned_universe.index.values[:, np.newaxis]

        # ç°åœ¨ list_dates_arr çš„ dtype å°†æ˜¯ <M8[ns]
        list_dates_arr = list_dates_converted.values

        # ç”±äº NaT - NaT = NaT, æˆ‘ä»¬éœ€è¦å¤„ç† NaTã€‚å¹¿æ’­è®¡ç®—æœ¬èº«ä¸ä¼šæŠ¥é”™ã€‚
        time_since_listing = dates_arr - list_dates_arr

        # --- 4. åˆ›å»ºå¹¶åº”ç”¨æ©ç  ---
        threshold = pd.Timedelta(days=months * 30.5)
        # NaT < threshold ä¼šæ˜¯ False, æ‰€ä»¥ NaT å€¼ä¸ä¼šè¢«é”™è¯¯åœ°å½“ä½œæ–°è‚¡
        is_new_mask = time_since_listing < threshold

        aligned_universe.values[is_new_mask] = False
        self.show_stock_nums_for_per_day("6ä¸ªæœˆå†…ä¸Šå¸‚çš„è¿‡æ»¤ï¼", aligned_universe)
        return aligned_universe

    # ok å·²ç»å¤„ç†å‰è§†åå·®
    def _filter_st_stocks(self, stock_pool_df: pd.DataFrame) -> pd.DataFrame:
        if self.st_matrix is None:
            raise ValueError("    è­¦å‘Š: æœªèƒ½æ„å»ºSTçŠ¶æ€çŸ©é˜µï¼Œæ— æ³•è¿‡æ»¤STè‚¡ç¥¨ã€‚")
        # ã€æ ¸å¿ƒã€‘å°†â€œå†å²çœŸç›¸â€çŸ©é˜µæ•´ä½“å‘å‰ï¼ˆæœªæ¥ï¼‰ç§»åŠ¨ä¸€å¤©ã€‚ (å› ä¸ºst_matrix æ˜¯ä»¥æ®ç”Ÿæ•ˆstart_Dayæ—¥è®¡ç®—çš„ã€‚tä¸‹å•ï¼Œåªèƒ½ç”¨t-1çš„æ•°æ®è·‘ï¼Œtå•æ—¥çš„stæ— æ³•æ„ŸçŸ¥ï¼
        # è¿™ç¡®ä¿äº†æˆ‘ä»¬åœ¨Tæ—¥åšå†³ç­–æ—¶ï¼Œçœ‹åˆ°çš„æ˜¯T-1æ—¥çš„çœŸå®çŠ¶æ€ ã€‚
        st_mask_shifted = self.st_matrix.shift(1, fill_value=False)
        # å¯¹é½ä¸¤ä¸ªDataFrameçš„ç´¢å¼•å’Œåˆ—ï¼Œç¡®ä¿ä¸‡æ— ä¸€å¤±
        # join='left' è¡¨ç¤ºä»¥stock_pool_dfçš„å½¢çŠ¶ä¸ºå‡†
        aligned_universe, aligned_st_status = stock_pool_df.align(st_mask_shifted, join='left',
                                                                  fill_value=False)  # è‡³å°‘åš è¡Œåˆ— ä¿æŒä¸€è‡´çš„å¯¹é½ã€‚ ä¸‹é¢æ‰åšèµ‹å€¼ï¼ #fill_value=False ï¼šst_Dfåªèƒ½å¯¹åº”ä¸€éƒ¨åˆ†çš„è‚¡ç¥¨æ± _Df.è‚¡ç¥¨æ± _Dfå‰©ä½™çš„è¡Œåˆ— ç”¨falseå¡«å……ï¼

        # å°†STçš„è‚¡ç¥¨ä»universeä¸­å‰”é™¤
        # aligned_st_statusä¸ºTrueçš„åœ°æ–¹ï¼Œåœ¨universeä¸­å°±åº”è¯¥ä¸ºFalse
        aligned_universe[aligned_st_status] = False

        # ç»Ÿè®¡è¿‡æ»¤æ•ˆæœ
        original_count = stock_pool_df.sum(axis=1).mean()
        filtered_count = aligned_universe.sum(axis=1).mean()
        st_filtered_count = original_count - filtered_count
        print(f"      STè‚¡ç¥¨è¿‡æ»¤: å¹³å‡æ¯æ—¥å‰”é™¤ {st_filtered_count:.0f} åªSTè‚¡ç¥¨")
        self.show_stock_nums_for_per_day(f'by_STçŠ¶æ€(åˆ¤å®šæ¥è‡ªäºnameçš„å˜åŒ–å†å²)_filter', aligned_universe)

        return aligned_universe

    # ok
    #
    def _filter_by_existence(self, stock_pool_df: pd.DataFrame) -> pd.DataFrame:
        """
        ã€V3.0-ä¼˜åŒ–ç‰ˆã€‘åŸºäºé¢„å…ˆæ„å»ºå¥½çš„â€œå­˜åœ¨æ€§â€çŸ©é˜µï¼Œè¿›è¡Œæœ€é«˜æ•ˆçš„è¿‡æ»¤ã€‚
        æ­¤è¿‡æ»¤å™¨åŒæ—¶å¤„ç†äº†â€œæœªä¸Šå¸‚â€å’Œâ€œå·²é€€å¸‚â€ä¸¤ç§æƒ…å†µï¼Œæ˜¯å­˜åœ¨æ€§æ£€éªŒçš„å”¯ä¸€å…¥å£ã€‚
        """
        logger.info("    åº”ç”¨ç»Ÿä¸€çš„å­˜åœ¨æ€§è¿‡æ»¤ (ä¸Šå¸‚ & é€€å¸‚)...")

        # 1. è·å–æˆ–æ„å»ºæƒå¨çš„å­˜åœ¨æ€§çŸ©é˜µ (åº”è¯¥å·²è¢«ç¼“å­˜)
        #    è¿™ä¸ªçŸ©é˜µå·²ç»åŒ…å«äº†æ‰€æœ‰ä¸Šå¸‚/é€€å¸‚çš„å®Œæ•´ä¿¡æ¯ã€‚
        if self._existence_matrix is None:
            self.build_existence_matrix()

        existence_matrix = self._existence_matrix

        # 2. ã€æ ¸å¿ƒã€‘åº”ç”¨T-1åŸåˆ™
        #    å°†æ•´ä¸ªâ€œå­˜åœ¨æ€§â€çŠ¶æ€çŸ©é˜µå‘å‰ç§»åŠ¨ä¸€å¤©ã€‚
        #    è¿™æ ·åœ¨Tæ—¥å†³ç­–æ—¶ï¼Œä½¿ç”¨çš„å°±æ˜¯T-1æ—¥è¯¥è‚¡ç¥¨æ˜¯å¦å­˜åœ¨çš„ä¿¡æ¯ã€‚
        existence_mask_shifted = existence_matrix.shift(1, fill_value=False)

        # 3. å®‰å…¨å¯¹é½å¹¶åº”ç”¨è¿‡æ»¤å™¨
        #    fill_value=False è¡¨ç¤ºï¼Œå¦‚æœä¸€ä¸ªè‚¡ç¥¨åœ¨æ‚¨çš„åŸºç¡€æ± ä¸­ï¼Œ
        #    ä½†ä¸åœ¨æˆ‘ä»¬çš„å­˜åœ¨æ€§çŸ©é˜µçš„è€ƒè™‘èŒƒå›´å†…ï¼Œæˆ‘ä»¬é»˜è®¤å®ƒä¸å­˜åœ¨ã€‚
        aligned_pool, aligned_existence_mask = stock_pool_df.align(
            existence_mask_shifted,
            join='left',
            axis=None,
            fill_value=False
        )

        filtered_pool = aligned_pool & aligned_existence_mask

        # 4. ç»Ÿè®¡æ—¥å¿—
        original_count = stock_pool_df.sum().sum()
        filtered_count = filtered_pool.sum().sum()
        delisted_removed_count = original_count - filtered_count
        logger.info(
            f"      existenceä¸Šå¸‚é€€å¸‚è‚¡ç¥¨è¿‡æ»¤(: åœ¨æ•´ä¸ªå›æµ‹æœŸé—´ï¼Œå…±ç§»é™¤äº† {delisted_removed_count:.0f} ä¸ª'å·²é€€å¸‚'çš„è‚¡ç¥¨æ¬¡ï¼ˆè‚¡ç¥¨ç´¯è®¡éexistenceå¤©æ•°ï¼‰")
        self.show_stock_nums_for_per_day('by_ç»Ÿä¸€å­˜åœ¨æ€§_filter', filtered_pool)

        return filtered_pool

    # é€‚é…åœç»å†å¤ç‰Œäº‹ä»¶çš„å¯äº¤æ˜“è‚¡ç¥¨æ±  ok
    def _filter_tradeable_matrix_by_suspend_resume(self, stock_pool_df: pd.DataFrame) -> pd.DataFrame:
        if self._tradeable_matrix_by_suspend_resume is None:
            raise ValueError("è­¦å‘Š: æœªèƒ½æ„å»º _tradeable_matrix_by_suspend_resume çŠ¶æ€çŸ©é˜µã€‚")

            # 1. ã€ä¿®æ­£ç»†èŠ‚ã€‘shift æ—¶ï¼Œç”¨ True å¡«å……ç¬¬ä¸€è¡Œï¼Œå› ä¸ºé»˜è®¤è‚¡ç¥¨æ˜¯å¯äº¤æ˜“çš„ã€‚
        tradeable_mask_shifted = self._tradeable_matrix_by_suspend_resume.shift(1, fill_value=True)

        # 2. å¯¹é½è‚¡ç¥¨æ± å’Œå¯äº¤æ˜“çŠ¶æ€æ©ç 
        #    join='left' ä¿è¯äº†è‚¡ç¥¨æ± çš„è‚¡ç¥¨é›†åˆä¸å‘ç”Ÿå˜åŒ–
        #    fill_value=True å‡è®¾æœªåœ¨åœå¤ç‰Œä¿¡æ¯ä¸­å‡ºç°çš„è‚¡ç¥¨æ˜¯å¯äº¤æ˜“çš„ï¼ˆå®‰å…¨åšæ³•ï¼‰
        aligned_universe, aligned_tradeable_mask = stock_pool_df.align(
            tradeable_mask_shifted,
            join='left',
            fill_value=True
        )

        # ç»Ÿè®¡è¿‡æ»¤å‰çš„æ•°é‡
        pre_filter_count = aligned_universe.sum().sum()

        # 3. ã€ä¿®æ­£æ ¸å¿ƒBugã€‘ä½¿ç”¨å¸ƒå°”â€œä¸â€è¿ç®—è¿›è¡Œè¿‡æ»¤
        #    æœ€ç»ˆçš„è‚¡ç¥¨æ±  = ä¹‹å‰çš„è‚¡ç¥¨æ±  AND å¯äº¤æ˜“çš„è‚¡ç¥¨æ± 
        final_pool = aligned_universe & aligned_tradeable_mask

        # ç»Ÿè®¡è¿‡æ»¤åçš„æ•°é‡
        post_filter_count = final_pool.sum().sum()
        filtered_out_count = pre_filter_count - post_filter_count
        logger.info(f"      åœç‰Œè‚¡ç¥¨è¿‡æ»¤: å…±å‰”é™¤ {filtered_out_count:.0f} ä¸ªåœç‰Œçš„è‚¡ç¥¨-æ—¥æœŸå¯¹ã€‚")
        self.show_stock_nums_for_per_day('è¿‡æ»¤åœç‰Œè‚¡å', final_pool)
        return final_pool

    # ok
    def _filter_by_liquidity(self, stock_pool_df: pd.DataFrame, min_percentile: float) -> pd.DataFrame:
        """æŒ‰æµåŠ¨æ€§è¿‡æ»¤ """
        if 'turnover_rate' not in self.raw_dfs:
            raise RuntimeError("ç¼ºå°‘æ¢æ‰‹ç‡æ•°æ®ï¼Œæ— æ³•è¿›è¡ŒæµåŠ¨æ€§è¿‡æ»¤")

        turnover_df = self.raw_dfs['turnover_rate']
        turnover_df = turnover_df.shift(1)  # å–ç”¨çš„tæ—¥æ•°æ®ï¼Œå¿…é¡»å‰ç§»

        # 1. ã€ç¡®å®šæ ·æœ¬ã€‘åªä¿ç•™ stock_pool_df ä¸­ä¸º True çš„æ¢æ‰‹ç‡æ•°æ®
        # â€œåªå¯¹å½“å‰è‚¡ç¥¨æ± è®¡ç®—â€
        valid_turnover = turnover_df.where(stock_pool_df)

        # 2. ã€è®¡ç®—æ ‡å‡†ã€‘æ²¿è¡Œï¼ˆaxis=1ï¼‰ä¸€æ¬¡æ€§è®¡ç®—å‡ºæ¯æ—¥çš„åˆ†ä½æ•°é˜ˆå€¼
        thresholds = valid_turnover.quantile(min_percentile, axis=1)

        # 3. ã€åº”ç”¨æ ‡å‡†ã€‘å°†åŸå§‹æ¢æ‰‹ç‡ä¸æ¯æ—¥é˜ˆå€¼è¿›è¡Œæ¯”è¾ƒï¼Œç”Ÿæˆè¿‡æ»¤æ©ç 
        low_liquidity_mask = turnover_df.lt(thresholds, axis=0)

        # 4. å°†éœ€è¦å‰”é™¤çš„è‚¡ç¥¨åœ¨ stock_pool_df ä¸­è®¾ä¸º False
        stock_pool_df[low_liquidity_mask] = False
        self.show_stock_nums_for_per_day(f'by_å‰”é™¤æµåŠ¨æ€§ä½çš„_filter', stock_pool_df)

        return stock_pool_df

    # ok
    def _filter_by_market_cap(self,
                              stock_pool_df: pd.DataFrame,
                              min_percentile: float) -> pd.DataFrame:
        """
        æŒ‰å¸‚å€¼è¿‡æ»¤ -

        Args:
            stock_pool_df: åŠ¨æ€è‚¡ç¥¨æ± 
            min_percentile: å¸‚å€¼æœ€ä½ç™¾åˆ†ä½é˜ˆå€¼

        Returns:
            è¿‡æ»¤åçš„åŠ¨æ€è‚¡ç¥¨æ± 
        """
        if 'total_mv' not in self.raw_dfs:
            raise RuntimeError("ç¼ºå°‘å¸‚å€¼æ•°æ®ï¼Œæ— æ³•è¿›è¡Œå¸‚å€¼è¿‡æ»¤")

        mv_df = self.raw_dfs['total_mv']
        mv_df = mv_df.shift(1)

        # 1. ã€å±è”½ã€‘åªä¿ç•™åœ¨å½“å‰è‚¡ç¥¨æ± (stock_pool_df)ä¸­çš„è‚¡ç¥¨å¸‚å€¼ï¼Œå…¶ä½™è®¾ä¸ºNaN
        valid_mv = mv_df.where(stock_pool_df)

        # 2. ã€è®¡ç®—æ ‡å‡†ã€‘å‘é‡åŒ–è®¡ç®—æ¯æ—¥çš„å¸‚å€¼åˆ†ä½æ•°é˜ˆå€¼
        # axis=1 ç¡®ä¿äº†æˆ‘ä»¬æ˜¯æŒ‰è¡Œï¼ˆæ¯æ—¥ï¼‰è®¡ç®—åˆ†ä½æ•°
        thresholds = valid_mv.quantile(min_percentile, axis=1)

        # 3. ã€ç”Ÿæˆæ©ç ã€‘å°†åŸå§‹å¸‚å€¼ä¸æ¯æ—¥é˜ˆå€¼è¿›è¡Œæ¯”è¾ƒ
        # .lt() æ˜¯â€œå°äºâ€æ“ä½œï¼Œaxis=0 ç¡®ä¿äº† thresholds è¿™ä¸ªSeriesèƒ½æŒ‰è¡Œæ­£ç¡®åœ°å¹¿æ’­
        small_cap_mask = mv_df.lt(thresholds, axis=0)

        # 4. ã€åº”ç”¨è¿‡æ»¤ã€‘å°†æ‰€æœ‰å¸‚å€¼å°äºå½“æ—¥é˜ˆå€¼çš„è‚¡ç¥¨ï¼Œåœ¨è‚¡ç¥¨æ± ä¸­æ ‡è®°ä¸ºFalse
        # è¿™æ˜¯ä¸€ä¸ªè·¨è¶Šæ•´ä¸ªDataFrameçš„å¸ƒå°”è¿ç®—ï¼Œæå…¶é«˜æ•ˆ
        stock_pool_df[small_cap_mask] = False
        self.show_stock_nums_for_per_day(f'by_å‰”é™¤å¸‚å€¼ä½çš„_filter', stock_pool_df)

        return stock_pool_df

    # ok è¿™ä¸ªå±äºæ„ŸçŸ¥æœªæ¥ï¼Œç”¨ä¸å¾—ï¼
    def _filter_next_day_limit_up(self, stock_pool_df: pd.DataFrame) -> pd.DataFrame:
        """
         å‰”é™¤åœ¨Tæ—¥å¼€ç›˜å³ä¸€å­—æ¶¨åœçš„è‚¡ç¥¨ã€‚
        è¿™æ˜¯ä¸ºäº†æ¨¡æ‹ŸçœŸå®äº¤æ˜“çº¦æŸï¼Œå› ä¸ºè¿™ç±»è‚¡ç¥¨åœ¨å¼€ç›˜æ—¶æ— æ³•ä¹°å…¥ã€‚
        Args:
            stock_pool_df: åŠ¨æ€è‚¡ç¥¨æ± DataFrame (T-1æ—¥å†³ç­–ï¼Œç”¨äºTæ—¥)
        Returns:
            è¿‡æ»¤åçš„åŠ¨æ€è‚¡ç¥¨æ± DataFrame
        """
        logger.info("    åº”ç”¨æ¬¡æ—¥æ¶¨åœè‚¡ç¥¨è¿‡æ»¤...")

        # --- 1. æ•°æ®å‡†å¤‡ä¸éªŒè¯ ---
        required_data = ['open', 'high', 'low', 'pre_close']
        for data_key in required_data:
            if data_key not in self.raw_dfs:
                raise RuntimeError(f"ç¼ºå°‘è¡Œæƒ…æ•°æ® '{data_key}'ï¼Œæ— æ³•è¿‡æ»¤æ¬¡æ—¥æ¶¨åœè‚¡ç¥¨")

        open_df = self.raw_dfs['open']
        high_df = self.raw_dfs['high']
        low_df = self.raw_dfs['low']
        pre_close_df = self.raw_dfs['pre_close']  # Tæ—¥çš„pre_closeå°±æ˜¯T-1æ—¥çš„close

        # --- 2. å‘é‡åŒ–è®¡ç®—æ¯æ—¥æ¶¨åœä»· ---
        # a) åˆ›å»ºä¸€ä¸ªä¸pre_close_dfå½¢çŠ¶ç›¸åŒçš„ã€é»˜è®¤å€¼ä¸º1.1çš„æ¶¨è·Œå¹…é™åˆ¶çŸ©é˜µ
        limit_rate = pd.DataFrame(1.1, index=pre_close_df.index, columns=pre_close_df.columns)

        # b) è¯†åˆ«ç§‘åˆ›æ¿(688å¼€å¤´)å’Œåˆ›ä¸šæ¿(300å¼€å¤´)çš„è‚¡ç¥¨ï¼Œå°†å…¶æ¶¨è·Œå¹…é™åˆ¶è®¾ä¸º1.2
        star_market_stocks = [col for col in limit_rate.columns if str(col).startswith('688')]
        chinext_stocks = [col for col in limit_rate.columns if str(col).startswith('300')]
        limit_rate[star_market_stocks] = 1.2
        limit_rate[chinext_stocks] = 1.2

        # c) è®¡ç®—ç†è®ºæ¶¨åœä»· (è¿™é‡Œä¸éœ€è¦shiftï¼Œå› ä¸ºpre_closeå·²ç»æ˜¯T-1æ—¥çš„ä¿¡æ¯)
        limit_up_price = (pre_close_df * limit_rate).round(2)

        # --- 3. ç”Ÿæˆâ€œå¼€ç›˜å³æ¶¨åœâ€çš„å¸ƒå°”æ©ç  (Mask) ---
        # æ¡ä»¶1: Tæ—¥çš„å¼€ç›˜ä»·ã€æœ€é«˜ä»·ã€æœ€ä½ä»·ä¸‰è€…ç›¸ç­‰ (ä¸€å­—æ¿çš„ç‰¹å¾)
        is_one_word_board = (open_df == high_df) & (open_df == low_df)

        # æ¡ä»¶2: Tæ—¥çš„å¼€ç›˜ä»·å¤§äºæˆ–ç­‰äºç†è®ºæ¶¨åœä»·
        is_at_limit_price = open_df >= limit_up_price

        # æœ€ç»ˆçš„æ©ç ï¼šä¸¤ä¸ªæ¡ä»¶åŒæ—¶æ»¡è¶³
        limit_up_mask = is_one_word_board & is_at_limit_price

        # --- 4. åº”ç”¨è¿‡æ»¤ ---
        # å°†åœ¨Tæ—¥å¼€ç›˜å³æ¶¨åœçš„è‚¡ç¥¨ï¼Œåœ¨Tæ—¥çš„universeä¸­å‰”é™¤
        # è¿™ä¸ªæ“ä½œæ˜¯â€œæœªæ¥â€çš„ï¼Œä½†å®ƒæ˜¯è‰¯æ€§çš„ï¼Œå› ä¸ºå®ƒæ¨¡æ‹Ÿçš„æ˜¯â€œæ— æ³•äº¤æ˜“â€çš„ç°å®
        # å®ƒä¸éœ€è¦.shift(1)ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯æ‹¿Tæ—¥çš„çŠ¶æ€ï¼Œæ¥è¿‡æ»¤Tæ—¥çš„æ± å­
        stock_pool_df[limit_up_mask] = False

        self.show_stock_nums_for_per_day('è¿‡æ»¤æ¬¡æ—¥æ¶¨åœè‚¡å--final', stock_pool_df)
        return stock_pool_df

    # def _filter_next_day_suspended(self, stock_pool_df: pd.DataFrame) -> pd.DataFrame: #todo å®ç›˜çš„åŠ¨æ€è‚¡ç¥¨æ±  å¯èƒ½ä¼šç”¨åˆ°
    #     """
    #       å‰”é™¤æ¬¡æ—¥åœç‰Œè‚¡ç¥¨ -
    #
    #       Args:
    #           stock_pool_df: åŠ¨æ€è‚¡ç¥¨æ± DataFrame
    #
    #       Returns:
    #           è¿‡æ»¤åçš„åŠ¨æ€è‚¡ç¥¨æ± DataFrame
    #       """
    #     if 'close' not in self.raw_dfs:
    #         raise RuntimeError(" ç¼ºå°‘ä»·æ ¼æ•°æ®ï¼Œæ— æ³•è¿‡æ»¤æ¬¡æ—¥åœç‰Œè‚¡ç¥¨")
    #
    #     close_df = self.raw_dfs['close']
    #
    #     # 1. åˆ›å»ºä¸€ä¸ªä»£è¡¨â€œå½“æ—¥æœ‰ä»·æ ¼â€çš„å¸ƒå°”çŸ©é˜µ
    #     today_has_price = close_df.notna()
    #
    #     # 2. åˆ›å»ºä¸€ä¸ªä»£è¡¨â€œæ¬¡æ—¥æœ‰ä»·æ ¼â€çš„å¸ƒå°”çŸ©é˜µ
    #     #    shift(-1) å°† T+1 æ—¥çš„æ•°æ®ï¼Œç§»åŠ¨åˆ° T æ—¥çš„è¡Œã€‚è¿™å°±åœ¨ä¸€ç¬é—´å®Œæˆäº†æ‰€æœ‰â€œnext_dateâ€çš„æŸ¥æ‰¾
    #     #    fill_value=True ä¼˜é›…åœ°å¤„ç†äº†æœ€åä¸€å¤©ï¼Œæˆ‘ä»¬å‡è®¾æœ€åä¸€å¤©ä¹‹åä¸ä¼šåœç‰Œ
    #     tomorrow_has_price = close_df.notna().shift(-1, fill_value=True)
    #
    #     # 3. è®¡ç®—å‡ºæ‰€æœ‰â€œæ¬¡æ—¥åœç‰Œâ€çš„æ©ç  (Mask) ï¼ˆä¸ºä»€ä¹ˆè¦å‰”é™¤ï¼è´¨ç–‘è‡ªå·±ï¼šæ˜å¤©çš„äº‹æƒ…æˆ‘ä¸ºä»€ä¹ˆè¦ç®¡ï¼Ÿ ç­”ï¼šä½ ä¸æ€•æ˜å¤©åœç‰Œå–ä¸å‡ºå»ï¼Ÿ  !!!!ç³Ÿç³•ï¼ï¼Œæ˜å¤©çš„äº‹æƒ…ä½ ä»Šå¤©æ— æ³•æ„ŸçŸ¥å•Šï¼Œè¿™ä¸ªå‡½æ•°å¿…é¡»åˆ é™¤
    #     #    æ¬¡æ—¥åœç‰Œ = ä»Šæ—¥æœ‰ä»· & æ˜æ—¥æ— ä»·
    #     next_day_suspended_mask = today_has_price & (~tomorrow_has_price)
    #
    #     # 4. ä¸€æ¬¡æ€§ä»è‚¡ç¥¨æ± ä¸­å‰”é™¤æ‰€æœ‰è¢«æ ‡è®°çš„è‚¡ç¥¨
    #     #    è¿™ä¸ªå¸ƒå°”è¿ç®—ä¼šè‡ªåŠ¨æŒ‰ç´¢å¼•å¯¹é½ï¼Œåº”ç”¨åˆ°æ•´ä¸ªDataFrame
    #     stock_pool_df[next_day_suspended_mask] = False
    #
    #     return stock_pool_df

    def _load_dynamic_index_components(self, index_code: str,
                                       start_date: str, end_date: str) -> pd.DataFrame:
        """åŠ è½½åŠ¨æ€æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®"""
        # print(f"    åŠ è½½ {index_code} åŠ¨æ€æˆåˆ†è‚¡æ•°æ®...")

        index_file_name = index_code.replace('.', '_')
        index_data_path = LOCAL_PARQUET_DATA_DIR / 'index_weights' / index_file_name

        if not index_data_path.exists():
            raise ValueError(f"æœªæ‰¾åˆ°æŒ‡æ•° {index_code} çš„æˆåˆ†è‚¡æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œdownloaderä¸‹è½½")

        # ç›´æ¥è¯»å–åˆ†åŒºæ•°æ®ï¼Œpandasä¼šè‡ªåŠ¨åˆå¹¶æ‰€æœ‰year=*åˆ†åŒº
        components_df = pd.read_parquet(index_data_path)
        components_df['trade_date'] = pd.to_datetime(components_df['trade_date'])

        # æ—¶é—´èŒƒå›´è¿‡æ»¤
        # å¤§å‘å•Š ï¼Œstart_dateå¿…é¡»æå‰6ä¸ªæœˆï¼ï¼ï¼  ä¸¤æ¡æ•°æ®æ—¶é—´è·¨åº¦é—´éš”ï¼ˆæ–°è€æ•°æ®é—´éš”æœ€é•¿å¯è¾¾6ä¸ªæœˆï¼ï¼‰ã€‚åé¢é€æ—¥å¡«å……æˆåˆ†è‚¡ä¿¡æ¯ï¼šåŸç†å°±æ˜¯å–ä¸Šæ¬¡æ•°æ®è¿›è¡Œå¡«å……çš„ï¼
        extended_start_date = pd.Timestamp(start_date) - pd.DateOffset(months=6)
        mask = (components_df['trade_date'] >= extended_start_date) & \
               (components_df['trade_date'] <= pd.Timestamp(end_date))
        components_df = components_df[mask]

        # print(f"    æˆåŠŸåŠ è½½ç¬¦åˆå½“å‰å›æµ‹æ—¶é—´æ®µï¼š {len(components_df)} æ¡æˆåˆ†è‚¡è®°å½•")
        return components_df

    # ok å·²ç»è§£å†³å‰è§†åå·® åœ¨äºï¼šavailable_components = components_df[components_df['trade_date'] < date]
    def _build_dynamic_index_universe(self, stock_pool_df, index_code: str) -> pd.DataFrame:
        """æ„å»ºåŠ¨æ€æŒ‡æ•°è‚¡ç¥¨æ±  """
        start_date = self.config['backtest']['start_date']
        end_date = self.config['backtest']['end_date']

        # åŠ è½½åŠ¨æ€æˆåˆ†è‚¡æ•°æ®
        components_df = self._load_dynamic_index_components(index_code, start_date, end_date)
        # ç¡®ä¿ components_df ä¸­çš„ trade_date æ˜¯ datetime ç±»å‹ï¼Œä»¥ä¾¿æ¯”è¾ƒ
        components_df['trade_date'] = pd.to_datetime(components_df['trade_date'])

        # è·å–äº¤æ˜“æ—¥åºåˆ—
        trading_dates = self.data_loader.get_trading_dates(start_date, end_date)
        index_stock_pool_df = stock_pool_df.copy()

        #  å¡«å…… ---
        for date in trading_dates:
            if date not in index_stock_pool_df.index:
                continue

            # 1. ã€å®‰å…¨æ¸¯æŸ¥è¯¢ã€‘æŸ¥æ‰¾æ‰€æœ‰åœ¨Tæ—¥ä¹‹å‰ï¼ˆä¸å«Tæ—¥ï¼‰å·²ç»å…¬å¸ƒçš„æˆåˆ†è‚¡åˆ—è¡¨
            #    è¿™æ˜¯ä¸ºäº†ç¡®ä¿æˆ‘ä»¬åªä½¿ç”¨ T-1 åŠæ›´æ—©çš„ä¿¡æ¯
            available_components = components_df[components_df['trade_date'] < date]

            # å¦‚æœå†å²ä¸Šæ²¡æœ‰ä»»ä½•æˆåˆ†è‚¡ä¿¡æ¯ï¼Œåˆ™å½“å¤©è‚¡ç¥¨æ± ä¸ºç©º
            if available_components.empty:
                index_stock_pool_df.loc[date, :] = False
                continue

            # 2. ä»è¿™äº›å¯ç”¨çš„å†å²åˆ—è¡¨ä¸­ï¼Œæ‰¾åˆ°æœ€è¿‘çš„ä¸€æ¬¡å‘å¸ƒçš„æ—¥æœŸ
            latest_available_date = available_components['trade_date'].max()

            # 3. è·å–è¿™ä»½æœ€æ–°çš„ã€åˆæ³•çš„æˆåˆ†è‚¡åˆ—è¡¨
            daily_components = components_df[
                components_df['trade_date'] == latest_available_date
                ]['con_code'].tolist()

            # --- åç»­é€»è¾‘ä¸ä½ åŸå…ˆçš„ç›¸åŒï¼Œå®ƒä»¬æ˜¯æ­£ç¡®çš„ ---

            # a) è·å–å½“å‰åŸºç¡€è‚¡ç¥¨æ± å’Œæˆåˆ†è‚¡çš„äº¤é›†
            valid_stocks = index_stock_pool_df.columns.intersection(daily_components)

            # b) æ¸…ç†å¹¶å¡«å……å½“æ—¥è‚¡ç¥¨æ± 
            current_universe = index_stock_pool_df.loc[date].copy()
            index_stock_pool_df.loc[date, :] = False

            # c) valid_stocks æ˜¯è‚¡ç¥¨æ± æ‰€æœ‰ ä¸å½“å¤©æˆåˆ†è‚¡çš„å¹¶é›†ï¼Œç°åœ¨ç»†çœ‹åˆ°æ¯ä¸€å¤©çš„è‚¡ç¥¨æ± ï¼Œå¦‚æœè‚¡ç¥¨æ± ï¼šä¹Ÿæ˜¯trueï¼šif current_universe[stock] åˆ™è§†ä¸ºå½“å¤©å¯ åŠ å…¥åˆ°final_valid_stocks
            final_valid_stocks = [stock for stock in valid_stocks if current_universe[stock]]
            index_stock_pool_df.loc[date, final_valid_stocks] = True

        self.show_stock_nums_for_per_day(f'by_æˆåˆ†è‚¡æŒ‡æ•°_filter', index_stock_pool_df)
        return index_stock_pool_df

    def get_factor_data(self) -> pd.DataFrame:
        """
        è®¡ç®—ç›®æ ‡å› å­æ•°æ®

        Returns:
            å› å­æ•°æ®DataFrame
        """
        target_factors_for_evaluation = self.config['target_factors_for_evaluation']
        factor_name = target_factors_for_evaluation['name']
        fields = target_factors_for_evaluation['fields']

        print(f"\nè®¡ç®—ç›®æ ‡å› å­: {factor_name}")

        # ä½¿ç”¨å¤„ç†åçš„æ•°æ®
        data_source = getattr(self, 'processed_data', self.raw_dfs)

        # ç®€å•çš„å› å­è®¡ç®—é€»è¾‘
        if factor_name == 'pe_inv' and 'pe_ttm' in fields:
            # PEå€’æ•°å› å­
            pe_data = data_source['pe_ttm']
            factor_data = 1 / pe_data
            factor_data = factor_data.replace([np.inf, -np.inf], np.nan)
        else:
            # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªå­—æ®µ
            factor_data = data_source[fields[0]]

        return factor_data

    def get_universe(self) -> pd.DataFrame:
        """è·å–è‚¡ç¥¨æ± """
        return self.stock_pool_df

    def get_price_data(self) -> pd.DataFrame:
        """è·å–ä»·æ ¼æ•°æ®"""
        return self.raw_dfs['close']

    def get_namechange_data(self) -> pd.DataFrame:
        """è·å–nameæ”¹å˜çš„æ•°æ®"""
        namechange_path = LOCAL_PARQUET_DATA_DIR / 'namechange.parquet'

        return pd.read_parquet(namechange_path)

    def save_data_summary(self, output_dir: str):
        """ä¿å­˜æ•°æ®æ‘˜è¦"""
        os.makedirs(output_dir, exist_ok=True)

        # ä¿å­˜è‚¡ç¥¨æ± ç»Ÿè®¡
        universe_stats = {
            'daily_count': self.stock_pool_df.sum(axis=1),
            'stock_coverage': self.stock_pool_df.sum(axis=0)
        }

        summary_path = os.path.join(output_dir, 'data_summary.xlsx')
        with pd.ExcelWriter(summary_path) as writer:
            # æ¯æ—¥è‚¡ç¥¨æ•°ç»Ÿè®¡
            universe_stats['daily_count'].to_frame('stock_count').to_excel(
                writer, sheet_name='daily_stock_count'
            )

            # è‚¡ç¥¨è¦†ç›–ç»Ÿè®¡
            universe_stats['stock_coverage'].to_frame('coverage_days').to_excel(
                writer, sheet_name='stock_coverage'
            )

            # æ•°æ®è´¨é‡æŠ¥å‘Š
            quality_report = []
            for field_name, df in self.raw_dfs.items():
                quality_report.append({
                    'field': field_name,
                    'shape': f"{df.shape[0]}x{df.shape[1]}",
                    'missing_ratio': f"{df.isnull().sum().sum() / (df.shape[0] * df.shape[1]):.2%}",
                    'valid_ratio': f"{df.notna().sum().sum() / (df.shape[0] * df.shape[1]):.2%}"
                })

            pd.DataFrame(quality_report).to_excel(
                writer, sheet_name='data_quality', index=False
            )

        print(f"æ•°æ®æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_path}")

    def show_stock_nums_for_per_day(self, describe_text, index_stock_pool_df):
        daily_count = index_stock_pool_df.sum(axis=1)
        logger.info(f"    {describe_text}åŠ¨æ€è‚¡ç¥¨æ± :")
        logger.info(f"      å¹³å‡æ¯æ—¥è‚¡ç¥¨æ•°: {daily_count.mean():.0f}")
        logger.info(f"      æœ€å°‘æ¯æ—¥è‚¡ç¥¨æ•°: {daily_count.min():.0f}")
        logger.info(f"      æœ€å¤šæ¯æ—¥è‚¡ç¥¨æ•°: {daily_count.max():.0f}")

    # è¾“å…¥å­¦æœ¯å› å­ï¼Œè¿”å›è®¡ç®—æ‰€å¿…é¡»çš„base å› å­
    def get_cal_base_factors(self, target_factors: list[str]) -> set:
        factor_definition_df = pd.DataFrame(self.config['factor_definition'])  # å°† list[dict] è½¬ä¸º DataFrame
        result = set()

        for target_factors_for_evaluation in target_factors:
            factor_definition = factor_definition_df[factor_definition_df['name'] == target_factors_for_evaluation]
            if not factor_definition.empty and factor_definition['cal_require_base_fields_from_daily'].iloc[0]:
                base_fields = factor_definition.iloc[0]['cal_require_base_fields']
                result.update(base_fields)  # ç”¨ update åˆå¹¶åˆ—è¡¨åˆ° set

        return result

    # ok #ok
    def create_stock_pool(self, stock_pool_config_profile, pool_name):
        """
                æ„å»ºåŠ¨æ€è‚¡ç¥¨æ± 
                Returns:
                    è‚¡ç¥¨æ± DataFrameï¼ŒTrueè¡¨ç¤ºè¯¥è‚¡ç¥¨åœ¨è¯¥æ—¥æœŸå¯ç”¨
                """
        logger.info(f"  æ„å»º{pool_name}åŠ¨æ€è‚¡ç¥¨æ± ...")
        # ç¬¬ä¸€æ­¥ï¼šåŸºç¡€è‚¡ç¥¨æ±  - æœ‰ä»·æ ¼æ•°æ®çš„è‚¡ç¥¨
        if 'close' not in self.raw_dfs:
            raise ValueError("ç¼ºå°‘ä»·æ ¼æ•°æ®ï¼Œæ— æ³•æ„å»ºè‚¡ç¥¨æ± ")

        final_stock_pool_df = self.raw_dfs['close'].notna()  # close æœ‰å€¼çš„åœ°æ–¹ ï¼štrue
        self.show_stock_nums_for_per_day('æ ¹æ®æ”¶ç›˜ä»·notnaç”Ÿæˆçš„', final_stock_pool_df)
        # ã€ç¬¬ä¸€é“é˜²çº¿ï¼šå­˜åœ¨æ€§è¿‡æ»¤ - å¿…é¡»ç½®äºæœ€å‰ï¼ã€‘
        # -------------------------------------------------------------------------
        if stock_pool_config_profile.get('remove_not_existence', True):
            final_stock_pool_df = self._filter_by_existence(final_stock_pool_df)
        # ç¬¬äºŒæ­¥ï¼šå„ç§è¿‡æ»¤ï¼
        # --åŸºç¡€è¿‡æ»¤ æŒ‡æ•°æˆåˆ†è‚¡è¿‡æ»¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        index_config = stock_pool_config_profile.get('index_filter', {})
        if index_config.get('enable', False):
            # print(f"    åº”ç”¨æŒ‡æ•°è¿‡æ»¤: {index_config['index_code']}")
            final_stock_pool_df = self._build_dynamic_index_universe(final_stock_pool_df, index_config['index_code'])
            # âœ… åœ¨è¿™é‡Œè¿›è¡Œåˆ—ä¿®å‰ªæ˜¯åˆç†çš„ï¼ å› ä¸ºä¸­è¯800æˆåˆ†è‚¡æ˜¯åŸºäºå¤–éƒ¨è§„åˆ™ï¼Œä¸æ˜¯åŸºäºæœªæ¥æ•°æ®è¡¨ç°
            valid_stocks = final_stock_pool_df.columns[final_stock_pool_df.any(axis=0)]
            final_stock_pool_df = final_stock_pool_df[valid_stocks]
        # å…¶ä»–å„ç§æŒ‡æ ‡è¿‡æ»¤æ¡ä»¶
        universe_filters = stock_pool_config_profile['filters']

        # --æ™®é€‚æ€§ è¿‡æ»¤ ï¼ˆé€šç”¨è¿‡æ»¤ï¼‰
        if universe_filters['remove_new_stocks']:
            final_stock_pool_df = self._filter_new_stocks(final_stock_pool_df, 6)  # æ–°è‚¡ç¥¨æ•°æ®å°‘ï¼Œæ•°æ®ä¸å…¨ä¸å…·å‚è€ƒï¼Œæ‰€ä»¥æ·˜æ±°
        if universe_filters['remove_st']:
            # æ„å»ºSTçŸ©é˜µ
            self.build_st_period_from_namechange()
            final_stock_pool_df = self._filter_st_stocks(final_stock_pool_df)  # å‰”é™¤STè‚¡ç¥¨
        if universe_filters['adapt_tradeable_matrix_by_suspend_resume']:
            # åŸºäºåœå¤ç‰Œäº‹ä»¶æ„å»ºçš„å¯äº¤æ˜“çš„æ± å­
            self.build_tradeable_matrix_by_suspend_resume()
            final_stock_pool_df = self._filter_tradeable_matrix_by_suspend_resume(final_stock_pool_df)

        # 2. æµåŠ¨æ€§è¿‡æ»¤
        if universe_filters.get('min_liquidity_percentile', 0) > 0:
            # print("    åº”ç”¨æµåŠ¨æ€§è¿‡æ»¤...")
            final_stock_pool_df = self._filter_by_liquidity(
                final_stock_pool_df,
                universe_filters['min_liquidity_percentile']
            )

        # 3. å¸‚å€¼è¿‡æ»¤
        if universe_filters.get('min_market_cap_percentile', 0) > 0:
            # print("    åº”ç”¨å¸‚å€¼è¿‡æ»¤...")
            final_stock_pool_df = self._filter_by_market_cap(
                final_stock_pool_df,
                universe_filters['min_market_cap_percentile']
            )

        return final_stock_pool_df

    def get_which_field_of_factor_definition_by_factor_name(self, factor_name, which_field):
        cur_factor_definition = self.get_factor_definition(factor_name)
        return cur_factor_definition[which_field]

    def get_factor_definition_df(self):
        return pd.DataFrame(self.config['factor_definition'])

    def get_factor_definition(self, factor_name):
        all_df = self.get_factor_definition_df()
        return all_df[all_df['name'] == factor_name]


def align_one_df_by_stock_pool_and_fill(factor_name=None, df=None,
                                        stock_pool_df: pd.DataFrame = None,
                                        _existence_matrix: pd.DataFrame = None):#è¿™ä¸ªåªæ˜¯ç”¨äºå¡«å……pct_chgè¿™ç±»æ•°æ®
    if stock_pool_df is None or stock_pool_df.empty:
        raise ValueError("stock_pool_df å¿…é¡»ä¼ å…¥ä¸”ä¸èƒ½ä¸ºç©ºçš„ DataFrame")
    # å®šä¹‰ä¸åŒç±»å‹æ•°æ®çš„å¡«å……ç­–ç•¥

    df = df.copy(deep=True)

    # æ­¥éª¤1: å¯¹é½åˆ°ä¿®å‰ªåçš„è‚¡ç¥¨æ±  å¯¹é½åˆ°ä¸»æ¨¡æ¿ï¼ˆstock_pool_dfçš„å½¢çŠ¶ï¼‰
    aligned_df = df.reindex(index=stock_pool_df.index, columns=stock_pool_df.columns)
    aligned_df = aligned_df.sort_index()
    aligned_df = aligned_df.where(stock_pool_df)

    # æ­¥éª¤2: æ ¹æ®é…ç½®å­—å…¸ï¼Œåº”ç”¨å¡«å……ç­–ç•¥
    # =================================================================
    strategy = FACTOR_FILL_CONFIG.get(factor_name)

    if strategy is None:
        raise KeyError(f"å› å­ '{factor_name}' çš„å¡«å……ç­–ç•¥æœªåœ¨ FACTOR_FILL_CONFIG ä¸­å®šä¹‰ï¼è¯·æ·»åŠ ã€‚")

    logger.info(f"  > æ­£åœ¨å¯¹å› å­ '{factor_name}' åº”ç”¨ '{strategy}' å¡«å……ç­–ç•¥...")

    if strategy == FILL_STRATEGY_FFILL_UNLIMITED:
        # å‰å‘å¡«å……ï¼šé€‚ç”¨äºä»·æ ¼ã€å¸‚å€¼ã€ä¼°å€¼ã€è¡Œä¸šç­‰
        # è¿™äº›å€¼åœ¨è‚¡ç¥¨ä¸äº¤æ˜“æ—¶ï¼Œåº”ä¿æŒå…¶æœ€åä¸€ä¸ªå·²çŸ¥å€¼
        return aligned_df.ffill()

    elif strategy == FILL_STRATEGY_CONDITIONAL_ZERO:
        # å¡«å……ä¸º0ï¼šé€‚ç”¨äºæˆäº¤é‡ã€æ¢æ‰‹ç‡ç­‰äº¤æ˜“è¡Œä¸ºæ•°æ®
        # ä¸äº¤æ˜“çš„æ—¥å­ï¼Œè¿™äº›æŒ‡æ ‡çš„çœŸå®å€¼å°±æ˜¯0
        if _existence_matrix is not None:
            return aligned_df.where(_existence_matrix, 0)  # æ•°æ®ä¸ºnanï¼Œä½†æ˜¯ä¸€çœ‹ æ˜¯ä¸å¯äº¤æ˜“çš„ï¼ˆåœç‰Œï¼‰ï¼Œåœç‰Œå¯¼è‡´çš„ æˆ‘è®¤ä¸ºå¯å¡«0
        return aligned_df  # ä¸å¡«å……~
    elif strategy == FILL_STRATEGY_FFILL_LIMIT_5:
        return aligned_df.ffill(limit=5)
    elif strategy == FILL_STRATEGY_FFILL_LIMIT_65:
        return aligned_df.ffill(limit=65)

    elif strategy == FILL_STRATEGY_NONE:
        # ä¸å¡«å……ï¼šé€‚ç”¨äºè®¡ç®—å‡ºçš„æŠ€æœ¯å› å­
        # å¦‚æœå› å­å› ä¸ºæ•°æ®ä¸è¶³è€Œæ— æ³•è®¡ç®—ï¼Œå°±ä¸åº”å‡­ç©ºåˆ›é€ å®ƒçš„å€¼
        return aligned_df

    raise RuntimeError(f"æ­¤å› å­{factor_name}æ²¡æœ‰æŒ‡æ˜é¢‘ç‡ï¼Œæ— æ³•è¿›è¡Œå¡«å……")


def create_data_manager(config_path: str) -> DataManager:
    """
    åˆ›å»ºæ•°æ®ç®¡ç†å™¨å®ä¾‹
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        DataManagerå®ä¾‹
    """
    return DataManager(config_path)

# if __name__ == '__main__':
#     # dataManager_temp = DataManager(
#     #     "../factory/config.yaml",
#     #     need_data_deal=False
#     # )
#     #
#     # calculate_rolling_beta(
#     #     dataManager_temp.config['backtest']['start_date'],
#     #     dataManager_temp.config['backtest']['end_date'],
#     #     dataManager_temp.get_pool_of_factor_name_of_stock_codes('beta')
#     # )
