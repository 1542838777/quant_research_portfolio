"""
æ•°æ®ç®¡ç†å™¨ - å•å› å­æµ‹è¯•ç»ˆæä½œæˆ˜æ‰‹å†Œ
ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®åŠ è½½ä¸è‚¡ç¥¨æ± æ„å»º

å®ç°é…ç½®é©±åŠ¨çš„æ•°æ®åŠ è½½å’ŒåŠ¨æ€è‚¡ç¥¨æ± æ„å»ºåŠŸèƒ½
"""

import pandas as pd
import numpy as np
import yaml
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import warnings
import sys
import os

from pandas import DatetimeIndex

from data.local_data_load import load_index_daily
from data.namechange_date_manager import fill_end_date_field
from quant_lib.data_loader import DataLoader

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from quant_lib.config.constant_config import LOCAL_PARQUET_DATA_DIR
from quant_lib.config.logger_config import setup_logger

warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logger = setup_logger(__name__)


def check_field_level_completeness(raw_df: Dict[str, pd.DataFrame]):
    dfs = raw_df.copy()
    for item_name, df in dfs.items():
        logger.info("åŸå§‹å­—æ®µç¼ºå¤±ç‡ä½“æ£€æŠ¥å‘Š:")
        # missing_rate_daily = df.isna().mean(axis=1)

        # logger.info(f"{item_name}å› å­ç¼ºå¤±ç‡æœ€é«˜çš„10å¤© between {first_date} and {end_date}")
        # logger.info(f"{missing_rate_daily.sort_values(ascending=False).head(10)}")  # å…¶å®ä¹Ÿä¸éœ€è¦å¤ªçœ‹é‡ï¼Œåªèƒ½è¯´æ˜¯è¾…åŠ©æ—¥å¿—ï¼Œå¦‚æœæ€»ç¼ºå¤±ç‡é«˜ å¯ä»¥çœ‹çœ‹æ•´ä¸ªè¾…åŠ©æ’æŸ¥è€Œå·²ï¼

        # è®¡ç®—æ¯åªè‚¡ç¥¨ï¼ˆæ¯ä¸€åˆ—ï¼‰çš„ç¼ºå¤±ç‡(ç›¸å½“äºçœ‹è¿™è‚¡ç¥¨ åœ¨è¿™ä¸€æ®µæ—¶é—´çš„å®Œæ•´ç‡ï¼---ã€‹æ¨å¯¼ï¼šæœ€åä¸€å¤©æ‰ä¸Šå¸‚ï¼ï¼Œé‚£ä¹ˆç¼ºå¤±ç‡å¯èƒ½é«˜è¾¾99.99% æ‰€ä»¥ä¸éœ€è¦çœ‹é‡è¿™ä¸ªï¼)  æ³¨é‡Šæ‰
        missing_rate_per_stock = df.isna().mean(axis=0)
        #
        # logger.info(f"{item_name}ï¼ˆä¸æ˜¯å¾ˆé‡è¦ï¼‰å› å­ç¼ºå¤±ç‡æœ€é«˜çš„10åªè‚¡ç¥¨ between {first_date} and {end_date}")
        # logger.info(f"{missing_rate_per_stock.sort_values(ascending=False).head(10)}")

        # è®¡ç®—æ•´ä¸ªDataFrameçš„ç¼ºå¤±ç‡
        total_cells = df.size
        df_all_cells = df.isna().sum().sum()
        global_na_ratio = df_all_cells / total_cells
        logger.info(_get_nan_comment(item_name, global_na_ratio))


def _get_nan_comment(field: str, rate: float) -> str:
    logger.info(f"fieldï¼š{field}åœ¨åŸå§‹raw_df ç¡®å®å æ¯”ä¸ºï¼š{rate}")
    if rate >= 0.5:
        raise f"field:{field}ç¼ºå¤±ç‡è¶…è¿‡50% å¿…é¡»æ£€æŸ¥"
    """æ ¹æ®å­—æ®µåç§°å’Œç¼ºå¤±ç‡ï¼Œæä¾›ä¸“å®¶è¯Šæ–­æ„è§"""
    if field in ['pe_ttm', 'pe', 'pb',
                 'pb_ttm'] and rate <= 0.4:  # äº²æµ‹ å¾ˆæ­£å¸¸ï¼Œæœ‰çš„åƒåœ¾è‚¡ç¥¨ price earning ä¸ºè´Ÿã€‚é‚£ä¹ˆtushareç»™æˆ‘çš„æ•°æ®å°±ç®—nanï¼Œåˆç†ï¼
        return " (æ­£å¸¸ç°è±¡: ä¸»è¦ä»£è¡¨å…¬å¸äºæŸ)"

    if field in ['dv_ttm', 'dv_ratio']:
        return " (æ­£å¸¸ç°è±¡: ä¸»è¦ä»£è¡¨å…¬å¸ä¸åˆ†çº¢, åç»­åº”å¡«å……ä¸º0)"

    if field in ['industry']:  # äº²æµ‹ industry å¯ä»¥ç›´æ¥æ”¾è¡Œï¼Œä¸éœ€è¦care å¤šå°‘ç¼ºå¤±ç‡ï¼å› ä¸ºä¹Ÿå°±300ä¸ªï¼Œè€Œä¸”å…¨æ˜¯é€€å¸‚çš„ï¼Œ
        return "æ­£å¸¸ç°è±¡ï¼šä¸éœ€è¦care å¤šå°‘ç¼ºå¤±ç‡"
    if field in ['circ_mv', 'close', 'total_mv',
                 'turnover_rate', 'open', 'high', 'low',
                 'pre_close'] and rate < 0.2:  # äº²æµ‹ ä¸€å¤§æ®µæ—¶é—´ï¼Œå¯èƒ½æœ‰çš„è‚¡ç¥¨æœ€åä¸€ä¸ªæœˆæ‰ä¸Šå¸‚ï¼Œå¯¼è‡´å‰é¢ç©ºç¼ºï¼Œæœ‰ç¼ºå¤± é‚£å¾ˆæ­£å¸¸ï¼
        return "æ­£å¸¸ç°è±¡ï¼šä¸éœ€è¦care å¤šå°‘ç¼ºå¤±ç‡"
    if field in ['list_date'] and rate <= 0.01:
        return "æ­£å¸¸ç°è±¡ï¼šä¸éœ€è¦care å¤šå°‘ç¼ºå¤±ç‡"
    if field in ['pct_chg'] and rate <= 0.10:
        return  "æ­£å¸¸"
    raise ValueError(f"(ğŸš¨ è­¦å‘Š: æ­¤å­—æ®µ{field}ç¼ºå¤±ratio:{rate}!) è¯·è‡ªè¡Œé…ç½®é€šè¿‡ratio æˆ–åˆ™æ˜¯ç¼ºå¤±ç‡å¤ªé«˜ï¼")


class DataManager:
    """
    æ•°æ®ç®¡ç†å™¨ - è´Ÿè´£æ•°æ®åŠ è½½å’Œè‚¡ç¥¨æ± æ„å»º
    
    æŒ‰ç…§é…ç½®æ–‡ä»¶çš„è¦æ±‚ï¼Œå®ç°ï¼š
    1. åŸå§‹æ•°æ®åŠ è½½
    2. åŠ¨æ€è‚¡ç¥¨æ± æ„å»º
    3. æ•°æ®è´¨é‡æ£€æŸ¥
    4. æ•°æ®å¯¹é½å’Œé¢„å¤„ç†
    """

    def __init__(self, config_path: str, need_data_deal: bool = True):
        """
        åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.st_matrix = None
        self.config = self._load_config(config_path)
        if need_data_deal:
            self.data_loader = DataLoader(data_path=LOCAL_PARQUET_DATA_DIR)
            self.raw_dfs = {}
            self.stock_pools_dict = None

    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config

    def processed_raw_data_dict_by_stock_pool_(self) -> Dict[str, pd.DataFrame]:
        """
        ä¼˜åŒ–çš„ä¸¤é˜¶æ®µæ•°æ®å¤„ç†æµæ°´çº¿ï¼ˆåªåŠ è½½ä¸€æ¬¡æ•°æ®ï¼‰

        Returns:
            å¤„ç†åçš„æ•°æ®å­—å…¸
        """

        # è·å–æ—¶é—´èŒƒå›´
        start_date = self.config['backtest']['start_date']
        end_date = self.config['backtest']['end_date']

        # ç¡®å®šæ‰€æœ‰éœ€è¦çš„å­—æ®µï¼ˆä¸€æ¬¡æ€§ç¡®å®šï¼‰
        all_required_fields = self._get_required_fields()

        # === ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰rawæ•°æ®(äº’ç›¸å¯¹é½) ===

        self.raw_dfs = self.data_loader.get_raw_dfs_by_require_fields(fields=all_required_fields,
                                                                      start_date=start_date, end_date=end_date)

        check_field_level_completeness(self.raw_dfs)
        logger.info(f"raw_dfsåŠ è½½å®Œæˆï¼Œå…±åŠ è½½ {len(self.raw_dfs)} ä¸ªå­—æ®µ")

        # === ç¬¬ä¸€é˜¶æ®µï¼šåŸºäºå·²åŠ è½½æ•°æ®æ„å»ºæƒå¨è‚¡ç¥¨æ±  ===
        logger.info("ç¬¬ä¸€é˜¶æ®µï¼šæ„å»ºä¸¤ä¸ªæƒå¨è‚¡ç¥¨æ± ï¼ˆå„ç§è¿‡æ»¤ï¼ï¼‰")
        self._build_stock_pools_from_loaded_data(start_date, end_date)

        # === ç¬¬äºŒé˜¶æ®µï¼šåŸºäºè‚¡ç¥¨æ± å¯¹é½å’Œæ¸…æ´—æ‰€æœ‰æ•°æ® ===
        logger.info("ç¬¬äºŒé˜¶æ®µï¼š(æ ¹æ®å› å­é—¨æ´¾ç±»åˆ«)å¯¹é½å’Œå¡«å……æ‰€æœ‰å› å­æ•°æ®")

        # ä½¿ç”¨æƒå¨è‚¡ç¥¨æ± å¯¹é½å’Œå¡«å……æ•°æ®
        self.processed_raw_data = self._align_many_raw_dfs_by_stock_pool_and_fill(self.raw_dfs)
        # å¼ºè¡Œæ£€æŸ¥ä¸€ä¸‹æ•°æ®ï¼å®Œæ•´ç‡ï¼ ä¸åº”è¯¥åœ¨è¿™é‡Œæ£€æŸ¥ï¼ï¼Œå¤ªæ™šäº†ï¼Œ å·²ç»è¢«stock_pool_df åŠ¨äº†æ‰‹è„šäº†ï¼ˆä½å¸‚å€¼çš„ä¼šè¢«ç½®ä¸ºnanï¼Œ

        return self.processed_raw_data

    # ok
    def _build_stock_pools_from_loaded_data(self, start_date: str, end_date: str) -> pd.DataFrame:
        """
        ç¬¬ä¸€é˜¶æ®µï¼šåŸºäºå·²åŠ è½½çš„æ•°æ®æ„å»ºæƒå¨è‚¡ç¥¨æ± 

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            æƒå¨è‚¡ç¥¨æ± DataFrame
        """
        # print("1. éªŒè¯è‚¡ç¥¨æ± æ„å»ºæ‰€éœ€æ•°æ®...")

        # éªŒè¯å¿…éœ€å­—æ®µæ˜¯å¦å·²åŠ è½½
        required_fields_for_universe = ['close', 'total_mv', 'turnover_rate', 'industry', 'list_date']
        missing_fields = [field for field in required_fields_for_universe if field not in self.raw_dfs]

        if missing_fields:
            raise ValueError(f"æ„å»ºè‚¡ç¥¨æ± ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")

        # è·å–æ‰€æœ‰è‚¡ç¥¨å’Œäº¤æ˜“æ—¥æœŸ
        ts_codes = list(set(self.get_price_data().columns))
        trading_dates = self.data_loader.get_trading_dates(start_date=start_date, end_date=end_date)

        # æ„å»ºSTçŸ©é˜µ
        self.build_st_period_from_namechange(ts_codes, self.get_namechange_data(), trading_dates)
        self.build_diff_stock_pools()

    def build_diff_stock_pools(self) -> pd.DataFrame:
        stock_pool_df_dict = {}
        stock_pool_profiles = self.config['stock_pool_profiles']
        for universe_profile in stock_pool_profiles:
            pool_name = next(iter(universe_profile))
            product_universe = self.product_stock_pool(universe_profile, pool_name)
            stock_pool_df_dict[pool_name] = product_universe
        self.stock_pools_dict = stock_pool_df_dict

    # institutional_profile   = stock_pool_profiles['institutional_profile']#ä¸ºâ€œåŸºæœ¬é¢æ´¾â€å’Œâ€œè¶‹åŠ¿æ´¾â€å› å­ï¼Œæä¾›ä¸€ä¸ªé«˜å¸‚å€¼ã€é«˜æµåŠ¨æ€§çš„ç¯å¢ƒ
    # microstructure_profile = stock_pool_profiles['microstructure_profile']#ç”¨äº å¾®è§‚ï¼ˆé‡ä»·/æƒ…ç»ªï¼‰å› å­
    # product_universe =self.product_universe (microstructure_profile,trading_dates)

    # å¯¹äº æ˜¯å…ˆ fill è¿˜æ˜¯å…ˆwhere çš„è€ƒé‡ ï¼šè¿˜æ˜¯åˆ«å…ˆffilläº†ï¼šæç«¯ä¾‹å­ï¼šåœç‰Œäº†99å¤©çš„ï¼Œ100ã€‚ è‹¥å…ˆffillé‚£ä¹ˆ è¿™100å¤©éƒ½æ˜¯å€Ÿæ¥çš„æ•°æ®ï¼  å¦‚æœå…ˆwhereã€‚é‚£ä¹ˆç›´æ¥ç»Ÿç»Ÿnanäº†ã€‚åœ¨ffillä¹Ÿæ˜¯nanï¼Œæ›´å…·çœŸå®
    # ok
    def _align_many_raw_dfs_by_stock_pool_and_fill(self, raw_dfs: Dict[str, pd.DataFrame],
                                                   stock_pool_param: pd.DataFrame = None,
                                                   ) -> Dict[str, pd.DataFrame]:
        """
        ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨æƒå¨è‚¡ç¥¨æ± å¯¹é½å’Œæ¸…æ´—æ‰€æœ‰æ•°æ®

        Args:
            raw_dfs: åŸå§‹æ•°æ®å­—å…¸
            stock_pool_df: æƒå¨è‚¡ç¥¨æ± DataFrame
        Returns:
            å¯¹é½å’Œæ¸…æ´—åçš„æ•°æ®å­—å…¸
        """
        aligned_data = {}
        for factor_name, raw_df in raw_dfs.items():
            # 1. ç¡®å®šå½“å‰å› å­éœ€è¦å“ªä¸ªè‚¡ç¥¨æ± ï¼
            aligned_df = self.__align_one_raw_dfs_by_stock_pool_and_fill(factor_name, raw_df, stock_pool_param)
            aligned_data[factor_name] = aligned_df
        return aligned_data

    def _get_required_fields(self) -> List[str]:
        """è·å–æ‰€æœ‰éœ€è¦çš„å­—æ®µ"""
        required_fields = set()

        # åŸºç¡€å­—æ®µ
        required_fields.update([
            'pct_chg',  # è‚¡ç¥¨æ”¶ç›Šä¸æŒ‡æ•°æ”¶ç›Šçš„è”åŠ¨beta (ç”¨äºä¸­æ€§åŒ– è¿›ä¸€æ­¥å‡€åŒ–å› å­ å®ƒèƒ½ä¸ºåŠ¨é‡å› å­â€œé™å™ªâ€ï¼Œé¢å¤–å‰”é™¤å¸‚åœºç³»ç»Ÿæ€§é£é™©ï¼ˆBetaï¼‰çš„å½±å“ã€‚

            'close',
            'pb',  # ä¸ºäº†è®¡ç®—ä»·å€¼ç±»å› å­
            'total_mv', 'turnover_rate',  # ä¸ºäº†è¿‡æ»¤ å¾ˆå·®åŠ²çš„è‚¡ç¥¨ ä»…æ­¤è€Œå·²ï¼Œä¸ä¼šä½œå…¶ä»–è®¡ç®— ã€'total_mv'è¿˜å¯ ç”¨äºè®¡ç®—ä¸­æ€§åŒ–
            'industry',  # ç”¨äºè®¡ç®—ä¸­æ€§åŒ–
            'circ_mv',  # æµé€šå¸‚å€¼ ç”¨äºWOSï¼ŒåŠ æƒæœ€å°äºŒæ–¹è·Ÿ  ï¼Œå›å½’æ³•ä¼šç”¨åˆ°
            'list_date',  # ä¸Šå¸‚æ—¥æœŸ,

            'open', 'high', 'low', 'pre_close'  # ä¸ºäº†è®¡ç®—æ¬¡æ—¥æ˜¯å¦ä¸€å­—é©¬æ¶¨åœ
        ])

        # ç›®æ ‡å› å­æ‰€éœ€åŸºç¡€å­—æ®µ
        target_factors_for_evaluation = self.config['target_factors_for_evaluation']
        required_fields.update(self.get_cal_base_factors(target_factors_for_evaluation['fields']))

        # ä¸­æ€§åŒ–éœ€è¦çš„å­—æ®µ
        neutralization = self.config['preprocessing']['neutralization']
        if neutralization['enable']:
            if 'industry' in neutralization['factors']:
                required_fields.add('industry')
            if 'market_cap' in neutralization['factors']:
                required_fields.add('total_mv')
        return list(required_fields)

    def _check_data_quality(self):
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        print("  æ£€æŸ¥æ•°æ®å®Œæ•´æ€§å’Œè´¨é‡...")

        for field_name, df in self.raw_dfs.items():
            # æ£€æŸ¥æ•°æ®å½¢çŠ¶
            print(f"  {field_name}: {df.shape}")

            # æ£€æŸ¥ç¼ºå¤±å€¼æ¯”ä¾‹
            missing_ratio = df.isnull().sum().sum() / (df.shape[0] * df.shape[1])
            print(f"    ç¼ºå¤±å€¼æ¯”ä¾‹: {missing_ratio:.2%}")

            # æ£€æŸ¥å¼‚å¸¸å€¼
            if field_name in ['close', 'total_mv', 'pb', 'pe_ttm']:
                negative_ratio = (df <= 0).sum().sum() / df.notna().sum().sum()
                print(f"  æå€¼(>99%åˆ†ä½) å æ¯”: {((df > df.quantile(0.99)).sum().sum()) / (df.shape[0] * df.shape[1])}")

                if negative_ratio > 0:
                    print(f"    è­¦å‘Š: {field_name} å­˜åœ¨ {negative_ratio:.2%} çš„éæ­£å€¼")

    def _build_universe(self) -> pd.DataFrame:
        """
        æ„å»ºåŠ¨æ€è‚¡ç¥¨æ± 
        Returns:
            è‚¡ç¥¨æ± DataFrameï¼ŒTrueè¡¨ç¤ºè¯¥è‚¡ç¥¨åœ¨è¯¥æ—¥æœŸå¯ç”¨
        """
        print("  æ„å»ºåŸºç¡€è‚¡ç¥¨æ± ...")

        # ç¬¬ä¸€æ­¥ï¼šåŸºç¡€è‚¡ç¥¨æ±  - æœ‰ä»·æ ¼æ•°æ®çš„è‚¡ç¥¨
        if 'close' not in self.raw_dfs:
            raise ValueError("ç¼ºå°‘ä»·æ ¼æ•°æ®ï¼Œæ— æ³•æ„å»ºè‚¡ç¥¨æ± ")

        base_stock_pool_df = self.raw_dfs['close'].notna()
        final_stock_pool_df = base_stock_pool_df
        self.show_stock_nums_for_per_day('æ ¹æ®æ”¶ç›˜ä»·notnaç”Ÿæˆçš„', base_stock_pool_df)
        # ç¬¬äºŒæ­¥ï¼šå„ç§è¿‡æ»¤ï¼
        # --åŸºç¡€è¿‡æ»¤ æŒ‡æ•°æˆåˆ†è‚¡è¿‡æ»¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        index_config = self.config['stack_pool'].get('index_filter', {})
        if index_config.get('enable', False):
            # print(f"    åº”ç”¨æŒ‡æ•°è¿‡æ»¤: {index_config['index_code']}")
            final_stock_pool_df = self._build_dynamic_index_universe(base_stock_pool_df, index_config['index_code'])
            # âœ… åœ¨è¿™é‡Œè¿›è¡Œåˆ—ä¿®å‰ªæ˜¯åˆç†çš„ï¼ å› ä¸ºä¸­è¯800æˆåˆ†è‚¡æ˜¯åŸºäºå¤–éƒ¨è§„åˆ™ï¼Œä¸æ˜¯åŸºäºæœªæ¥æ•°æ®è¡¨ç°
            valid_stocks = final_stock_pool_df.columns[final_stock_pool_df.any(axis=0)]
            final_stock_pool_df = final_stock_pool_df[valid_stocks]
        # --æ™®é€‚æ€§ è¿‡æ»¤ ï¼ˆé€šç”¨è¿‡æ»¤ï¼‰
        final_stock_pool_df = self._filter_new_stocks(final_stock_pool_df, 6)  # æ–°è‚¡ç¥¨æ•°æ®å°‘ï¼Œä¸å…·å‚è€ƒ
        final_stock_pool_df = self._filter_st_stocks(final_stock_pool_df)  # å‰”é™¤STè‚¡ç¥¨

        # å…¶ä»–å„ç§æŒ‡æ ‡è¿‡æ»¤æ¡ä»¶
        universe_filters = self.config['stack_pool']['filters']

        # 2. æµåŠ¨æ€§è¿‡æ»¤
        if 'min_liquidity_percentile' in universe_filters:
            print("    åº”ç”¨æµåŠ¨æ€§è¿‡æ»¤...")
            final_stock_pool_df = self._filter_by_liquidity(
                final_stock_pool_df,
                universe_filters['min_liquidity_percentile']
            )

        # 3. å¸‚å€¼è¿‡æ»¤
        if 'min_market_cap_percentile' in universe_filters:
            # print("    åº”ç”¨å¸‚å€¼è¿‡æ»¤...")
            final_stock_pool_df = self._filter_by_market_cap(
                final_stock_pool_df,
                universe_filters['min_market_cap_percentile']
            )

        # å‰”é™¤æ¬¡æ—¥åœç‰Œè‚¡ç¥¨
        final_stock_pool_df = self._filter_next_day_suspended(final_stock_pool_df)
        # å‰”é™¤æ¶¨åœè‚¡ç¥¨
        final_stock_pool_df = self._filter_next_day_limit_up(final_stock_pool_df)
        return final_stock_pool_df

    # ok
    def build_st_period_from_namechange(
            self,
            ts_codes: list,
            namechange_df: pd.DataFrame,
            trading_dates: pd.DatetimeIndex
    ) -> pd.DataFrame:
        """
         ã€æœ€ç»ˆæ— æ‡ˆå¯å‡»ç‰ˆã€‘æ ¹æ®namechangeå†å²ï¼Œé‡å»ºæ¯æ—¥â€œå·²çŸ¥é£é™©â€çŠ¶æ€çŸ©é˜µã€‚
         æ­¤ç‰ˆæœ¬é€šè¿‡searchsortedéšå¼å¤„ç†åˆå§‹çŠ¶æ€ï¼Œé€»è¾‘æœ€ç®€ä¸”ç»“æœæ­£ç¡®ã€‚
         """
        logger.info("æ­£åœ¨æ ¹æ®åç§°å˜æ›´å†å²ï¼Œé‡å»ºæ¯æ—¥â€˜å·²çŸ¥é£é™©â€™çŠ¶æ€stçŸ©é˜µ...")

        # --- 1. å‡†å¤‡å·¥ä½œ ---
        if not trading_dates._is_monotonic_increasing:
            trading_dates = trading_dates.sort_values(ascending=True)

        # ã€å…³é”®ã€‘å¿…é¡»æŒ‰â€œç”Ÿæ•ˆæ—¥â€æ’åºï¼Œä»¥ç¡®ä¿çŠ¶æ€çš„æ­£ç¡®å»¶ç»­å’Œè¦†ç›–
        namechange_df['start_date'] = pd.to_datetime(namechange_df['start_date'])
        namechange_df.sort_values(by=['ts_code', 'start_date'], inplace=True)

        # ã€å…³é”®ã€‘å¿…é¡»ç”¨ np.nan åˆå§‹åŒ–ï¼Œä½œä¸ºâ€œæœªçŸ¥çŠ¶æ€â€
        st_matrix = pd.DataFrame(np.nan, index=trading_dates, columns=ts_codes)

        # --- 2. â€œæ‰“ç‚¹â€ï¼šä¸€ä¸ªå¾ªç¯å¤„ç†æ‰€æœ‰å†å²äº‹ä»¶ ---
        for ts_code, group in namechange_df.groupby('ts_code'):
            group_sorted = group.sort_values(by='start_date')
            for _, row in group_sorted.iterrows():
                start_date = row['start_date']

                # å‘ç”Ÿåœ¨å›æµ‹æœŸå‰çš„æ—¥æœŸï¼Œä¼šè¢«è‡ªåŠ¨æ˜ å°„åˆ°ä½ç½® 0  or å‘ç”Ÿåœ¨å›æµ‹æœŸå†…çš„æ—¥æœŸï¼Œä¼šè¢«æ˜ å°„åˆ°å®ƒå¯¹åº”çš„æ­£ç¡®ä½ç½®
                start_date_loc = trading_dates.searchsorted(start_date,
                                                            side='left')  # éå†trading_datesæ‰¾åˆ°é¦–ä¸ª>=start_dateçš„ä¸‹æ ‡ï¼ å¦‚æœæ˜¯rigths ï¼šåˆ™é¦–ä¸ª>çš„ä¸‹æ ‡

                # åªå¤„ç†é‚£äº›èƒ½å½±å“åˆ°æˆ‘ä»¬å›æµ‹å‘¨æœŸçš„äº‹ä»¶
                if start_date_loc < len(trading_dates):
                    name_upper = row['name'].upper()
                    is_risk_event = 'ST' in name_upper or name_upper.startswith('S')
                    # ä½¿ç”¨.ilocè¿›è¡Œèµ‹å€¼
                    start_trade_date = pd.DatetimeIndex(trading_dates)[start_date_loc]
                    st_matrix.loc[start_trade_date, ts_code] = is_risk_event

        # --- 3. â€œä¼ æ’­â€ä¸â€œæ”¶å°¾â€ ---
        st_matrix.ffill(inplace=True)
        st_matrix.fillna(False, inplace=True)

        logger.info("æ¯æ—¥â€˜å·²çŸ¥é£é™©â€™çŠ¶æ€çŸ©é˜µé‡å»ºå®Œæ¯•ã€‚")
        self.st_matrix = st_matrix.astype(bool)
        return self.st_matrix

    # ok ä¸ºä»€ä¹ˆä¸éœ€è¦shift1 å› ä¸ºä¼ä¸šä¸Šå¸‚ä¿¡æ¯ï¼Œå¾ˆå¾ˆæ—©çš„ä¿¡æ¯ï¼Œä¸å±äºåé¢ä¿¡æ¯
    def _filter_new_stocks(self, stock_pool_df: pd.DataFrame, months: int = 6) -> pd.DataFrame:
        """
        å‰”é™¤ä¸Šå¸‚æ—¶é—´å°äºæŒ‡å®šæœˆæ•°çš„è‚¡ç¥¨ã€‚
        """

        if 'list_date' not in self.raw_dfs:
            raise ValueError("ç¼ºå°‘ä¸Šå¸‚æ—¥æœŸæ•°æ®(list_date)ï¼Œè·³è¿‡æ–°è‚¡è¿‡æ»¤ã€‚")

        list_dates_df = self.raw_dfs['list_date']
        if list_dates_df.empty:
            return stock_pool_df

        # --- 1. å¯¹é½æ•°æ® ---
        aligned_universe, aligned_list_dates = stock_pool_df.align(list_dates_df, join='left')

        # --- 2. ã€æ ¸å¿ƒä¿®æ­£ã€‘å¼ºåˆ¶è½¬æ¢æ•°æ®ç±»å‹ ---
        # åœ¨æå– .values ä¹‹å‰ï¼Œç¡®ä¿æ•´ä¸ªDataFrameæ˜¯np.datetime64ç±»å‹
        # errors='coerce' ä¼šå°†ä»»ä½•æ— æ³•è½¬æ¢çš„å€¼ï¼ˆæ¯”å¦‚ç©ºå€¼æˆ–é”™è¯¯å­—ç¬¦ä¸²ï¼‰å˜æˆ NaT (Not a Time)
        try:
            list_dates_converted = aligned_list_dates.apply(pd.to_datetime, errors='raise')
        except Exception as e:
            raise ValueError(f"ä¸Šå¸‚æ—¥æœŸæ•°æ®æ— æ³•è½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼ï¼Œè¯·æ£€æŸ¥æ•°æ®æº: {e}")
            # return stock_pool_df  # Or handle error appropriately

        # --- 3. å‘é‡åŒ–è®¡ç®— ---
        dates_arr = aligned_universe.index.values[:, np.newaxis]

        # ç°åœ¨ list_dates_arr çš„ dtype å°†æ˜¯ <M8[ns]
        list_dates_arr = list_dates_converted.values

        # ç”±äº NaT - NaT = NaT, æˆ‘ä»¬éœ€è¦å¤„ç† NaTã€‚å¹¿æ’­è®¡ç®—æœ¬èº«ä¸ä¼šæŠ¥é”™ã€‚
        time_since_listing = dates_arr - list_dates_arr

        # --- 4. åˆ›å»ºå¹¶åº”ç”¨æ©ç  ---
        threshold = pd.Timedelta(days=months * 30.5)
        # NaT < threshold ä¼šæ˜¯ False, æ‰€ä»¥ NaT å€¼ä¸ä¼šè¢«é”™è¯¯åœ°å½“ä½œæ–°è‚¡
        is_new_mask = time_since_listing < threshold

        aligned_universe.values[is_new_mask] = False
        self.show_stock_nums_for_per_day("6ä¸ªæœˆå†…ä¸Šå¸‚çš„è¿‡æ»¤ï¼", aligned_universe)
        return aligned_universe

    # ok
    def _filter_st_stocks(self, stock_pool_df: pd.DataFrame) -> pd.DataFrame:
        if self.st_matrix is None:
            raise ValueError("    è­¦å‘Š: æœªèƒ½æ„å»ºSTçŠ¶æ€çŸ©é˜µï¼Œæ— æ³•è¿‡æ»¤STè‚¡ç¥¨ã€‚")
        # ã€æ ¸å¿ƒã€‘å°†â€œå†å²çœŸç›¸â€çŸ©é˜µæ•´ä½“å‘å‰ï¼ˆæœªæ¥ï¼‰ç§»åŠ¨ä¸€å¤©ã€‚ (å› ä¸ºst_matrix æ˜¯ä»¥æ®ç”Ÿæ•ˆstart_Dayæ—¥è®¡ç®—çš„ã€‚tä¸‹å•ï¼Œåªèƒ½ç”¨t-1çš„æ•°æ®è·‘ï¼Œtå•æ—¥çš„stæ— æ³•æ„ŸçŸ¥ï¼
        # è¿™ç¡®ä¿äº†æˆ‘ä»¬åœ¨Tæ—¥åšå†³ç­–æ—¶ï¼Œçœ‹åˆ°çš„æ˜¯T-1æ—¥çš„çœŸå®çŠ¶æ€ ã€‚
        st_mask_shifted = self.st_matrix.shift(1, fill_value=False)
        # å¯¹é½ä¸¤ä¸ªDataFrameçš„ç´¢å¼•å’Œåˆ—ï¼Œç¡®ä¿ä¸‡æ— ä¸€å¤±
        # join='left' è¡¨ç¤ºä»¥stock_pool_dfçš„å½¢çŠ¶ä¸ºå‡†
        aligned_universe, aligned_st_status = stock_pool_df.align(st_mask_shifted, join='left',
                                                                  fill_value=False)  # è‡³å°‘åš è¡Œåˆ— ä¿æŒä¸€è‡´çš„å¯¹é½ã€‚ ä¸‹é¢æ‰åšèµ‹å€¼ï¼ #fill_value=False ï¼šst_Dfåªèƒ½å¯¹åº”ä¸€éƒ¨åˆ†çš„è‚¡ç¥¨æ± _Df.è‚¡ç¥¨æ± _Dfå‰©ä½™çš„è¡Œåˆ— ç”¨falseå¡«å……ï¼

        # å°†STçš„è‚¡ç¥¨ä»universeä¸­å‰”é™¤
        # aligned_st_statusä¸ºTrueçš„åœ°æ–¹ï¼Œåœ¨universeä¸­å°±åº”è¯¥ä¸ºFalse
        aligned_universe[aligned_st_status] = False

        # ç»Ÿè®¡è¿‡æ»¤æ•ˆæœ
        original_count = stock_pool_df.sum(axis=1).mean()
        filtered_count = aligned_universe.sum(axis=1).mean()
        st_filtered_count = original_count - filtered_count
        print(f"      STè‚¡ç¥¨è¿‡æ»¤: å¹³å‡æ¯æ—¥å‰”é™¤ {st_filtered_count:.0f} åªSTè‚¡ç¥¨")
        self.show_stock_nums_for_per_day(f'by_STçŠ¶æ€(åˆ¤å®šæ¥è‡ªäºnameçš„å˜åŒ–å†å²)_filter', aligned_universe)

        return aligned_universe

    # ok
    def _filter_by_liquidity(self, stock_pool_df: pd.DataFrame, min_percentile: float) -> pd.DataFrame:
        """æŒ‰æµåŠ¨æ€§è¿‡æ»¤ """
        if 'turnover_rate' not in self.raw_dfs:
            raise RuntimeError("ç¼ºå°‘æ¢æ‰‹ç‡æ•°æ®ï¼Œæ— æ³•è¿›è¡ŒæµåŠ¨æ€§è¿‡æ»¤")

        turnover_df = self.raw_dfs['turnover_rate']
        turnover_df = turnover_df.shift(1)  # å–ç”¨çš„tæ—¥æ•°æ®ï¼Œå¿…é¡»å‰ç§»

        # 1. ã€ç¡®å®šæ ·æœ¬ã€‘åªä¿ç•™ stock_pool_df ä¸­ä¸º True çš„æ¢æ‰‹ç‡æ•°æ®
        # â€œåªå¯¹å½“å‰è‚¡ç¥¨æ± è®¡ç®—â€
        valid_turnover = turnover_df.where(stock_pool_df)

        # 2. ã€è®¡ç®—æ ‡å‡†ã€‘æ²¿è¡Œï¼ˆaxis=1ï¼‰ä¸€æ¬¡æ€§è®¡ç®—å‡ºæ¯æ—¥çš„åˆ†ä½æ•°é˜ˆå€¼
        thresholds = valid_turnover.quantile(min_percentile, axis=1)

        # 3. ã€åº”ç”¨æ ‡å‡†ã€‘å°†åŸå§‹æ¢æ‰‹ç‡ä¸æ¯æ—¥é˜ˆå€¼è¿›è¡Œæ¯”è¾ƒï¼Œç”Ÿæˆè¿‡æ»¤æ©ç 
        low_liquidity_mask = turnover_df.lt(thresholds, axis=0)

        # 4. å°†éœ€è¦å‰”é™¤çš„è‚¡ç¥¨åœ¨ stock_pool_df ä¸­è®¾ä¸º False
        stock_pool_df[low_liquidity_mask] = False
        self.show_stock_nums_for_per_day(f'by_å‰”é™¤æµåŠ¨æ€§ä½çš„_filter', stock_pool_df)

        return stock_pool_df

    # ok
    def _filter_by_market_cap(self,
                              stock_pool_df: pd.DataFrame,
                              min_percentile: float) -> pd.DataFrame:
        """
        æŒ‰å¸‚å€¼è¿‡æ»¤ -

        Args:
            stock_pool_df: åŠ¨æ€è‚¡ç¥¨æ± 
            min_percentile: å¸‚å€¼æœ€ä½ç™¾åˆ†ä½é˜ˆå€¼

        Returns:
            è¿‡æ»¤åçš„åŠ¨æ€è‚¡ç¥¨æ± 
        """
        if 'total_mv' not in self.raw_dfs:
            raise RuntimeError("ç¼ºå°‘å¸‚å€¼æ•°æ®ï¼Œæ— æ³•è¿›è¡Œå¸‚å€¼è¿‡æ»¤")

        mv_df = self.raw_dfs['total_mv']
        mv_df = mv_df.shift(1)

        # 1. ã€å±è”½ã€‘åªä¿ç•™åœ¨å½“å‰è‚¡ç¥¨æ± (stock_pool_df)ä¸­çš„è‚¡ç¥¨å¸‚å€¼ï¼Œå…¶ä½™è®¾ä¸ºNaN
        valid_mv = mv_df.where(stock_pool_df)

        # 2. ã€è®¡ç®—æ ‡å‡†ã€‘å‘é‡åŒ–è®¡ç®—æ¯æ—¥çš„å¸‚å€¼åˆ†ä½æ•°é˜ˆå€¼
        # axis=1 ç¡®ä¿äº†æˆ‘ä»¬æ˜¯æŒ‰è¡Œï¼ˆæ¯æ—¥ï¼‰è®¡ç®—åˆ†ä½æ•°
        thresholds = valid_mv.quantile(min_percentile, axis=1)

        # 3. ã€ç”Ÿæˆæ©ç ã€‘å°†åŸå§‹å¸‚å€¼ä¸æ¯æ—¥é˜ˆå€¼è¿›è¡Œæ¯”è¾ƒ
        # .lt() æ˜¯â€œå°äºâ€æ“ä½œï¼Œaxis=0 ç¡®ä¿äº† thresholds è¿™ä¸ªSeriesèƒ½æŒ‰è¡Œæ­£ç¡®åœ°å¹¿æ’­
        small_cap_mask = mv_df.lt(thresholds, axis=0)

        # 4. ã€åº”ç”¨è¿‡æ»¤ã€‘å°†æ‰€æœ‰å¸‚å€¼å°äºå½“æ—¥é˜ˆå€¼çš„è‚¡ç¥¨ï¼Œåœ¨è‚¡ç¥¨æ± ä¸­æ ‡è®°ä¸ºFalse
        # è¿™æ˜¯ä¸€ä¸ªè·¨è¶Šæ•´ä¸ªDataFrameçš„å¸ƒå°”è¿ç®—ï¼Œæå…¶é«˜æ•ˆ
        stock_pool_df[small_cap_mask] = False
        self.show_stock_nums_for_per_day(f'by_å‰”é™¤å¸‚å€¼ä½çš„_filter', stock_pool_df)

        return stock_pool_df

    # ok
    def _filter_next_day_limit_up(self, stock_pool_df: pd.DataFrame) -> pd.DataFrame:
        """
         å‰”é™¤åœ¨Tæ—¥å¼€ç›˜å³ä¸€å­—æ¶¨åœçš„è‚¡ç¥¨ã€‚
        è¿™æ˜¯ä¸ºäº†æ¨¡æ‹ŸçœŸå®äº¤æ˜“çº¦æŸï¼Œå› ä¸ºè¿™ç±»è‚¡ç¥¨åœ¨å¼€ç›˜æ—¶æ— æ³•ä¹°å…¥ã€‚
        Args:
            stock_pool_df: åŠ¨æ€è‚¡ç¥¨æ± DataFrame (T-1æ—¥å†³ç­–ï¼Œç”¨äºTæ—¥)
        Returns:
            è¿‡æ»¤åçš„åŠ¨æ€è‚¡ç¥¨æ± DataFrame
        """
        logger.info("    åº”ç”¨æ¬¡æ—¥æ¶¨åœè‚¡ç¥¨è¿‡æ»¤...")

        # --- 1. æ•°æ®å‡†å¤‡ä¸éªŒè¯ ---
        required_data = ['open', 'high', 'low', 'pre_close']
        for data_key in required_data:
            if data_key not in self.raw_dfs:
                raise RuntimeError(f"ç¼ºå°‘è¡Œæƒ…æ•°æ® '{data_key}'ï¼Œæ— æ³•è¿‡æ»¤æ¬¡æ—¥æ¶¨åœè‚¡ç¥¨")

        open_df = self.raw_dfs['open']
        high_df = self.raw_dfs['high']
        low_df = self.raw_dfs['low']
        pre_close_df = self.raw_dfs['pre_close']  # Tæ—¥çš„pre_closeå°±æ˜¯T-1æ—¥çš„close

        # --- 2. å‘é‡åŒ–è®¡ç®—æ¯æ—¥æ¶¨åœä»· ---
        # a) åˆ›å»ºä¸€ä¸ªä¸pre_close_dfå½¢çŠ¶ç›¸åŒçš„ã€é»˜è®¤å€¼ä¸º1.1çš„æ¶¨è·Œå¹…é™åˆ¶çŸ©é˜µ
        limit_rate = pd.DataFrame(1.1, index=pre_close_df.index, columns=pre_close_df.columns)

        # b) è¯†åˆ«ç§‘åˆ›æ¿(688å¼€å¤´)å’Œåˆ›ä¸šæ¿(300å¼€å¤´)çš„è‚¡ç¥¨ï¼Œå°†å…¶æ¶¨è·Œå¹…é™åˆ¶è®¾ä¸º1.2
        star_market_stocks = [col for col in limit_rate.columns if str(col).startswith('688')]
        chinext_stocks = [col for col in limit_rate.columns if str(col).startswith('300')]
        limit_rate[star_market_stocks] = 1.2
        limit_rate[chinext_stocks] = 1.2

        # c) è®¡ç®—ç†è®ºæ¶¨åœä»· (è¿™é‡Œä¸éœ€è¦shiftï¼Œå› ä¸ºpre_closeå·²ç»æ˜¯T-1æ—¥çš„ä¿¡æ¯)
        limit_up_price = (pre_close_df * limit_rate).round(2)

        # --- 3. ç”Ÿæˆâ€œå¼€ç›˜å³æ¶¨åœâ€çš„å¸ƒå°”æ©ç  (Mask) ---
        # æ¡ä»¶1: Tæ—¥çš„å¼€ç›˜ä»·ã€æœ€é«˜ä»·ã€æœ€ä½ä»·ä¸‰è€…ç›¸ç­‰ (ä¸€å­—æ¿çš„ç‰¹å¾)
        is_one_word_board = (open_df == high_df) & (open_df == low_df)

        # æ¡ä»¶2: Tæ—¥çš„å¼€ç›˜ä»·å¤§äºæˆ–ç­‰äºç†è®ºæ¶¨åœä»·
        is_at_limit_price = open_df >= limit_up_price

        # æœ€ç»ˆçš„æ©ç ï¼šä¸¤ä¸ªæ¡ä»¶åŒæ—¶æ»¡è¶³
        limit_up_mask = is_one_word_board & is_at_limit_price

        # --- 4. åº”ç”¨è¿‡æ»¤ ---
        # å°†åœ¨Tæ—¥å¼€ç›˜å³æ¶¨åœçš„è‚¡ç¥¨ï¼Œåœ¨Tæ—¥çš„universeä¸­å‰”é™¤
        # è¿™ä¸ªæ“ä½œæ˜¯â€œæœªæ¥â€çš„ï¼Œä½†å®ƒæ˜¯è‰¯æ€§çš„ï¼Œå› ä¸ºå®ƒæ¨¡æ‹Ÿçš„æ˜¯â€œæ— æ³•äº¤æ˜“â€çš„ç°å®
        # å®ƒä¸éœ€è¦.shift(1)ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯æ‹¿Tæ—¥çš„çŠ¶æ€ï¼Œæ¥è¿‡æ»¤Tæ—¥çš„æ± å­
        stock_pool_df[limit_up_mask] = False

        self.show_stock_nums_for_per_day('è¿‡æ»¤æ¬¡æ—¥æ¶¨åœè‚¡å--final', stock_pool_df)
        return stock_pool_df

    # ok
    def _filter_next_day_suspended(self, stock_pool_df: pd.DataFrame) -> pd.DataFrame:
        """
          å‰”é™¤æ¬¡æ—¥åœç‰Œè‚¡ç¥¨ -

          Args:
              stock_pool_df: åŠ¨æ€è‚¡ç¥¨æ± DataFrame

          Returns:
              è¿‡æ»¤åçš„åŠ¨æ€è‚¡ç¥¨æ± DataFrame
          """
        if 'close' not in self.raw_dfs:
            raise RuntimeError(" ç¼ºå°‘ä»·æ ¼æ•°æ®ï¼Œæ— æ³•è¿‡æ»¤æ¬¡æ—¥åœç‰Œè‚¡ç¥¨")

        close_df = self.raw_dfs['close']

        # 1. åˆ›å»ºä¸€ä¸ªä»£è¡¨â€œå½“æ—¥æœ‰ä»·æ ¼â€çš„å¸ƒå°”çŸ©é˜µ
        today_has_price = close_df.notna()

        # 2. åˆ›å»ºä¸€ä¸ªä»£è¡¨â€œæ¬¡æ—¥æœ‰ä»·æ ¼â€çš„å¸ƒå°”çŸ©é˜µ
        #    shift(-1) å°† T+1 æ—¥çš„æ•°æ®ï¼Œç§»åŠ¨åˆ° T æ—¥çš„è¡Œã€‚è¿™å°±åœ¨ä¸€ç¬é—´å®Œæˆäº†æ‰€æœ‰â€œnext_dateâ€çš„æŸ¥æ‰¾
        #    fill_value=True ä¼˜é›…åœ°å¤„ç†äº†æœ€åä¸€å¤©ï¼Œæˆ‘ä»¬å‡è®¾æœ€åä¸€å¤©ä¹‹åä¸ä¼šåœç‰Œ
        tomorrow_has_price = close_df.notna().shift(-1, fill_value=True)

        # 3. è®¡ç®—å‡ºæ‰€æœ‰â€œæ¬¡æ—¥åœç‰Œâ€çš„æ©ç  (Mask) ï¼ˆä¸ºä»€ä¹ˆè¦å‰”é™¤ï¼è´¨ç–‘è‡ªå·±ï¼šæ˜å¤©çš„äº‹æƒ…æˆ‘ä¸ºä»€ä¹ˆè¦ç®¡ï¼Ÿ ç­”ï¼šä½ ä¸æ€•æ˜å¤©åœç‰Œå–ä¸å‡ºå»ï¼Ÿ è¿˜æœ‰ä¸ªåŸå› ï¼šic è®¡ç®—æ”¶ç›Šç‡ï¼Œä¼šæŠŠæ˜å¤©çš„æ”¶ç›Š0 ä¸€æ ·è¿›è¡Œè®¡ç®—ï¼ã€‚é‚£æ€ä¹ˆå¾—äº†ï¼ï¼‰
        #    æ¬¡æ—¥åœç‰Œ = ä»Šæ—¥æœ‰ä»· & æ˜æ—¥æ— ä»·
        next_day_suspended_mask = today_has_price & (~tomorrow_has_price)

        # 4. ä¸€æ¬¡æ€§ä»è‚¡ç¥¨æ± ä¸­å‰”é™¤æ‰€æœ‰è¢«æ ‡è®°çš„è‚¡ç¥¨
        #    è¿™ä¸ªå¸ƒå°”è¿ç®—ä¼šè‡ªåŠ¨æŒ‰ç´¢å¼•å¯¹é½ï¼Œåº”ç”¨åˆ°æ•´ä¸ªDataFrame
        stock_pool_df[next_day_suspended_mask] = False

        return stock_pool_df

    def _load_dynamic_index_components(self, index_code: str,
                                       start_date: str, end_date: str) -> pd.DataFrame:
        """åŠ è½½åŠ¨æ€æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®"""
        # print(f"    åŠ è½½ {index_code} åŠ¨æ€æˆåˆ†è‚¡æ•°æ®...")

        index_file_name = index_code.replace('.', '_')
        index_data_path = LOCAL_PARQUET_DATA_DIR / 'index_weights' / index_file_name

        if not index_data_path.exists():
            raise ValueError(f"æœªæ‰¾åˆ°æŒ‡æ•° {index_code} çš„æˆåˆ†è‚¡æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œdownloaderä¸‹è½½")

        # ç›´æ¥è¯»å–åˆ†åŒºæ•°æ®ï¼Œpandasä¼šè‡ªåŠ¨åˆå¹¶æ‰€æœ‰year=*åˆ†åŒº
        components_df = pd.read_parquet(index_data_path)
        components_df['trade_date'] = pd.to_datetime(components_df['trade_date'])

        # æ—¶é—´èŒƒå›´è¿‡æ»¤
        # å¤§å‘å•Š ï¼Œstart_dateå¿…é¡»æå‰6ä¸ªæœˆï¼ï¼ï¼ å› ä¸ºæœ€åœº6ä¸ªæœˆæ‰æœ‰æ–°çš„æ•°æ®ï¼ ï¼ˆæ–°è€æ•°æ®é—´éš”æœ€é•¿å¯è¾¾6ä¸ªæœˆï¼ï¼‰ã€‚åé¢é€æ—¥å¡«å……æˆåˆ†è‚¡ä¿¡æ¯ï¼šåŸç†å°±æ˜¯å–ä¸Šæ¬¡æ•°æ®è¿›è¡Œå¡«å……çš„ï¼
        extended_start_date = pd.Timestamp(start_date) - pd.DateOffset(months=6)
        mask = (components_df['trade_date'] >= extended_start_date) & \
               (components_df['trade_date'] <= pd.Timestamp(end_date))
        components_df = components_df[mask]

        # print(f"    æˆåŠŸåŠ è½½ç¬¦åˆå½“å‰å›æµ‹æ—¶é—´æ®µï¼š {len(components_df)} æ¡æˆåˆ†è‚¡è®°å½•")
        return components_df

    def _build_dynamic_index_universe(self, stock_pool_df, index_code: str) -> pd.DataFrame:
        """æ„å»ºåŠ¨æ€æŒ‡æ•°è‚¡ç¥¨æ± """
        start_date = self.config['backtest']['start_date']
        end_date = self.config['backtest']['end_date']

        # åŠ è½½åŠ¨æ€æˆåˆ†è‚¡æ•°æ®
        components_df = self._load_dynamic_index_components(index_code, start_date, end_date)

        # è·å–äº¤æ˜“æ—¥åºåˆ—
        trading_dates = self.data_loader.get_trading_dates(start_date, end_date)

        # ğŸ”§ ä¿®å¤ï¼šåˆ›å»ºæ–°çš„DataFrameï¼Œè€Œä¸æ˜¯ä¿®æ”¹åŸæœ‰çš„
        index_stock_pool_df = stock_pool_df.copy()

        # é€æ—¥å¡«å……æˆåˆ†è‚¡ä¿¡æ¯
        for date in trading_dates:
            if date not in index_stock_pool_df.index:
                continue

            # è·å–å½“æ—¥æˆåˆ†è‚¡
            daily_components = components_df[
                components_df['trade_date'] == date
                ]['con_code'].tolist()

            if daily_components:
                # ğŸ”§ ä¿®å¤ï¼šåœ¨åŸºç¡€è‚¡ç¥¨æ± çš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥ç­›é€‰æŒ‡æ•°æˆåˆ†è‚¡
                valid_stocks = index_stock_pool_df.columns.intersection(daily_components)

                # åªä¿ç•™æ—¢åœ¨åŸºç¡€è‚¡ç¥¨æ± ä¸­ï¼Œåˆæ˜¯æŒ‡æ•°æˆåˆ†è‚¡çš„è‚¡ç¥¨
                current_universe = index_stock_pool_df.loc[date].copy()  # å½“å‰åŸºç¡€è‚¡ç¥¨æ± 
                index_stock_pool_df.loc[date, :] = False  # å…ˆæ¸…é›¶

                # åŒæ—¶æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ï¼š1)åœ¨åŸºç¡€è‚¡ç¥¨æ± ä¸­ 2)æ˜¯æŒ‡æ•°æˆåˆ†è‚¡
                final_valid_stocks = []
                for stock in valid_stocks:
                    if current_universe[stock]:  # åœ¨åŸºç¡€è‚¡ç¥¨æ± ä¸­
                        final_valid_stocks.append(stock)

                index_stock_pool_df.loc[
                    date, final_valid_stocks] = True  # ä»¥ä¸Š å¼ºè¡Œä¿è¯äº† ä¸€å®šæ˜¯æœ‰closeï¼ˆå³current_universe[stock]ä¸ºtrueï¼‰ è¿˜ä¿è¯ä¸€å®šæ˜¯ç›®æ ‡æˆåˆ†è‚¡

            else:
                # å½“æ—¥æ— æˆåˆ†è‚¡æ•°æ®ï¼Œä½¿ç”¨æœ€è¿‘ä¸€æ¬¡çš„æˆåˆ†è‚¡
                recent_components = components_df[
                    components_df['trade_date'] <= date
                    ]
                if not recent_components.empty:
                    latest_date = recent_components['trade_date'].max()
                    latest_components = recent_components[
                        recent_components['trade_date'] == latest_date
                        ]['con_code'].tolist()

                    valid_stocks = index_stock_pool_df.columns.intersection(latest_components)
                    current_universe = index_stock_pool_df.loc[date].copy()

                    index_stock_pool_df.loc[date, :] = False
                    final_valid_stocks = [stock for stock in valid_stocks if current_universe[stock]]
                    index_stock_pool_df.loc[date, final_valid_stocks] = True
        self.show_stock_nums_for_per_day(f'by_æˆåˆ†è‚¡æŒ‡æ•°_filter', index_stock_pool_df)

        return index_stock_pool_df

    def get_factor_data(self) -> pd.DataFrame:
        """
        è®¡ç®—ç›®æ ‡å› å­æ•°æ®

        Returns:
            å› å­æ•°æ®DataFrame
        """
        target_factors_for_evaluation = self.config['target_factors_for_evaluation']
        factor_name = target_factors_for_evaluation['name']
        fields = target_factors_for_evaluation['fields']

        print(f"\nè®¡ç®—ç›®æ ‡å› å­: {factor_name}")

        # ä½¿ç”¨å¤„ç†åçš„æ•°æ®
        data_source = getattr(self, 'processed_data', self.raw_dfs)

        # ç®€å•çš„å› å­è®¡ç®—é€»è¾‘
        if factor_name == 'pe_inv' and 'pe_ttm' in fields:
            # PEå€’æ•°å› å­
            pe_data = data_source['pe_ttm']
            factor_data = 1 / pe_data
            factor_data = factor_data.replace([np.inf, -np.inf], np.nan)
        else:
            # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªå­—æ®µ
            factor_data = data_source[fields[0]]

        return factor_data

    def get_universe(self) -> pd.DataFrame:
        """è·å–è‚¡ç¥¨æ± """
        return self.stock_pool_df

    def get_price_data(self) -> pd.DataFrame:
        """è·å–ä»·æ ¼æ•°æ®"""
        return self.raw_dfs['close']

    def get_namechange_data(self) -> pd.DataFrame:
        """è·å–nameæ”¹å˜çš„æ•°æ®"""
        namechange_path = LOCAL_PARQUET_DATA_DIR / 'namechange.parquet'

        return pd.read_parquet(namechange_path)

    def save_data_summary(self, output_dir: str):
        """ä¿å­˜æ•°æ®æ‘˜è¦"""
        os.makedirs(output_dir, exist_ok=True)

        # ä¿å­˜è‚¡ç¥¨æ± ç»Ÿè®¡
        universe_stats = {
            'daily_count': self.stock_pool_df.sum(axis=1),
            'stock_coverage': self.stock_pool_df.sum(axis=0)
        }

        summary_path = os.path.join(output_dir, 'data_summary.xlsx')
        with pd.ExcelWriter(summary_path) as writer:
            # æ¯æ—¥è‚¡ç¥¨æ•°ç»Ÿè®¡
            universe_stats['daily_count'].to_frame('stock_count').to_excel(
                writer, sheet_name='daily_stock_count'
            )

            # è‚¡ç¥¨è¦†ç›–ç»Ÿè®¡
            universe_stats['stock_coverage'].to_frame('coverage_days').to_excel(
                writer, sheet_name='stock_coverage'
            )

            # æ•°æ®è´¨é‡æŠ¥å‘Š
            quality_report = []
            for field_name, df in self.raw_dfs.items():
                quality_report.append({
                    'field': field_name,
                    'shape': f"{df.shape[0]}x{df.shape[1]}",
                    'missing_ratio': f"{df.isnull().sum().sum() / (df.shape[0] * df.shape[1]):.2%}",
                    'valid_ratio': f"{df.notna().sum().sum() / (df.shape[0] * df.shape[1]):.2%}"
                })

            pd.DataFrame(quality_report).to_excel(
                writer, sheet_name='data_quality', index=False
            )

        print(f"æ•°æ®æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_path}")

    def show_stock_nums_for_per_day(self, describe_text, index_stock_pool_df):
        daily_count = index_stock_pool_df.sum(axis=1)
        logger.info(f"    {describe_text}åŠ¨æ€è‚¡ç¥¨æ± :")
        logger.info(f"      å¹³å‡æ¯æ—¥è‚¡ç¥¨æ•°: {daily_count.mean():.0f}")
        logger.info(f"      æœ€å°‘æ¯æ—¥è‚¡ç¥¨æ•°: {daily_count.min():.0f}")
        logger.info(f"      æœ€å¤šæ¯æ—¥è‚¡ç¥¨æ•°: {daily_count.max():.0f}")

    # è¾“å…¥å­¦æœ¯å› å­ï¼Œè¿”å›è®¡ç®—æ‰€å¿…é¡»çš„base å› å­
    def get_cal_base_factors(self, target_factors: list[str]) -> set:
        factor_df = pd.DataFrame(self.config['factor_definition'])  # å°† list[dict] è½¬ä¸º DataFrame
        result = set()

        for target_factors_for_evaluation in target_factors:
            matched = factor_df[factor_df['name'] == target_factors_for_evaluation]
            if not matched.empty:
                base_fields = matched.iloc[0]['cal_require_base_fields']
                result.update(base_fields)  # ç”¨ update åˆå¹¶åˆ—è¡¨åˆ° set

        return result
    #ok
    def product_stock_pool(self, stock_pool_config_profile, pool_name):
        """
                æ„å»ºåŠ¨æ€è‚¡ç¥¨æ± 
                Returns:
                    è‚¡ç¥¨æ± DataFrameï¼ŒTrueè¡¨ç¤ºè¯¥è‚¡ç¥¨åœ¨è¯¥æ—¥æœŸå¯ç”¨
                """
        logger.info(f"  æ„å»º{pool_name}åŠ¨æ€è‚¡ç¥¨æ± ...")
        # ç¬¬ä¸€æ­¥ï¼šåŸºç¡€è‚¡ç¥¨æ±  - æœ‰ä»·æ ¼æ•°æ®çš„è‚¡ç¥¨
        if 'close' not in self.raw_dfs:
            raise ValueError("ç¼ºå°‘ä»·æ ¼æ•°æ®ï¼Œæ— æ³•æ„å»ºè‚¡ç¥¨æ± ")

        final_stock_pool_df = self.raw_dfs['close'].notna()
        self.show_stock_nums_for_per_day('æ ¹æ®æ”¶ç›˜ä»·notnaç”Ÿæˆçš„', final_stock_pool_df)
        # ç¬¬äºŒæ­¥ï¼šå„ç§è¿‡æ»¤ï¼
        # --åŸºç¡€è¿‡æ»¤ æŒ‡æ•°æˆåˆ†è‚¡è¿‡æ»¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        index_config = stock_pool_config_profile[pool_name].get('index_filter', {})
        if index_config.get('enable', False):
            # print(f"    åº”ç”¨æŒ‡æ•°è¿‡æ»¤: {index_config['index_code']}")
            final_stock_pool_df = self._build_dynamic_index_universe(final_stock_pool_df, index_config['index_code'])
            # âœ… åœ¨è¿™é‡Œè¿›è¡Œåˆ—ä¿®å‰ªæ˜¯åˆç†çš„ï¼ å› ä¸ºä¸­è¯800æˆåˆ†è‚¡æ˜¯åŸºäºå¤–éƒ¨è§„åˆ™ï¼Œä¸æ˜¯åŸºäºæœªæ¥æ•°æ®è¡¨ç°
            valid_stocks = final_stock_pool_df.columns[final_stock_pool_df.any(axis=0)]
            final_stock_pool_df = final_stock_pool_df[valid_stocks]
        # --æ™®é€‚æ€§ è¿‡æ»¤ ï¼ˆé€šç”¨è¿‡æ»¤ï¼‰
        final_stock_pool_df = self._filter_new_stocks(final_stock_pool_df, 6)  # æ–°è‚¡ç¥¨æ•°æ®å°‘ï¼Œä¸å…·å‚è€ƒ
        final_stock_pool_df = self._filter_st_stocks(final_stock_pool_df)  # å‰”é™¤STè‚¡ç¥¨

        # å…¶ä»–å„ç§æŒ‡æ ‡è¿‡æ»¤æ¡ä»¶
        universe_filters = stock_pool_config_profile[pool_name]['filters']

        # 2. æµåŠ¨æ€§è¿‡æ»¤
        if 'min_liquidity_percentile' in universe_filters:
            print("    åº”ç”¨æµåŠ¨æ€§è¿‡æ»¤...")
            final_stock_pool_df = self._filter_by_liquidity(
                final_stock_pool_df,
                universe_filters['min_liquidity_percentile']
            )

        # 3. å¸‚å€¼è¿‡æ»¤
        if 'min_market_cap_percentile' in universe_filters:
            # print("    åº”ç”¨å¸‚å€¼è¿‡æ»¤...")
            final_stock_pool_df = self._filter_by_market_cap(
                final_stock_pool_df,
                universe_filters['min_market_cap_percentile']
            )
        # å‰”é™¤æ¬¡æ—¥åœç‰Œè‚¡ç¥¨
        if universe_filters['remove_next_day_suspended']:
            final_stock_pool_df = self._filter_next_day_suspended(final_stock_pool_df)
        # å‰”é™¤æ¶¨åœè‚¡ç¥¨
        if universe_filters['remove_next_day_limit_up']:
            final_stock_pool_df = self._filter_next_day_limit_up(final_stock_pool_df)
        return final_stock_pool_df

    def get_school_code_by_factor_name(self, factor_name):
        factor_dict = {item['name']: item for item in self.config['factor_definition']}
        return factor_dict[factor_name]['school']

    def get_stock_pool_by_factor_name(self, factor_name):
        school_code = self.get_school_code_by_factor_name(factor_name)
        if school_code in ['fundamentals', 'trend']:
            return self.stock_pools_dict['institutional_stock_pool']
        if school_code in ['microstructure']:
            return self.stock_pools_dict['microstructure_stock_pool']
        raise ValueError(f'æ²¡æœ‰å®šä¹‰å› å­:{factor_name}å±äºå“ªä¸€é—¨æ´¾')

    def get_stock_pool_name_by_factor_school(self, factor_school):
        if factor_school in ['fundamentals', 'trend']:
            return 'institutional_stock_pool'
        if factor_school in ['microstructure']:
            return 'microstructure_stock_pool'
        raise ValueError('æ²¡æœ‰å®šä¹‰å› å­å±äºå“ªä¸€é—¨æ´¾')

    def __align_one_raw_dfs_by_stock_pool_and_fill(self, factor_name, raw_df_param,
                                                   stock_pool_param: pd.DataFrame = None):
        # å®šä¹‰ä¸åŒç±»å‹æ•°æ®çš„å¡«å……ç­–ç•¥
        HIGH_FREQ_FIELDS = ['turnover', 'volume', 'returns', 'turnover_rate','pct_chg']  #
        SLOW_MOVING_FIELDS = ['pe_ttm', 'pb', 'total_mv', 'circ_mv']  # ç¼“å˜æ•°æ®ï¼Œé™åˆ¶å‰å‘å¡«å……
        STATIC_FIELDS = ['industry', 'list_date']  # é™æ€æ•°æ®ï¼Œæ— é™å‰å‘å¡«å……
        PRICE_FIELDS = ['close', 'open', 'high', 'low', 'pre_close']  # ä»·æ ¼æ•°æ®ï¼Œç‰¹æ®Šå¤„ç†
        raw_df = raw_df_param.copy(deep=True)
        if stock_pool_param is not None:
            stock_pool_df = stock_pool_param
        else:
            stock_pool_df = self.get_stock_pool_by_factor_name(factor_name)

        # æ­¥éª¤1: å¯¹é½åˆ°ä¿®å‰ªåçš„è‚¡ç¥¨æ±  å¯¹é½åˆ°ä¸»æ¨¡æ¿ï¼ˆstock_pool_dfçš„å½¢çŠ¶ï¼‰
        aligned_df = raw_df.reindex(index=stock_pool_df.index, columns=stock_pool_df.columns)
        aligned_df = aligned_df.sort_index()
        aligned_df = aligned_df.where(stock_pool_df)

        # æ­¥éª¤2: æ ¹æ®æ•°æ®ç±»å‹åº”ç”¨ä¸åŒçš„å¡«å……ç­–ç•¥
        if factor_name in HIGH_FREQ_FIELDS:
            # é«˜é¢‘æ•°æ® æš‚æ—¶ä¸ffillï¼Œå› ä¸ºåœ¨åœç‰Œæ—¥ï¼Œäº¤æ˜“ç›¸å…³çš„æ´»åŠ¨æ´»åŠ¨ï¼ˆï¼ˆæˆäº¤é‡ã€æ¢æ‰‹ç‡ ç¡®å®æ˜¯ç©ºçš„ï¼‰ï¼Œä½ å»ffillä¹‹æ°”çš„é‚£ä¸å°±å¤§é”™äº†ï¼›è‡³äºfillï¼ˆ0ï¼‰è¿˜æ˜¯ä¿æŒnanï¼Œè®©ä¸‹æ¸¸è‡ªå·±è€ƒè™‘ï¼Œè¿™é‡Œä¸æå‰ä¸€æ£å­æ‰“æ­»
            # aligned_df = aligned_df.where(stock_pool_df).fillna(0)
            aligned_df = aligned_df

        elif factor_name in SLOW_MOVING_FIELDS:
            # ç¼“å˜æ•°æ®ï¼šå…ˆé™åˆ¶å‰å‘å¡«å……ï¼Œå†åº”ç”¨è‚¡ç¥¨æ± è¿‡æ»¤
            aligned_df = aligned_df.ffill(limit=2)  # æœ€å¤šå‰å‘å¡«å……2å¤©

        elif factor_name in STATIC_FIELDS:
            # é™æ€æ•°æ®ï¼šæ— é™å‰å‘å¡«å……ï¼Œå†åº”ç”¨è‚¡ç¥¨æ± è¿‡æ»¤
            aligned_df = aligned_df.ffill()  # ä»»ç”±ä»–å¡«å……åˆä½•å¦¨ï¼Œåæ­£æˆ‘å‰æœŸåšäº†è‡ªåŠ¨å®½åŒ–å¡«å……

        elif factor_name in PRICE_FIELDS:
            # ä»·æ ¼æ•°æ®ï¼šåªä¿ç•™è‚¡ç¥¨æ± å†…çš„æ•°æ®  å•å› å­æµ‹è¯•éœ€è¦è®¡ç®—æ”¶ç›Šç‡ï¼Œä»·æ ¼æ•°æ®ä¸èƒ½ä¸­æ–­ å€¼å¾—æ·±å…¥æ€è€ƒã€‚
            #èµæˆfillç†ç”±ï¼šã€‚æ ¹æ®æ ‡å‡†çš„åŸºé‡‘ä¼šè®¡å‡†åˆ™ï¼Œåœ¨åœç‰ŒæœŸé—´ï¼Œä¸€åªè‚¡ç¥¨çš„ä»·å€¼å¹¶æ²¡æœ‰æ¶ˆå¤±æˆ–å˜æˆæœªçŸ¥ã€‚ä¸ºäº†è®¡ç®—æ¯æ—¥çš„æŠ•èµ„ç»„åˆå‡€å€¼ï¼Œå®ƒçš„ä»·å€¼å¿…é¡»è¢«å®šä¹‰ä¸º**â€œæœ€åä¸€ä¸ªå¯è·å¾—çš„å…¬å…ä»·å€¼â€**ï¼Œä¹Ÿå°±æ˜¯å®ƒåœç‰Œå‰çš„æœ€åä¸€ä¸ªä»·æ ¼/å¸‚å€¼ã€‚
            # æˆ‘è¿˜æ˜¯è§‰å¾— æ±¡æŸ“äº†æ­£ç¡®æ€§ï¼ åæœŸæœ‰ç©ºå†è§£å†³ todo
            # åœç‰Œè‚¡ç¥¨ä»éœ€å®šä»·æ¥è®¡ç®—ç»„åˆå‡€å€¼å’Œæ”¶ç›Š
            aligned_df = aligned_df.ffill()

        else:
            raise RuntimeError(f"æ­¤å› å­{factor_name}æ²¡æœ‰æŒ‡æ˜é¢‘ç‡ï¼Œæ— æ³•è¿›è¡Œå¡«å……")
        return aligned_df

def create_data_manager(config_path: str) -> DataManager:
    """
    åˆ›å»ºæ•°æ®ç®¡ç†å™¨å®ä¾‹
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        DataManagerå®ä¾‹
    """
    return DataManager(config_path)
